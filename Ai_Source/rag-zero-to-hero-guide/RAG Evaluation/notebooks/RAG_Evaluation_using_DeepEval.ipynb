{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ee9607f0460437887714c5ddbaec455": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_dae5cfe8c19a4838a8130b3e9447e002",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": " ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o-mini, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o-mini, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "dae5cfe8c19a4838a8130b3e9447e002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00fe2c3985704851a7d258bf09edf2ee": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1f32aa5e9aa04b82bd21483c789a000e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o-mini, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o-mini, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1f32aa5e9aa04b82bd21483c789a000e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d33589b5adb47d09117ad154d32b6d8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_14326d08443c446c88becf84a316c19d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o-mini, strict=False, async_mode=False)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o-mini, strict=False, async_mode=False)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "14326d08443c446c88becf84a316c19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab999462c47a4629bbc1845a395c8e52": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c5e688d9b8a641ad851c866abe4ef618",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o-mini, strict=False, async_mode=True…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o-mini, strict=False, async_mode=True…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c5e688d9b8a641ad851c866abe4ef618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85bf1603c5774633879433e05b192bf4": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_45de7bb2849d49b5ac2b52ecedbf0942",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o-mini, strict=False, async_mode=True)...\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o-mini, strict=False, async_mode=True)...</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "45de7bb2849d49b5ac2b52ecedbf0942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7a43c97655447283706a08ee23107d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f620ca6083c94e95a6c4b6747304881a",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o-mini, strict=False, async_mode=True…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o-mini, strict=False, async_mode=True…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f620ca6083c94e95a6c4b6747304881a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Evaluation using DeepEval**\n",
        "\n",
        "Authored by [Kalyan KS](https://www.linkedin.com/in/kalyanksnlp/). To stay updated with LLMs, RAG and Agents, you can follow him on [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/), [Twitter](https://x.com/kalyan_kpl) and [YouTube](https://youtube.com/@kalyanksnlp?si=ZdoC0WPN9TmAOvKB).\n",
        "\n",
        "\n",
        "\n",
        "- DeepEval is a simple-to-use, open-source LLM evaluation framework.\n",
        "- DeepEval includes popular metrics to evaluate both the retriever and generator components of RAG system."
      ],
      "metadata": {
        "id": "LHQPblo752p3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9dITz2eMaPo",
        "outputId": "f6dd1924-3848-4eab-adf0-ef0ae5ba9ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m576.3/576.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.6/243.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.0/739.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU deepeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "k12q0jpBaJFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `query` and `reference` are required to create an LLMTestCase.\n",
        "\n",
        "So it is mandatory to pass these two along with the necessary inputs required to compute the evaluation metric."
      ],
      "metadata": {
        "id": "rKK34_ui3Nxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Retriever Evaluation**"
      ],
      "metadata": {
        "id": "OuzxBsoPzniz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context Precision**\n",
        "\n",
        "- Context Precision is a metric that evaluates how well a RAG system ranks  relevant chunks within the retrieved contexts.\n",
        "\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}}\n",
        "$$"
      ],
      "metadata": {
        "id": "1rlPePt8zp6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import ContextualPrecisionMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Initialize the metric\n",
        "metric = ContextualPrecisionMetric(\n",
        "    threshold=0.75,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "# Define the test case\n",
        "query = \"Will it rain this afternoon?\"\n",
        "response = \"There's a 60% chance of rain after 2 PM today.\"\n",
        "reference = \"Expect a 60% probability of rainfall this afternoon after 2 PM.\"\n",
        "context = [\n",
        "    \"The weather forecast indicates a 60% chance of rain starting after 2 PM today.\",\n",
        "    \"Temperatures will drop slightly in the afternoon due to cloud cover.\",\n",
        "    \"Yesterday’s forecast was unrelated to today’s weather patterns.\",\n",
        "    \"Rain is more likely in the northern regions this afternoon.\"\n",
        "]\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=response,\n",
        "    expected_output=reference,\n",
        "    retrieval_context=context\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "metric.measure(test_case)\n",
        "\n",
        "# Display score and explanation\n",
        "print(f\"Context Precision Score: {metric.score}\")\n",
        "print(f\"Explanation: {metric.reason}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "ab999462c47a4629bbc1845a395c8e52",
            "c5e688d9b8a641ad851c866abe4ef618"
          ]
        },
        "id": "3nprlspF0AjV",
        "outputId": "b3c537c7-c823-4d76-f624-2761416b405d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab999462c47a4629bbc1845a395c8e52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Precision Score: 1.0\n",
            "Explanation: The score is 1.00 because all relevant nodes, like the first node, directly state a 60% chance of rain after 2 PM, placing them at the top of the ranking. The irrelevant nodes rank lower since they provide details such as temperatures dropping (2nd node) or unrelated forecasts (3rd node) that do not answer the question regarding rain. This clear hierarchy of relevant over irrelevant information justifies the perfect score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context Recall**\n",
        "- Context Recall is computed as the ratio of number of ground truth claims supported by the context to the total number of ground truth claims.\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Context Recall} = \\frac{|\\text{Number of GT claims that can be attributed to context}|}{|\\text{Total number of claims in GT}|}\n",
        "$$\n",
        "\n",
        "Here \"GT\" refer to ground truth."
      ],
      "metadata": {
        "id": "Uvqz6n6Y1RNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import ContextualRecallMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Initialize the metric\n",
        "metric = ContextualRecallMetric(\n",
        "    threshold=0.8,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "# Define the test case\n",
        "query = \"What caused the power outage last night?\"\n",
        "response = \"The power outage was due to a severe thunderstorm that damaged power lines.\"\n",
        "reference = \"Last night's power outage resulted from a thunderstorm causing damage to electrical infrastructure.\"\n",
        "context = [\n",
        "    \"A severe thunderstorm passed through the area last night, bringing strong winds.\",\n",
        "    \"Power lines were reported damaged around 10 PM due to fallen trees from the storm.\"\n",
        "]\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=response,\n",
        "    expected_output=reference,\n",
        "    retrieval_context=context\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "metric.measure(test_case)\n",
        "\n",
        "# Display score and explanation\n",
        "print(f\"Context Recall Score: {metric.score}\")\n",
        "print(f\"Explanation: {metric.reason}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "85bf1603c5774633879433e05b192bf4",
            "45de7bb2849d49b5ac2b52ecedbf0942"
          ]
        },
        "id": "9FR5MXkD1TeP",
        "outputId": "17601846-aeef-4c4f-a9e0-2318ef72acbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85bf1603c5774633879433e05b192bf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Recall Score: 1.0\n",
            "Explanation: The score is 1.00 because all aspects of the expected output are directly supported by the information in the nodes in retrieval context, specifically aligning perfectly with the details of the thunderstorm and the resulting power outage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context Relevancy**\n",
        "- Context Relevancy is computed as the ratio of number of statements in the context relevant to the user query to the total number of statements in the context.\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Contextual Relevancy} = \\frac{\\text{Number of statements in the context relevant to the query}}{\\text{Total number of statements in the context}}\n",
        "$$"
      ],
      "metadata": {
        "id": "PqSUWOqv1Tko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import ContextualRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Initialize the metric\n",
        "metric = ContextualRelevancyMetric(\n",
        "    threshold=0.7,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "# Define the test case\n",
        "query = \"Why did the stock market drop today?\"\n",
        "response = \"The stock market dropped due to concerns over rising inflation rates and a tech sector sell-off.\"\n",
        "context = [\n",
        "    \"Recent economic reports showed inflation reaching a 5-year high this month.\",\n",
        "    \"Major tech companies reported disappointing earnings, triggering a sell-off.\",\n",
        "    \"The weather was sunny and pleasant throughout the day.\"\n",
        "]\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=response,\n",
        "    retrieval_context=context\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "metric.measure(test_case)\n",
        "\n",
        "# Display score and explanation\n",
        "print(f\"Context Relevancy Score: {metric.score}\")\n",
        "print(f\"Reasoning: {metric.reason}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "bb7a43c97655447283706a08ee23107d",
            "f620ca6083c94e95a6c4b6747304881a"
          ]
        },
        "id": "75yZD5j91WfC",
        "outputId": "0272216c-f688-4a5c-ed68-65e563c45814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb7a43c97655447283706a08ee23107d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Relevancy Score: 0.3333333333333333\n",
            "Reasoning: The score is 0.33 because while the relevant statement indicates that 'Major tech companies reported disappointing earnings, triggering a sell-off,' it is overshadowed by the irrelevant reasons regarding inflation and weather, which do not address the stock market drop directly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Generator Evaluation**"
      ],
      "metadata": {
        "id": "VRwA8bqeZlVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Response Relevancy**\n",
        "\n",
        "- The Response Relevancy metric evaluates how relevant a generated response is to the original user query.\n",
        "- Formula is\n",
        "\n",
        " $$\n",
        "\\text{Response Relevancy Score} = \\frac{\\text{Number of statements relevant to the user query in the response}}{\\text{Total number of statements in the response}}\n",
        " $$\n",
        "\n",
        " **Note** - Response relevancy metric is also referred to as Answer relevancy."
      ],
      "metadata": {
        "id": "aBHoeV9kZoty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Initialize the metric\n",
        "metric = AnswerRelevancyMetric(\n",
        "    threshold=0.75,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "# Define the test case\n",
        "query = \"How can I improve my coding skills?\"\n",
        "response = \"Practice daily, read documentation, and build small projects. The weather is nice today.\"\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=response\n",
        ")\n",
        "\n",
        "# Compute the score\n",
        "metric.measure(test_case)\n",
        "\n",
        "# Display score and explanation\n",
        "print(f\"Answer Relevancy Score: {metric.score}\")\n",
        "print(f\"Explanation: {metric.reason}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "1ee9607f0460437887714c5ddbaec455",
            "dae5cfe8c19a4838a8130b3e9447e002"
          ]
        },
        "id": "eOm6bEpKZueP",
        "outputId": "cd4dfd04-03d3-45c9-8db9-1111e5d54da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ee9607f0460437887714c5ddbaec455"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer Relevancy Score: 0.75\n",
            "Explanation: The score is 0.75 because the output included an irrelevant statement about the weather, which does not contribute to the topic of improving coding skills. However, it still provided some relevant tips, which justifies the current score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Faithfulness**\n",
        "\n",
        "- The Faithfulness metric measures how factually consistent a generated response is with the retrieved context.\n",
        "- The Faithfulness metric is computed as the ratio of number of claims in the response supported by retrieved context to total number of claims in the response.\n",
        "- Formula is\n",
        "\n",
        "$$\n",
        "\\text{Faithfulness Score} = \\frac{\\text{Number of claims in the response supported by the retrieved context}}{\\text{Total number of claims in the response}}\n",
        "$$"
      ],
      "metadata": {
        "id": "3Zp6jsxedIrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import FaithfulnessMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Initialize the metric\n",
        "metric = FaithfulnessMetric(\n",
        "    threshold=0.7,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "# Define the test case\n",
        "query = \"What are some tips for maintaining a healthy diet?\"\n",
        "response = \"Eating fruits and vegetables daily, drinking enough water, and avoiding processed foods can improve your diet.\"\n",
        "context = [\n",
        "    \"A healthy diet includes regular consumption of fruits and vegetables.\",\n",
        "    \"Staying hydrated by drinking sufficient water is essential for good health.\",\n",
        "    \"Processed foods should be limited to maintain a balanced diet.\"\n",
        "]\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=response,\n",
        "    retrieval_context=context\n",
        ")\n",
        "\n",
        "# Compute the score\n",
        "metric.measure(test_case)\n",
        "\n",
        "# Display score and explanation\n",
        "print(f\"Faithfulness Score: {metric.score}\")\n",
        "print(f\"Explanation: {metric.reason}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "00fe2c3985704851a7d258bf09edf2ee",
            "1f32aa5e9aa04b82bd21483c789a000e"
          ]
        },
        "id": "YwBVDrdCdm_X",
        "outputId": "4c0439d5-b689-476c-a84b-f51faed9d612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00fe2c3985704851a7d258bf09edf2ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faithfulness Score: 1.0\n",
            "Explanation: The score is 1.00 because there are no contradictions, indicating that the actual output aligns perfectly with the retrieval context. Great job maintaining consistency!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hallucination**\n",
        "\n",
        "- The Hallucination metric measures how factually inconsistent a generated response is with the retrieved context.\n",
        "- The Hallucination metric is computed as the ratio of number of claims in the response unsupported by retrieved context to total number of claims in the response.\n",
        "- Formula is\n",
        "\n",
        "$$\n",
        "\\text{Hallucination Score} = \\frac{\\text{Number of claims in the response unsupported by the retrieved context}}{\\text{Total number of claims in the response}}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "FEZVAT1PfDEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import HallucinationMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "# Initialize the metric\n",
        "metric = HallucinationMetric(\n",
        "    threshold=0.6,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "# Define the test case\n",
        "query = \"What are some effective exercises for building strength?\"\n",
        "response = \"Lifting weights and doing push-ups can help build muscle strength.\"\n",
        "context = [\n",
        "    \"Strength training often involves weight lifting to increase muscle mass.\",\n",
        "    \"Bodyweight exercises like push-ups are effective for building strength.\",\n",
        "    \"Yoga improves both flexibility and muscular strength through specific poses.\"\n",
        "]\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=query,\n",
        "    actual_output=response,\n",
        "    context=context\n",
        ")\n",
        "\n",
        "# Compute the score\n",
        "metric.measure(test_case)\n",
        "\n",
        "# Display score and explanation\n",
        "print(f\"Hallucination Score: {metric.score}\")\n",
        "print(f\"Reasoning: {metric.reason}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "8d33589b5adb47d09117ad154d32b6d8",
            "14326d08443c446c88becf84a316c19d"
          ]
        },
        "id": "ugLwOD6Ifm6T",
        "outputId": "eff67048-8e72-4862-9f60-71cbed8741d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d33589b5adb47d09117ad154d32b6d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hallucination Score: 0.3333333333333333\n",
            "Reasoning: The score is 0.33 because the actual output aligns with the provided contexts regarding strength training and bodyweight exercises, but it contradicts by omitting yoga and its benefits, indicating some misinformation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Evaluation using DeepEval - Full Example**"
      ],
      "metadata": {
        "id": "tOWtmTaxHwdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import FaithfulnessMetric, ContextualRelevancyMetric"
      ],
      "metadata": {
        "id": "GCXTsaskH3e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame with RAG outputs\n",
        "data = {\n",
        "    \"query\": [\"What is the capital of France?\", \"Who won the 2020 US election?\"],\n",
        "    \"reference\": [\"The capital of France is Paris.\", \"Joe Biden won the 2020 US election.\"],\n",
        "    \"response\": [\"The capital of France is Paris.\", \"Joe Biden won the election in 2020.\"],\n",
        "    \"context\": [\n",
        "        [\"France is a country in Europe.\", \"The capital of France is Paris.\"],\n",
        "        [\"The 2020 US election was held on November 3.\", \"Joe Biden was declared the winner.\"]\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "-mQuVEw3IEy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame rows to LLMTestCase objects\n",
        "def create_test_cases(df):\n",
        "    test_cases = []\n",
        "    for index, row in df.iterrows():\n",
        "        test_case = LLMTestCase(\n",
        "            input=row[\"query\"],\n",
        "            actual_output=row[\"response\"],\n",
        "            retrieval_context=row[\"context\"]\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "    return test_cases\n",
        "\n",
        "# Create test cases from the DataFrame\n",
        "test_cases = create_test_cases(df)"
      ],
      "metadata": {
        "id": "d6BXSsxBIV7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the metrics\n",
        "faithfulness_metric = FaithfulnessMetric(\n",
        "    threshold=0.7,  # Minimum score to pass (0-1 scale)\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")\n",
        "\n",
        "context_relevancy_metric = ContextualRelevancyMetric(\n",
        "    threshold=0.7,  # Minimum score to pass (0-1 scale)\n",
        "    model=\"gpt-4o-mini\",\n",
        "    include_reason=True\n",
        ")"
      ],
      "metadata": {
        "id": "k2JX4dtdIZz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metric scores\n",
        "evaluation_results = evaluate(\n",
        "    test_cases=test_cases,\n",
        "    metrics=[faithfulness_metric, context_relevancy_metric]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "_mtVEQ3CIefS",
        "outputId": "a418d444-e13e-477b-eb10-53d35ab12928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o-mini, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
              "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o-mini, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
              "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 2 test case(s) in parallel: |██████████|100% (2/2) [Time Taken: 00:05,  2.99s/test case]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because there are no contradictions, indicating that the actual output perfectly aligns with the retrieval context., error: None)\n",
            "  - ❌ Contextual Relevancy (score: 0.5, threshold: 0.7, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.50 because while the relevant statement 'The capital of France is Paris.' directly answers the query, the irrelevant context mentions 'France is a country in Europe,' which does not help in identifying the capital., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is the capital of France?\n",
            "  - actual output: The capital of France is Paris.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: ['France is a country in Europe.', 'The capital of France is Paris.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o-mini, reason: The score is 1.00 because there are no contradictions present, indicating the actual output perfectly aligns with the retrieval context., error: None)\n",
            "  - ❌ Contextual Relevancy (score: 0.5, threshold: 0.7, strict: False, evaluation model: gpt-4o-mini, reason: The score is 0.50 because while the statement 'Joe Biden was declared the winner' is relevant, it is somewhat overshadowed by the irrelevance of the context that only mentions the date of the election without directly answering the question., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Who won the 2020 US election?\n",
            "  - actual output: Joe Biden won the election in 2020.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: ['The 2020 US election was held on November 3.', 'Joe Biden was declared the winner.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Faithfulness: 100.00% pass rate\n",
            "Contextual Relevancy: 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
              " \n",
              "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
              "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
              " \n",
              "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
              "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process and display results\n",
        "results = []\n",
        "for test_result in evaluation_results.test_results:\n",
        "    # Extract metrics data\n",
        "    faithfulness_data = next(m for m in test_result.metrics_data if m.name == \"Faithfulness\")\n",
        "    context_relevancy_data = next(m for m in test_result.metrics_data if m.name == \"Contextual Relevancy\")\n",
        "\n",
        "    # Append result dictionary\n",
        "    results.append({\n",
        "        \"query\": test_result.input,\n",
        "        \"response\": test_result.actual_output,\n",
        "        \"faithfulness_score\": faithfulness_data.score,\n",
        "        \"faithfulness_reason\": faithfulness_data.reason,\n",
        "        \"context_relevancy_score\": context_relevancy_data.score,\n",
        "        \"context_relevancy_reason\": context_relevancy_data.reason\n",
        "    })"
      ],
      "metadata": {
        "id": "zU1z2MGBIjJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame for easier viewing\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print the results\n",
        "print(\"Evaluation Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMLjAJezIqR3",
        "outputId": "36615abe-c320-41a9-e336-6dab098523f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results:\n",
            "                            query                             response  \\\n",
            "0  What is the capital of France?      The capital of France is Paris.   \n",
            "1   Who won the 2020 US election?  Joe Biden won the election in 2020.   \n",
            "\n",
            "   faithfulness_score                                faithfulness_reason  \\\n",
            "0                 1.0  The score is 1.00 because there are no contrad...   \n",
            "1                 1.0  The score is 1.00 because there are no contrad...   \n",
            "\n",
            "   context_relevancy_score                           context_relevancy_reason  \n",
            "0                      0.5  The score is 0.50 because while the relevant s...  \n",
            "1                      0.5  The score is 0.50 because while the statement ...  \n"
          ]
        }
      ]
    }
  ]
}