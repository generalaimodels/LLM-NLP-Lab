{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65ac6df1c72e43afb6aaf2b617c4ea1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0616a639cb944dcbc8812d5bb90268d",
              "IPY_MODEL_5136edd84c8a4d01be57e738979d346b",
              "IPY_MODEL_786e5393875b4be18054359f9fb95596"
            ],
            "layout": "IPY_MODEL_3bce8baff8254198adf392c4f842fc80"
          }
        },
        "a0616a639cb944dcbc8812d5bb90268d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1587a4172f114deea27eff9edf8f95e8",
            "placeholder": "​",
            "style": "IPY_MODEL_a946a9ff7d224a318387588662c69272",
            "value": "Evaluating: 100%"
          }
        },
        "5136edd84c8a4d01be57e738979d346b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb033460c6c24d79b4b9cda0a29642a4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dd14828b9664a89b6e036d493e8c140",
            "value": 4
          }
        },
        "786e5393875b4be18054359f9fb95596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c07c8767449346dea9bd94fe4ba1b327",
            "placeholder": "​",
            "style": "IPY_MODEL_4f2ea84a58bd4f63808d9333269dc37e",
            "value": " 4/4 [00:02&lt;00:00,  1.92it/s]"
          }
        },
        "3bce8baff8254198adf392c4f842fc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1587a4172f114deea27eff9edf8f95e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a946a9ff7d224a318387588662c69272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb033460c6c24d79b4b9cda0a29642a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dd14828b9664a89b6e036d493e8c140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c07c8767449346dea9bd94fe4ba1b327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2ea84a58bd4f63808d9333269dc37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Evaluation using RAGAS**\n",
        "\n",
        "Authored by [Kalyan KS](https://www.linkedin.com/in/kalyanksnlp/). To stay updated with LLMs, RAG and Agents, you can follow him on [LinkedIn](https://www.linkedin.com/in/kalyanksnlp/), [Twitter](https://x.com/kalyan_kpl) and [YouTube](https://youtube.com/@kalyanksnlp?si=ZdoC0WPN9TmAOvKB).\n",
        "\n",
        "- RAGAS is one of the popular open-source libraries for RAG evaluation.\n",
        "- RAGAS includes popular metrics to evaluate both the retriever and generator components of RAG system."
      ],
      "metadata": {
        "id": "LHQPblo752p3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9dITz2eMaPo",
        "outputId": "08d38818-be1a-4901-f1f9-940aa0f76ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/187.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/420.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU ragas langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "k12q0jpBaJFf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Retriever Evaluation**"
      ],
      "metadata": {
        "id": "OuzxBsoPzniz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context Precision**\n",
        "\n",
        "- Context Precision is a metric that evaluates how well a RAG retriever ranks  relevant chunks within the retrieved contexts.\n",
        "\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}}\n",
        "$$"
      ],
      "metadata": {
        "id": "1rlPePt8zp6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import LLMContextPrecisionWithReference\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Initialize the metric\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "context_precision = LLMContextPrecisionWithReference(llm=evaluator_llm)\n",
        "\n",
        "# Define the test case\n",
        "query = \"Will it rain this afternoon?\"\n",
        "response = \"There's a 60% chance of rain after 2 PM today.\"\n",
        "reference = \"Expect a 60% probability of rainfall this afternoon after 2 PM.\"\n",
        "context = [\n",
        "    \"The weather forecast indicates a 60% chance of rain starting after 2 PM today.\",\n",
        "    \"Temperatures will drop slightly in the afternoon due to cloud cover.\",\n",
        "    \"Yesterday’s forecast was unrelated to today’s weather patterns.\",\n",
        "    \"Rain is more likely in the northern regions this afternoon.\"\n",
        "]\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input= query,\n",
        "    reference= reference,\n",
        "    retrieved_contexts=context,\n",
        ")\n",
        "\n",
        "# Compute the metric score\n",
        "await context_precision.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nprlspF0AjV",
        "outputId": "e673d34a-78b0-4231-f6f8-ea684f62eda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context Recall**\n",
        "- Context Recall is computed as the ratio of number of ground truth claims supported by the context to the total number of ground truth claims.\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Context Recall} = \\frac{|\\text{Number of GT claims that can be attributed to context}|}{|\\text{Total number of claims in GT}|}\n",
        "$$\n",
        "\n",
        "Here \"GT\" refer to ground truth."
      ],
      "metadata": {
        "id": "Uvqz6n6Y1RNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import LLMContextRecall\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Initialize the metric\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "context_recall = LLMContextRecall(llm=evaluator_llm)\n",
        "\n",
        "# Define the test case\n",
        "query = \"What caused the power outage last night?\"\n",
        "response = \"The power outage was due to a severe thunderstorm that damaged power lines.\"\n",
        "reference = \"Last night's power outage resulted from a thunderstorm causing damage to electrical infrastructure.\"\n",
        "context = [\n",
        "    \"A severe thunderstorm passed through the area last night, bringing strong winds.\",\n",
        "    \"Power lines were reported damaged around 10 PM due to fallen trees from the storm.\"\n",
        "]\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input= query,\n",
        "    response= response,\n",
        "    reference= reference,\n",
        "    retrieved_contexts= context,\n",
        ")\n",
        "\n",
        "# Compute the metric score\n",
        "await context_recall.single_turn_ascore(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FR5MXkD1TeP",
        "outputId": "d5e0887a-58e7-4d2a-bada-24c619246b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context Entities Recall**\n",
        "\n",
        "- Context Entities Recall is computed as the ratio of number of common entities between reference and retrieved context to the total number of entities in the reference\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Context Entities Recall} = \\frac{\\text{Number of common entities between RCE and RE}}{\\text{Total number of entities in RE}}\n",
        "$$\n",
        "\n",
        "Here RE represents referenec entities and RCE represents reference context entities."
      ],
      "metadata": {
        "id": "MTjHeAFJG_Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import ContextEntityRecall\n",
        "import asyncio\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Initialize the metric\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "context_entity_recall = ContextEntityRecall(llm=evaluator_llm)\n",
        "\n",
        "# Define the test case\n",
        "query = \"What is the capital city of France?\"\n",
        "reference = \"The capital city of France is Paris.\"\n",
        "response = \"Paris is the capital of France.\"\n",
        "context = \"France is a country in Europe with a rich history and culture.\"\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input=query,\n",
        "    reference=reference,\n",
        "    response=response,\n",
        "    retrieved_contexts=[context],\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "score = asyncio.run(context_entity_recall.single_turn_ascore(sample))\n",
        "\n",
        "# Output the result\n",
        "print(f\"Context Entities Recall Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0QIvcEaIu0e",
        "outputId": "29a218a8-a385-40a9-8d82-b33f24b16de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Entities Recall Score: 0.4999999975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Generator Evaluation**"
      ],
      "metadata": {
        "id": "VRwA8bqeZlVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Response Relevancy**\n",
        "\n",
        "- The Response Relevancy metric evaluates how relevant a generated response is to the original user query.\n",
        "- Formula is\n",
        "\n",
        " $$\n",
        "\\text{Response Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{E_{g_i} \\cdot E_o}{\\|E_{g_i}\\| \\|E_o\\|}\n",
        " $$\n",
        "\n",
        " Where:\n",
        "\n",
        "     - $E_o$ : Embedding of the user query.\n",
        "     - $E_{g_i}$ : Embedding of the (i)-th synthetic query.\n",
        "     - (N): Number of synthetic queries (default is 3).\n",
        "     - $\\cos(E_{g_i}, E_o)$ : Cosine similarity between the embeddings."
      ],
      "metadata": {
        "id": "aBHoeV9kZoty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import ResponseRelevancy\n",
        "import asyncio\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "# Set up the embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
        "\n",
        "# Initialize the metric\n",
        "response_relevancy = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
        "\n",
        "# Define the test case\n",
        "query = \"What is the tallest mountain in the world?\"\n",
        "response = \"Mount Everest is the tallest mountain in the world, standing at 8,848 meters (29,029 feet).\"\n",
        "context = [\n",
        "        \"Mount Everest, located in the Himalayas on the border between Nepal and China, has an elevation of 8,848 meters above sea level, making it the highest peak on Earth.\",\n",
        "        \"The height of Mount Everest was officially determined to be 8,848 meters by the Survey of India in 1955.\"\n",
        "    ]\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input = query,\n",
        "    response = response,\n",
        "    retrieved_contexts = context\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "score = asyncio.run(response_relevancy.single_turn_ascore(sample))\n",
        "\n",
        "# Display the score\n",
        "print(f\"Response Relevancy Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOm6bEpKZueP",
        "outputId": "70de62ed-ed59-416e-9aa9-c3243ec4c213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Relevancy Score: 0.9102625215899899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Faithfulness**\n",
        "\n",
        "- The Faithfulness metric measures how factually consistent a generated response is with the retrieved context.\n",
        "- The Faithfulness metric is computed as the ratio of number of claims in the response supported by retrieved context to the total number of claims in the response.\n",
        "- Formula is\n",
        "\n",
        "$$\n",
        "\\text{Faithfulness Score} = \\frac{\\text{Number of claims in the response supported by the retrieved context}}{\\text{Total number of claims in the response}}\n",
        "$$"
      ],
      "metadata": {
        "id": "3Zp6jsxedIrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import Faithfulness\n",
        "import asyncio\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Initialize the metric\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "faithfulness = Faithfulness(llm=evaluator_llm)\n",
        "\n",
        "# Define the test case\n",
        "query = \"What are some tips for maintaining a healthy diet?\"\n",
        "response = \"Eating fruits and vegetables daily, drinking enough water, and avoiding processed foods can improve your diet.\"\n",
        "context = [\n",
        "    \"A healthy diet includes regular consumption of fruits and vegetables.\",\n",
        "    \"Staying hydrated by drinking sufficient water is essential for good health.\",\n",
        "    \"Processed foods should be limited to maintain a balanced diet.\"\n",
        "]\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input = query,\n",
        "    response = response,\n",
        "    retrieved_contexts = context,\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "score = asyncio.run(faithfulness.single_turn_ascore(sample))\n",
        "\n",
        "# Output the result\n",
        "print(f\"Faithfulness Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwBVDrdCdm_X",
        "outputId": "8e78b852-965b-4c0e-e23b-5d3d12a2e72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faithfulness Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Relevant Noise Sensitivity**\n",
        "\n",
        "- Relevant Noise Sensitivity refers to the proportion of incorrect claims in a model’s response that are entailed (i.e., supported or implied) by relevant retrieved chunks to the toal number of response claims.\n",
        "\n",
        "- A relevant chunk is a chunk that contains at least one claim from the ground truth answer.\n",
        "\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Relevant Noise Sensitivity} = \\frac{\\text{Number of incorrect claims in the model response entailed by relevant chunks}}{\\text{Total number of claims in the response}}\n",
        "$$"
      ],
      "metadata": {
        "id": "QQTnAhsvT4UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import NoiseSensitivity\n",
        "import asyncio\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Initialize the metric\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "relevant_noise_sensitivity = NoiseSensitivity(llm=evaluator_llm, mode=\"relevant\")\n",
        "\n",
        "# Define the test case\n",
        "query = \"Who painted the Mona Lisa and in what century was it painted?\"\n",
        "response = \"Leonardo da Vinci painted the Mona Lisa, and it was painted in the 15th century.\"\n",
        "reference = \"Leonardo da Vinci painted the Mona Lisa in the 16th century.\"\n",
        "context = [\n",
        "    \"The Mona Lisa is a famous portrait painted by Leonardo da Vinci. It is believed to have been started in the early 1500s. Some art historians date its completion to around 1519, placing it firmly in the 15th century according to certain periodizations.\"\n",
        "]\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input = query,\n",
        "    response = response,\n",
        "    reference = reference,\n",
        "    retrieved_contexts = context,\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "score = asyncio.run(relevant_noise_sensitivity.single_turn_ascore(sample))\n",
        "\n",
        "# Output the result\n",
        "print(f\"Relevant Noise Sensitivity Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4dSghELUNaP",
        "outputId": "ecf1fee1-7775-4d55-8a43-05e7dc22d798"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant Noise Sensitivity Score: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Irrelevant Noise Sensitivity**\n",
        "\n",
        "- Irrelevant Noise Sensitivity refers to the proportion of incorrect claims in a model’s response that are entailed (i.e., supported or implied) by irrelevant retrieved chunks to the toal number of response claims.\n",
        "\n",
        "- A irrelevant chunk is a chunk with no claims from the ground truth answer.\n",
        "\n",
        "- Formula is\n",
        "$$\n",
        "\\text{Irrelevant Noise Sensitivity} = \\frac{\\text{Number of incorrect claims in the model response entailed by irrelevant chunks}}{\\text{Total number of claims in the response}}\n",
        "$$"
      ],
      "metadata": {
        "id": "Q9-BPFwbWZCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import SingleTurnSample\n",
        "from ragas.metrics import NoiseSensitivity\n",
        "import asyncio\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Initialize the metric\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "irrelevant_noise_sensitivity = NoiseSensitivity(llm=evaluator_llm, mode=\"irrelevant\")\n",
        "\n",
        "# Define the test case\n",
        "query = \"Who wrote the novel 'Pride and Prejudice'?\"\n",
        "response = \"Charlotte Brontë wrote 'Pride and Prejudice,' and she is famous for 'Jane Eyre.'\"\n",
        "reference = \"Jane Austen wrote 'Pride and Prejudice.'\"\n",
        "context = [\n",
        "    \"Jane Austen published 'Pride and Prejudice' in 1813, a classic romance novel.\",\n",
        "    \"Charlotte Brontë, a renowned author, is best known for her novel 'Jane Eyre,' published in 1847.\"\n",
        "]\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input = query,\n",
        "    response = response,\n",
        "    reference = reference,\n",
        "    retrieved_contexts = context,\n",
        ")\n",
        "\n",
        "# Compute the metric\n",
        "score = asyncio.run(irrelevant_noise_sensitivity.single_turn_ascore(sample))\n",
        "\n",
        "# Output the result\n",
        "print(f\"Irrelevant Noise Sensitivity Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnzEPNyZWoH-",
        "outputId": "ccd8344c-905b-44c7-e22a-da4668ecef66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Irrelevant Noise Sensitivity Score: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG Evaluation using RAGAS - Full Example**"
      ],
      "metadata": {
        "id": "qDZfNPqO1BF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ragas import EvaluationDataset, evaluate\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    LLMContextRecall\n",
        ")"
      ],
      "metadata": {
        "id": "zFA7Nrpa1Ht9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataframe (replace with your actual dataframe)\n",
        "data = {\n",
        "    \"query\": [\"What is the capital of France?\", \"Who invented the telephone?\"],\n",
        "    \"reference\": [\"The capital of France is Paris.\", \"Alexander Graham Bell invented the telephone.\"],\n",
        "    \"response\": [\"The capital of France is Paris.\", \"The telephone was invented by Alexander Graham Bell.\"],\n",
        "    \"context\": [\n",
        "        [\"France is a country in Europe. Its capital is Paris.\"],\n",
        "        [\"Alexander Graham Bell was an inventor. He is credited with inventing the telephone.\"]\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "rtCJsJqF1QOi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data as a list of dictionaries for EvaluationDataset\n",
        "evaluation_data = [\n",
        "    {\n",
        "        \"user_input\": row[\"query\"],\n",
        "        \"reference\": row[\"reference\"],\n",
        "        \"response\": row[\"response\"],\n",
        "        \"retrieved_contexts\": row[\"context\"]\n",
        "    }\n",
        "    for _, row in df.iterrows()\n",
        "]\n",
        "\n",
        "# Create an EvaluationDataset object\n",
        "dataset = EvaluationDataset.from_list(evaluation_data)"
      ],
      "metadata": {
        "id": "BvjPXxS41Sn1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "95fP-Drx11W8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the metrics\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "context_recall = LLMContextRecall(llm=evaluator_llm)\n",
        "faithfulness = Faithfulness(llm=evaluator_llm)\n"
      ],
      "metadata": {
        "id": "eAMvMrOQ12zr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics to evaluate\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    context_recall\n",
        "]"
      ],
      "metadata": {
        "id": "0sdNfgGH1VrE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the metric scores\n",
        "results = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "65ac6df1c72e43afb6aaf2b617c4ea1a",
            "a0616a639cb944dcbc8812d5bb90268d",
            "5136edd84c8a4d01be57e738979d346b",
            "786e5393875b4be18054359f9fb95596",
            "3bce8baff8254198adf392c4f842fc80",
            "1587a4172f114deea27eff9edf8f95e8",
            "a946a9ff7d224a318387588662c69272",
            "bb033460c6c24d79b4b9cda0a29642a4",
            "2dd14828b9664a89b6e036d493e8c140",
            "c07c8767449346dea9bd94fe4ba1b327",
            "4f2ea84a58bd4f63808d9333269dc37e"
          ]
        },
        "id": "-c3b73PW2N5q",
        "outputId": "83b93e41-a340-46a4-ba4e-0b2fdf99d83b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65ac6df1c72e43afb6aaf2b617c4ea1a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2HsVyh02622",
        "outputId": "4e6f298a-88b6-44b7-8db1-a5fa9398eba7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'faithfulness': 1.0000, 'context_recall': 1.0000}\n"
          ]
        }
      ]
    }
  ]
}