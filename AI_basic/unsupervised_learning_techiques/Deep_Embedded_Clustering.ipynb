{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Embedded Clustering (DEC)\n",
        "### Definition\n",
        "Deep Embedded Clustering (DEC): an unsupervised algorithm that simultaneously learns feature representations optimal for clustering and performs cluster assignments. It iteratively refines clusters by optimizing a Kullback-Leibler (KL) divergence-based clustering loss, using an auxiliary target distribution derived from current high-confidence assignments to guide the learning of embeddings from a deep neural network, typically a pre-trained autoencoder's encoder part.\n",
        "\n",
        "---\n",
        "\n",
        "### Pertinent Equations\n",
        "\n",
        "1.  **Autoencoder (AE) for Pre-training:**\n",
        "*   Encoder: $$ \\mathbf{z}_i = f_{\\theta_e}(\\mathbf{x}_i) $$\n",
        "*   Decoder: $$ \\hat{\\mathbf{x}}_i = g_{\\theta_d}(\\mathbf{z}_i) $$\n",
        "*   Reconstruction Loss: $$ L_{AE} = \\frac{1}{N} \\sum_{i=1}^{N} ||\\mathbf{x}_i - g_{\\theta_d}(f_{\\theta_e}(\\mathbf{x}_i))||_2^2 $$\n",
        "Where $ \\mathbf{x}_i \\in \\mathbb{R}^D $ is the $i$-th input sample, $ \\mathbf{z}_i \\in \\mathbb{R}^d $ is its latent representation ($d < D$), $ f_{\\theta_e} $ and $ g_{\\theta_d} $ are the encoder and decoder networks with parameters $ \\theta_e $ and $ \\theta_d $ respectively.\n",
        "\n",
        "2.  **Soft Assignment (Student's t-distribution):**\n",
        "The probability $ q_{ij} $ of assigning sample $i$ to cluster $j$ is given by:\n",
        "$$ q_{ij} = \\frac{(1 + ||\\mathbf{z}_i - \\boldsymbol{\\mu}_j||^2 / \\alpha)^{-\\frac{\\alpha+1}{2}}}{\\sum_{j'=1}^{K}(1 + ||\\mathbf{z}_i - \\boldsymbol{\\mu}_{j'}||^2 / \\alpha)^{-\\frac{\\alpha+1}{2}}} $$\n",
        "Where $ \\mathbf{z}_i = f_{\\theta_e}(\\mathbf{x}_i) $ is the embedding of sample $i$, $ \\boldsymbol{\\mu}_j \\in \\mathbb{R}^d $ is the $j$-th cluster centroid, $ K $ is the number of clusters, and $ \\alpha $ is the degrees of freedom of the Student's t-distribution (typically $ \\alpha=1 $).\n",
        "\n",
        "3.  **Auxiliary Target Distribution $ P $:**\n",
        "To guide the learning process and prevent degenerate solutions, an auxiliary target distribution $ p_{ij} $ is computed:\n",
        "$$ p_{ij} = \\frac{q_{ij}^2 / f_j}{\\sum_{j'=1}^{K} q_{ij'}^2 / f_{j'}} $$\n",
        "Where $ f_j = \\sum_{i=1}^{N} q_{ij} $ is the soft cluster frequency (sum of probabilities for cluster $j$). This distribution emphasizes high-confidence assignments.\n",
        "\n",
        "4.  **Clustering Loss $ L_C $ (KL Divergence):**\n",
        "The clustering objective is to minimize the KL divergence between the soft assignment distribution $ Q = [q_{ij}] $ and the auxiliary target distribution $ P = [p_{ij}] $:\n",
        "$$ L_C = \\text{KL}(P||Q) = \\sum_{i=1}^{N} \\sum_{j=1}^{K} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} $$\n",
        "This loss is minimized with respect to the encoder parameters $ \\theta_e $ and cluster centroids $ \\boldsymbol{\\mu}_j $.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Principles\n",
        "\n",
        "*   **Joint Optimization:** Simultaneously learns low-dimensional feature representations and cluster assignments.\n",
        "*   **Self-Supervision via Auxiliary Distribution:** The target distribution $ P $ provides supervision generated from the model's own high-confidence predictions, iteratively improving cluster quality.\n",
        "*   **Student's t-distribution Kernel:** Used for soft assignments, providing heavier tails than a Gaussian, which helps separate dissimilar points and group similar ones.\n",
        "*   **Iterative Refinement:** Cluster assignments and feature representations are refined iteratively.\n",
        "*   **Initialization with Pre-trained AE:** The encoder part of a pre-trained autoencoder provides initial feature representations, and K-means on these initial embeddings provides initial cluster centroids.\n",
        "\n",
        "---\n",
        "\n",
        "### Detailed Concept Analysis\n",
        "\n",
        "#### 4.1 Data Pre-processing\n",
        "*   **Normalization/Standardization:** Input data $ \\mathbf{X} $ is typically normalized (e.g., pixel values to $[0,1]$ for images) or standardized (zero mean, unit variance) to facilitate stable training of the autoencoder.\n",
        "$$ \\mathbf{x}'_i = (\\mathbf{x}_i - \\boldsymbol{\\mu}_{\\text{data}}) / \\boldsymbol{\\sigma}_{\\text{data}} $$\n",
        "\n",
        "#### 4.2 Model Architecture\n",
        "\n",
        "*   **Phase 1: Autoencoder Pre-training**\n",
        "*   **Encoder $ f_{\\theta_e} $:** A deep neural network (e.g., MLP for tabular data, CNN for images) that maps input $ \\mathbf{x}_i $ to a lower-dimensional latent space $ \\mathbf{z}_i $.\n",
        "Example MLP encoder layer: $ \\mathbf{h}^{(l+1)} = \\sigma(\\mathbf{W}^{(l)}\\mathbf{h}^{(l)} + \\mathbf{b}^{(l)}) $, where $ \\mathbf{h}^{(0)} = \\mathbf{x}_i $ and $ \\mathbf{z}_i = \\mathbf{h}^{(L_e)} $.\n",
        "*   **Decoder $ g_{\\theta_d} $:** A network, often symmetric to the encoder, that reconstructs $ \\hat{\\mathbf{x}}_i $ from $ \\mathbf{z}_i $.\n",
        "Example MLP decoder layer: $ \\hat{\\mathbf{h}}^{(l+1)} = \\sigma(\\mathbf{W}'^{(l)}\\hat{\\mathbf{h}}^{(l)} + \\mathbf{b}'^{(l)}) $, where $ \\hat{\\mathbf{h}}^{(0)} = \\mathbf{z}_i $ and $ \\hat{\\mathbf{x}}_i = \\hat{\\mathbf{h}}^{(L_d)} $.\n",
        "*   **Training:** The AE is trained by minimizing $ L_{AE} $ using SGD or variants. This step learns an initial manifold representation.\n",
        "\n",
        "*   **Phase 2: Clustering with KL Divergence Optimization**\n",
        "*   **Initialization:**\n",
        "1.  Discard the decoder $ g_{\\theta_d} $.\n",
        "2.  Feed all data $ \\mathbf{X} $ through the pre-trained encoder $ f_{\\theta_e} $ to obtain initial embeddings $ \\mathbf{Z}^{(0)} = \\{f_{\\theta_e}(\\mathbf{x}_i)\\}_{i=1}^N $.\n",
        "3.  Apply K-means algorithm to $ \\mathbf{Z}^{(0)} $ to obtain initial cluster centroids $ \\{\\boldsymbol{\\mu}_j^{(0)}\\}_{j=1}^K $.\n",
        "*   **Iterative Optimization:**\n",
        "1.  **Compute Soft Assignments ($ Q $):** Given current embeddings $ \\mathbf{Z}^{(t)} $ (from $ f_{\\theta_e}^{(t)} $) and centroids $ \\boldsymbol{\\mu}_j^{(t)} $, compute $ q_{ij}^{(t)} $ using the Student's t-distribution kernel.\n",
        "2.  **Compute Target Distribution ($ P $):** Calculate $ p_{ij}^{(t)} $ based on $ q_{ij}^{(t)} $. This step is typically performed less frequently than gradient updates (e.g., every epoch).\n",
        "3.  **Compute Clustering Loss ($ L_C $):** Calculate $ L_C^{(t)} = \\text{KL}(P^{(t)}||Q^{(t)}) $.\n",
        "4.  **Update Parameters:** Update encoder parameters $ \\theta_e $ and cluster centroids $ \\boldsymbol{\\mu}_j $ by computing gradients of $ L_C^{(t)} $ and applying an optimizer (e.g., SGD).\n",
        "*   Gradient w.r.t. $ \\mathbf{z}_i $:\n",
        "$$ \\frac{\\partial L_C}{\\partial \\mathbf{z}_i} = \\sum_j \\frac{\\partial L_C}{\\partial q_{ij}} \\frac{\\partial q_{ij}}{\\partial \\mathbf{z}_i} = \\sum_j p_{ij} (-\\frac{1}{q_{ij}}) \\frac{\\partial q_{ij}}{\\partial \\mathbf{z}_i} $$\n",
        "$$ \\frac{\\partial q_{ij}}{\\partial ||\\mathbf{z}_i - \\boldsymbol{\\mu}_j||^2} = -\\frac{\\alpha+1}{2\\alpha} q_{ij} \\left( (1 + ||\\mathbf{z}_i - \\boldsymbol{\\mu}_j||^2 / \\alpha)^{-1} - \\sum_{j'} q_{ij'} (1 + ||\\mathbf{z}_i - \\boldsymbol{\\mu}_{j'}||^2 / \\alpha)^{-1} \\right) $$\n",
        "$$ \\frac{\\partial ||\\mathbf{z}_i - \\boldsymbol{\\mu}_j||^2}{\\partial \\mathbf{z}_i} = 2(\\mathbf{z}_i - \\boldsymbol{\\mu}_j) $$\n",
        "*   Gradient w.r.t. $ \\boldsymbol{\\mu}_j $: Similar derivation.\n",
        "*   Gradients w.r.t. $ \\theta_e $ are obtained via backpropagation through $ \\mathbf{z}_i = f_{\\theta_e}(\\mathbf{x}_i) $.\n",
        "5.  Repeat steps 1-4 until convergence or a maximum number of iterations.\n",
        "\n",
        "#### 4.3 Post-Training Procedures\n",
        "*   **Cluster Assignment:** Assign each sample $ \\mathbf{x}_i $ to cluster $ k^* = \\arg\\max_j q_{ij} $ using the final learned $ q_{ij} $ values.\n",
        "*   **Centroid Refinement (Optional):** After convergence, one final K-means iteration can be performed on the final embeddings $ \\mathbf{Z} $ if centroids were updated via gradient descent and might not be exact means.\n",
        "\n",
        "---\n",
        "\n",
        "### Importance\n",
        "\n",
        "*   **End-to-End Clustering:** Integrates feature learning directly with the clustering objective, leading to representations more suitable for clustering than generic pre-trained features.\n",
        "*   **Non-linear Manifold Learning:** Capable of capturing complex, non-linear structures in the data via deep neural networks.\n",
        "*   **Improved Performance:** Often achieves state-of-the-art clustering performance on various benchmarks by avoiding suboptimal feature spaces.\n",
        "*   **Scalability:** While computationally more intensive than traditional methods, it scales better to high-dimensional data due to dimensionality reduction.\n",
        "\n",
        "---\n",
        "\n",
        "### Pros versus Cons\n",
        "\n",
        "**Pros:**\n",
        "*   Learns cluster-specific feature representations.\n",
        "*   No explicit assumptions on cluster shapes beyond what the t-distribution kernel and deep features can model.\n",
        "*   Can handle high-dimensional raw data (e.g., images, text embeddings).\n",
        "*   Often outperforms methods that separate feature learning and clustering.\n",
        "\n",
        "**Cons:**\n",
        "*   Performance is sensitive to the quality of autoencoder pre-training and initialization of centroids.\n",
        "*   Requires specifying the number of clusters $ K $ beforehand.\n",
        "*   Optimization can be unstable; convergence not always guaranteed.\n",
        "*   Computationally more expensive than shallow clustering methods like K-means.\n",
        "*   The target distribution $ P $ can lead to local optima if initial clustering is poor.\n",
        "\n",
        "---\n",
        "\n",
        "### Cutting-Edge Advances\n",
        "\n",
        "*   **Improved DEC (IDEC):** Incorporates the AE reconstruction loss $ L_{AE} $ into the clustering phase, training jointly: $ L = L_C + \\gamma L_{AE} $. This helps preserve local structure and prevent feature space distortion.\n",
        "$$ L_{\\text{IDEC}} = \\sum_{i=1}^{N} \\sum_{j=1}^{K} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} + \\gamma \\sum_{i=1}^{N} ||\\mathbf{x}_i - g_{\\theta_d}(f_{\\theta_e}(\\mathbf{x}_i))||_2^2 $$\n",
        "*   **Deep Convolutional Embedded Clustering (DCEC):** Tailors DEC for image data using convolutional autoencoders.\n",
        "*   **Variational Deep Embedding (VaDE):** A probabilistic generative model that combines GMMs with VAEs for clustering.\n",
        "*   **Deep Clustering Network (DCN):** Combines K-means with autoencoder in an end-to-end fashion, but with a K-means-like objective rather than KL divergence.\n",
        "*   **Self-Supervised Deep Clustering:** Leveraging contrastive learning principles (e.g., SwAV, SimCLR adaptations) to learn representations that are then clustered, or integrating contrastive losses directly into the clustering framework.\n",
        "*   **Attention Mechanisms:** Incorporating attention in the encoder to focus on salient features for clustering.\n",
        "*   **Graph-based Deep Clustering:** Using Graph Neural Networks (GNNs) as encoders, and incorporating graph-based regularizers or objectives. Example: DAEGC (Deep Attentional Embedded Graph Clustering).\n",
        "$$ L_{\\text{DAEGC}} = L_C + \\gamma L_{AE} + \\lambda L_{\\text{graph\\_reg}} $$\n",
        "where $ L_{\\text{graph\\_reg}} $ might be a graph reconstruction loss or a smoothness prior on the graph.\n",
        "\n",
        "---\n",
        "\n",
        "### Training Pseudo-Algorithm\n",
        "\n",
        "```pseudo\n",
        "Input: Data X, number of clusters K, AE architecture, learning rate η_AE, η_C, AE epochs T_AE, clustering epochs T_C, P update interval U_P, α=1\n",
        "Output: Cluster assignments C, Encoder parameters θ_e, Centroids {μ_j}\n",
        "\n",
        "// Phase 1: Autoencoder Pre-training\n",
        "Initialize AE parameters θ_e, θ_d\n",
        "for t_ae = 1 … T_AE:\n",
        "  for each minibatch X_b ⊂ X:\n",
        "    Z_b ← f_θe(X_b)\n",
        "    X_hat_b ← g_θd(Z_b)\n",
        "    L_AE ← ||X_b - X_hat_b||_2^2\n",
        "    Update θ_e, θ_d using SGD: θ ← θ - η_AE ∇_θ L_AE\n",
        "\n",
        "// Phase 2: DEC Clustering\n",
        "Store pre-trained encoder parameters θ_e^*\n",
        "Z_all ← f_θe^*(X) // Get all initial embeddings\n",
        "{μ_j}_(0) ← KMeans(Z_all, K) // Initialize centroids\n",
        "\n",
        "for t_c = 1 … T_C:\n",
        "  // (Optional, less frequent) Update target distribution P\n",
        "  if t_c % U_P == 1:\n",
        "    For all x_i ∈ X:\n",
        "      z_i ← f_θe(x_i) // Current embeddings\n",
        "      For j = 1 … K:\n",
        "        q_ij ← (1 + ||z_i - μ_j||^2/α)^(-(α+1)/2) / Σ_k'(1 + ||z_i - μ_k'||^2/α)^(-(α+1)/2)\n",
        "    For j = 1 … K: f_j ← Σ_i q_ij\n",
        "    For all x_i ∈ X, j = 1 … K:\n",
        "      p_ij ← (q_ij^2 / f_j) / Σ_k'(q_ik'^2 / f_k')\n",
        "    P_target ← {p_ij}\n",
        "\n",
        "  // Optimize L_C\n",
        "  for each minibatch X_b ⊂ X:\n",
        "    Z_b ← f_θe(X_b)\n",
        "    Q_b ← compute_soft_assignments(Z_b, {μ_j}, α) // q_ij for batch\n",
        "    P_target_b ← corresponding subset of P_target\n",
        "    L_C_b ← Σ_i Σ_j p_ij_target_b log(p_ij_target_b / q_ij_b)\n",
        "    \n",
        "    // Compute gradients ∇_θe L_C_b and ∇_μj L_C_b\n",
        "    Update θ_e using SGD: θ_e ← θ_e - η_C ∇_θe L_C_b\n",
        "    Update {μ_j} using SGD: μ_j ← μ_j - η_C ∇_μj L_C_b\n",
        "\n",
        "C ← assign_clusters_from_final_Q(f_θe(X), {μ_j})\n",
        "Return C, θ_e, {μ_j}\n",
        "```\n",
        "**Mathematical Justification:**\n",
        "*   AE Pre-training: Minimizes reconstruction error, forcing the encoder to learn a compressed representation capturing salient data variations.\n",
        "*   Clustering Phase: Minimizes KL(P||Q). This encourages Q to match P. Since P is constructed to sharpen Q and emphasize high-confidence assignments, this process iteratively refines clusters by pushing embeddings towards their assigned centroids and making assignments more confident. Gradients are derived using the chain rule.\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluation Phase\n",
        "\n",
        "#### 9.1 Metrics (Requires ground-truth labels $ \\mathbf{y} $)\n",
        "*   **Clustering Accuracy (ACC):**\n",
        "    $$ \\text{ACC} = \\max_{m \\in \\mathcal{P}} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(y_i = m(c_i)) $$\n",
        "    Where $ y_i $ is the true label, $ c_i $ is the assigned cluster label for sample $i$, and $ m $ is the best mapping from cluster labels to true labels found by the Hungarian algorithm over all permutations $ \\mathcal{P} $.\n",
        "*   **Normalized Mutual Information (NMI):**\n",
        "    $$ \\text{NMI}(Y, C) = \\frac{I(Y; C)}{\\sqrt{H(Y)H(C)}} $$\n",
        "    Where $ I(Y; C) $ is the mutual information between true labels $ Y $ and cluster assignments $ C $, and $ H(\\cdot) $ is entropy. $ \\text{NMI} \\in [0,1] $.\n",
        "*   **Adjusted Rand Index (ARI):**\n",
        "    $$ \\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]} $$\n",
        "    Where RI is the Rand Index: $ \\text{RI} = (TP+TN)/(TP+TN+FP+FN) $. ARI corrects for chance and has an expected value of 0 for random clustering. $ \\text{ARI} \\in [-1,1] $.\n",
        "\n",
        "#### 9.2 Loss Functions\n",
        "*   **Pre-training Loss:** $ L_{AE} $ (Mean Squared Error typically).\n",
        "*   **Clustering Loss:** $ L_C = \\text{KL}(P||Q) $.\n",
        "*   **IDEC Combined Loss:** $ L_{\\text{IDEC}} = L_C + \\gamma L_{AE} $.\n",
        "\n",
        "#### 9.3 Metrics (SOTA)\n",
        "(Values are approximate and depend on specific AE architecture and dataset variations.)\n",
        "*   **MNIST:**\n",
        "    *   DEC: ACC ≈ 84.3%, NMI ≈ 80.1%\n",
        "    *   IDEC: ACC ≈ 88.2%, NMI ≈ 86.7%\n",
        "    *   Recent GNN/Contrastive methods: ACC > 95-98%\n",
        "*   **Reuters-10k (TF-IDF features):**\n",
        "    *   DEC: ACC ≈ 75.1%, NMI ≈ 47.2%\n",
        "    *   IDEC: ACC ≈ 77.9%, NMI ≈ 49.9%\n",
        "*   **STL-10 (Image dataset):**\n",
        "    *   DCEC: ACC ≈ 47.9%, NMI ≈ 36.4%\n",
        "    *   More recent self-supervised methods (e.g., SCAN, PICA): ACC > 70-80%\n",
        "\n",
        "**Domain-Specific Metrics:**\n",
        "*   For document clustering: Purity, F-measure.\n",
        "*   For image segmentation (if clustering pixels): Intersection over Union (IoU) per segment.\n",
        "\n",
        "**Best Practices & Potential Pitfalls:**\n",
        "*   **Pitfall:** Sensitivity to $K$. Use domain knowledge or heuristics (e.g., silhouette score on initial AE embeddings) to estimate $K$.\n",
        "*   **Best Practice:** Robust AE pre-training is crucial. Use appropriate architectures (e.g., CNNs for images, MLPs for flat features).\n",
        "*   **Pitfall:** Vanishing gradients if $ q_{ij} $ values become too small. The KL divergence formulation mitigates this to some extent compared to directly optimizing centroids with squared error in embedding space.\n",
        "*   **Best Practice:** Gradual increase of confidence in $ P $. The squaring mechanism in $ P $ helps, but too aggressive updates can lead to poor local optima.\n",
        "*   **Reproducibility:** Careful initialization of AE weights and K-means centroids is important for reproducibility. Standardize random seeds.\n",
        "*   **Robustness:** The choice of $ \\alpha $ (degrees of freedom) in the t-distribution can impact performance. $ \\alpha=1 $ is common, but tuning might be beneficial."
      ],
      "metadata": {
        "id": "JSuSLoZwqduD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep InfoMax (DIM)\n",
        "### I. Definition\n",
        "\n",
        "Deep InfoMax (DIM) is a self-supervised representation learning method that learns feature representations by maximizing the mutual information (MI) between global features of an input (e.g., an entire image) and local features from different parts of the same input (e.g., patches of the image). The core idea is that representations are rich if local properties can be inferred from a global summary, and vice-versa. DIM often employs a discriminator-based approach to estimate and maximize a lower bound on MI.\n",
        "\n",
        "### II. Pertinent Equations and Mathematical Concepts\n",
        "\n",
        "**A. Mutual Information (MI)**\n",
        "MI measures the statistical dependence between two random variables $X$ and $Y$.\n",
        "$$ I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} $$\n",
        "In terms of Kullback-Leibler (KL) divergence:\n",
        "$$ I(X;Y) = D_{KL}(P_{XY} || P_X \\otimes P_Y) $$\n",
        "where $P_{XY}$ is the joint probability distribution and $P_X \\otimes P_Y$ is the product of the marginal distributions.\n",
        "\n",
        "**B. MI Estimators**\n",
        "Estimating MI directly from samples is hard, especially for high-dimensional continuous variables. DIM utilizes neural network-based estimators for lower bounds of MI.\n",
        "\n",
        "1.  **Donsker-Varadhan Representation (KL-divergence based)**\n",
        "A lower bound on $D_{KL}(P||Q)$ can be expressed as:\n",
        "$$ D_{KL}(P||Q) \\ge \\sup_{T: \\Omega \\to \\mathbb{R}} \\left( \\mathbb{E}_{x \\sim P}[T(x)] - \\mathbb{E}_{x \\sim Q}[e^{T(x)-1}] \\right) $$\n",
        "where $T$ is a function (neural network) mapping from the sample space $\\Omega$ to scalars.\n",
        "\n",
        "2.  **Jensen-Shannon Divergence (JSD) Estimator**\n",
        "The JSD between two distributions $P$ and $Q$ is:\n",
        "$$ D_{JSD}(P||Q) = \\frac{1}{2} D_{KL}(P||M) + \\frac{1}{2} D_{KL}(Q||M) $$\n",
        "where $M = \\frac{1}{2}(P+Q)$.\n",
        "A lower bound on MI based on JSD, often used in DIM, is:\n",
        "$$ I(X;Y) \\ge \\hat{I}_{JSD}(X;Y) = \\mathbb{E}_{P_{XY}}[-\\text{sp}(-T(x,y))] - \\mathbb{E}_{P_X \\otimes P_Y}[\\text{sp}(T(x,y))] $$\n",
        "where $T(x,y)$ is a discriminator network outputting a scalar score (logit), and $\\text{sp}(z) = \\log(1+e^z)$ is the softplus function. $P_{XY}$ represents samples $(x,y)$ drawn from the joint distribution (positive pairs), and $P_X \\otimes P_Y$ represents samples drawn from the product of marginals (negative pairs, where $x$ and $y$ are independent).\n",
        "\n",
        "3.  **Noise Contrastive Estimation (InfoNCE)**\n",
        "Another popular MI estimator, related to Noise Contrastive Estimation:\n",
        "$$ I(X;Y) \\ge \\hat{I}_{NCE}(X;Y) = \\mathbb{E}_{P_{XY}} \\left[ \\log \\frac{e^{T(x,y)}}{\\frac{1}{K} \\sum_{k=1}^K e^{T(x, y'_k)}} \\right] $$\n",
        "where $y'_k$ are $K$ negative samples drawn from $P_Y$, and $T(x,y)$ is a critic function scoring the compatibility of $x$ and $y$. DIM primarily uses the JSD-based estimator.\n",
        "\n",
        "### III. Key Principles of Deep InfoMax\n",
        "\n",
        "*   **Unsupervised/Self-Supervised Learning**: Learns representations from unlabeled data.\n",
        "*   **Mutual Information Maximization**: The core objective is to maximize MI between different views or scales of the input data.\n",
        "*   **Local-Global Information**: Focuses on maximizing MI between local features (e.g., from image patches) and a global summary feature vector of the entire input.\n",
        "*   **Encoder-Discriminator Architecture**: Employs an encoder to generate features and one or more discriminator networks to estimate and help maximize the MI lower bounds.\n",
        "*   **Statistical Independence**: The features from different local patches are encouraged to be statistically independent when conditioned on the global representation.\n",
        "\n",
        "### IV. Detailed Concept Analysis\n",
        "\n",
        "**A. Model Architecture**\n",
        "\n",
        "1.  **Encoder Network ($E$)**\n",
        "    *   **Input**: Raw input data $x$ (e.g., an image).\n",
        "*   **Function**: Maps input $x$ to a global feature vector $y_{global}$ and a set of local feature maps/vectors $\\{M_j\\}$.\n",
        "*   **Structure**: Typically a convolutional neural network (CNN).\n",
        "*   Let $E_{\\theta}$ be the encoder parameterized by $\\theta$.\n",
        "*   The global feature vector $y_{global} = E_{global}(x; \\theta)$ is usually obtained from the later layers of the CNN, possibly after global pooling.\n",
        "For a CNN, $y_{global} = \\text{Flatten}(\\text{Pool}(\\text{Conv}_L(...\\text{Conv}_1(x))))$.\n",
        "*   The local features $M = \\{M_j\\}$ are feature maps from an intermediate layer of $E_{\\theta}$, e.g., $M = \\text{Conv}_k(...\\text{Conv}_1(x))$. Each $M_j$ corresponds to a spatial location in the feature map.\n",
        "*   **Generic Convolutional Layer Equation**:\n",
        "$$ (h_k)_{u,v} = \\sigma \\left( \\sum_{c=1}^{C_{in}} \\sum_{i=0}^{K_h-1} \\sum_{j=0}^{K_w-1} (W_k)_{c,i,j} \\cdot (I)_{c, u \\cdot S + i, v \\cdot S + j} + (b_k) \\right) $$\n",
        "where $h_k$ is the $k$-th output feature map, $(I)$ is the input feature map tensor, $W_k$ is the $k$-th filter kernel, $b_k$ is its bias, $S$ is the stride, $\\sigma$ is an activation function (e.g., ReLU: $\\sigma(z) = \\max(0,z)$).\n",
        "\n",
        "2.  **Local/Global Discriminator Network ($D_{LG}$)**\n",
        "*   **Input**: Pairs of (local feature $M_j$, global feature $y_{global}$) or (local feature $M_j$, \"fake\" global feature $y'_{global}$ from a different input).\n",
        "*   **Function**: A binary classifier $D_{LG, \\psi}$ (parameterized by $\\psi$) that outputs a score (logit) indicating whether the local and global features come from the same input.\n",
        "        $s_{j} = D_{LG}(M_j, y_{global})$.\n",
        "*   **Structure**: Can be a multi-layer perceptron (MLP) or a small CNN that combines $M_j$ and $y_{global}$. For instance, $y_{global}$ might be replicated and concatenated with $M_j$ before being passed through convolutional and fully connected layers.\n",
        "$$ D_{LG}(M_j, y_{global}) = \\text{FC}_L(... \\text{FC}_1(\\text{Concat}(M_j, \\text{Transform}(y_{global})))) $$\n",
        "\n",
        "3.  **(Optional) Prior Discriminator Network ($D_P$)**\n",
        "*   **Input**: Global feature vectors $y_{global}$ produced by the encoder, and samples $z$ drawn from a desired prior distribution $P_Z(z)$ (e.g., unit Gaussian $\\mathcal{N}(0,I)$ or uniform $U[-1,1]$).\n",
        "*   **Function**: A binary classifier $D_{P, \\phi}$ (parameterized by $\\phi$) that distinguishes $y_{global}$ from samples $z$.\n",
        "*   **Structure**: Typically an MLP.\n",
        "$$ D_P(y) = \\text{FC}_K(... \\text{FC}_1(y)) $$\n",
        "\n",
        "**B. Mathematical Formulation of DIM**\n",
        "\n",
        "1.  **Local-Global Mutual Information Maximization Objective ($\\mathcal{L}_{LG}$)**\n",
        "DIM aims to maximize the MI between each local feature map $M_j$ (from a set of $N_{patches}$ local regions) and the global summary vector $y_{global}$. Using the JSD-based MI estimator:\n",
        "$$ \\hat{I}(M_j; y_{global}) = \\mathbb{E}_{(x \\sim \\mathcal{X})} [-\\text{sp}(-D_{LG}(M_j(x), y_{global}(x)))] - \\mathbb{E}_{(x \\sim \\mathcal{X}, x' \\sim \\mathcal{X})} [\\text{sp}(D_{LG}(M_j(x), y_{global}(x')))] $$\n",
        "where $M_j(x)$ and $y_{global}(x)$ are derived from the same input $x$, while $y_{global}(x')$ is derived from a different input $x'$.\n",
        "The overall local-global MI objective to be maximized by both encoder $E$ and discriminator $D_{LG}$ is the sum over all local patches:\n",
        "$$ \\mathcal{J}_{LG}(E, D_{LG}) = \\frac{1}{N_{samples}} \\sum_{k=1}^{N_{samples}} \\frac{1}{N_{patches}} \\sum_{j=1}^{N_{patches}} \\hat{I}(M_j(x_k); y_{global}(x_k)) $$\n",
        "The loss function to be minimized is $L_{LG} = -\\mathcal{J}_{LG}(E, D_{LG})$.\n",
        "\n",
        "2.  **(Optional) Prior Matching Objective ($\\mathcal{L}_P$)**\n",
        "To encourage the distribution of global features $P_Y(y_{global})$ to match a simple prior $P_Z(z)$. This also uses a JSD-based divergence estimator (similar to the MI estimator):\n",
        "$$ \\hat{D}_{JSD}(P_Y || P_Z) = \\mathbb{E}_{y_{global} \\sim P_Y} [-\\text{sp}(-D_P(y_{global}))] - \\mathbb{E}_{z \\sim P_Z} [\\text{sp}(D_P(z))] $$\n",
        "The encoder $E$ aims to minimize this divergence (making $P_Y$ similar to $P_Z$), while the prior discriminator $D_P$ aims to maximize it (distinguishing $P_Y$ from $P_Z$).\n",
        "*   Loss for $E$: $L_{P,E} = \\hat{D}_{JSD}(P_Y || P_Z)$.\n",
        "*   Loss for $D_P$: $L_{P,D_P} = -\\hat{D}_{JSD}(P_Y || P_Z)$.\n",
        "\n",
        "3.  **Total DIM Objective ($L_{DIM\\_total}$)**\n",
        "The encoder parameters $\\theta$ are updated to minimize:\n",
        "$$ L_{E\\_total} = L_{LG} + \\beta L_{P,E} = -\\mathcal{J}_{LG}(E, D_{LG}) + \\beta \\hat{D}_{JSD}(P_Y || P_Z) $$\n",
        "The local-global discriminator parameters $\\psi$ are updated to minimize:\n",
        "$$ L_{D_{LG}} = - \\mathcal{J}_{LG}(E, D_{LG}) $$\n",
        "The prior discriminator parameters $\\phi$ are updated to minimize:\n",
        "$$ L_{D_P} = - \\hat{D}_{JSD}(P_Y || P_Z) $$\n",
        "where $\\beta$ is a hyperparameter weighting the prior matching term.\n",
        "\n",
        "**C. Pre-processing Steps**\n",
        "*   **Normalization**: Input data (e.g., images) are typically normalized:\n",
        "$$ x_{norm} = \\frac{x - \\mu}{\\sigma} $$\n",
        "where $\\mu$ and $\\sigma$ are the mean and standard deviation of the training dataset, often per channel.\n",
        "*   **Data Augmentation**: Standard augmentations like random crops, resizing, and horizontal flips might be applied, though DIM's original formulation is less reliant on strong augmentation compared to later contrastive methods.\n",
        "\n",
        "**D. Post-training Procedures**\n",
        "\n",
        "1.  **Linear Evaluation**\n",
        "    *   The learned encoder $E$ is frozen.\n",
        "    *   A linear classifier (e.g., logistic regression or a single fully-connected layer) is trained on top of the extracted features $y_{global} = E(x)$ using labeled data from a downstream task.\n",
        "    *   The performance of this linear classifier measures the quality of the learned representations.\n",
        "    *   If $W_{linear}$ and $b_{linear}$ are the weights and bias of the linear classifier, the predictions are $\\hat{p} = \\text{softmax}(W_{linear}^T y_{global} + b_{linear})$.\n",
        "    *   The classifier is trained by minimizing a cross-entropy loss on the labeled dataset.\n",
        "\n",
        "2.  **Fine-tuning**\n",
        "    *   The pre-trained encoder $E$ is used as initialization for a supervised model on a downstream task.\n",
        "    *   All parameters of $E$ (or a subset) are updated (fine-tuned) during supervised training.\n",
        "\n",
        "### V. Training Pseudo-algorithm\n",
        "\n",
        "**A. Initialization**\n",
        "1.  Initialize encoder parameters $\\theta$.\n",
        "2.  Initialize local-global discriminator parameters $\\psi$.\n",
        "3.  If using prior matching, initialize prior discriminator parameters $\\phi$.\n",
        "4.  Choose optimizers (e.g., Adam) for $E$, $D_{LG}$, and $D_P$.\n",
        "\n",
        "**B. Training Loop**\n",
        "- For `epoch` from 1 to `N_EPOCHS`:\n",
        "  - For each mini-batch of input data $\\{x^{(k)}\\}_{k=1}^{B}$:\n",
        "    1.  **Data Sampling and Pre-processing**: Apply normalization and any augmentations to $\\{x^{(k)}\\}$.\n",
        "    2.  **Feature Extraction (Encoder Forward Pass)**:\n",
        "        For each $x^{(k)}$ in the mini-batch:\n",
        "        *   Obtain global feature vector: $y_{global}^{(k)} = E_{global}(x^{(k)}; \\theta)$.\n",
        "        *   Obtain local feature maps: $M^{(k)} = E_{local}(x^{(k)}; \\theta)$, from which individual local features $\\{M_j^{(k)}\\}$ are derived.\n",
        "\n",
        "    3.  **Discriminator Training ($D_{LG}$ and $D_P$)**:\n",
        "        *   **Local-Global Discriminator $D_{LG}$ Update**:\n",
        "            *   Construct positive pairs: $(M_j^{(k)}, y_{global}^{(k)})$ for $j=1...N_{patches}$.\n",
        "            *   Construct negative pairs: $(M_j^{(k)}, y_{global}^{(l)})$ where $k \\neq l$ (i.e., global feature from a different sample in the batch).\n",
        "            *   Calculate scores for positive pairs: $s_{pos,j}^{(k)} = D_{LG}(M_j^{(k)}, y_{global}^{(k)}; \\psi)$.\n",
        "            *   Calculate scores for negative pairs: $s_{neg,j}^{(k,l)} = D_{LG}(M_j^{(k)}, y_{global}^{(l)}; \\psi)$.\n",
        "            *   Compute $D_{LG}$ loss (aiming to maximize $\\mathcal{J}_{LG}$, so minimize $-\\mathcal{J}_{LG}$):\n",
        "              $$ L_{D_{LG}} = - \\frac{1}{B \\cdot N_{patches}} \\sum_{k=1}^B \\sum_{j=1}^{N_{patches}} \\left( [-\\text{sp}(-s_{pos,j}^{(k)})] - \\frac{1}{B-1}\\sum_{l \\neq k} [\\text{sp}(s_{neg,j}^{(k,l)})] \\right) $$\n",
        "            *   Update $\\psi$: $\\psi \\leftarrow \\psi - \\eta_{D_{LG}} \\nabla_{\\psi} L_{D_{LG}}$.\n",
        "\n",
        "        *   **(Optional) Prior Discriminator $D_P$ Update**:\n",
        "            *   Sample $B$ vectors $\\{z^{(k)}\\}_{k=1}^B$ from the prior distribution $P_Z(z)$.\n",
        "            *   Calculate scores for encoded features: $s_{enc}^{(k)} = D_P(y_{global}^{(k)}; \\phi)$.\n",
        "            *   Calculate scores for prior samples: $s_{prior}^{(k)} = D_P(z^{(k)}; \\phi)$.\n",
        "            *   Compute $D_P$ loss (aiming to maximize $\\hat{D}_{JSD}$, so minimize $-\\hat{D}_{JSD}$):\n",
        "              $$ L_{D_P} = - \\frac{1}{B} \\sum_{k=1}^B \\left( [-\\text{sp}(-s_{enc}^{(k)})] - [\\text{sp}(s_{prior}^{(k)})] \\right) $$\n",
        "            *   Update $\\phi$: $\\phi \\leftarrow \\phi - \\eta_{D_P} \\nabla_{\\phi} L_{D_P}$.\n",
        "\n",
        "    4.  **Encoder Training ($E$)**:\n",
        "        *   Using the current $D_{LG}$ and $D_P$.\n",
        "        *   Re-evaluate scores for positive pairs: $s_{pos,j}^{(k)} = D_{LG}(M_j^{(k)}, y_{global}^{(k)}; \\psi)$.\n",
        "        *   Re-evaluate scores for negative pairs using shuffled global features: $s_{neg,j}^{(k,l)} = D_{LG}(M_j^{(k)}, y_{global}^{(l)}; \\psi)$.\n",
        "        *   Compute local-global MI term for encoder (aiming to maximize $\\mathcal{J}_{LG}$, so minimize $-\\mathcal{J}_{LG}$):\n",
        "            $$ L_{LG,E} = - \\frac{1}{B \\cdot N_{patches}} \\sum_{k=1}^B \\sum_{j=1}^{N_{patches}} \\left( [-\\text{sp}(-s_{pos,j}^{(k)})] - \\frac{1}{B-1}\\sum_{l \\neq k} [\\text{sp}(s_{neg,j}^{(k,l)})] \\right) $$\n",
        "        *   **(Optional) Prior matching term for encoder**:\n",
        "            *   Re-evaluate scores for encoded features: $s_{enc}^{(k)} = D_P(y_{global}^{(k)}; \\phi)$.\n",
        "            *   Sample $B$ vectors $\\{z^{(k)}\\}_{k=1}^B$ from $P_Z(z)$ and get $s_{prior}^{(k)} = D_P(z^{(k)}; \\phi)$. (Prior samples don't depend on encoder).\n",
        "            *   Compute prior matching loss for $E$ (aiming to minimize $\\hat{D}_{JSD}$):\n",
        "              $$ L_{P,E} = \\frac{1}{B} \\sum_{k=1}^B \\left( [-\\text{sp}(-s_{enc}^{(k)})] - [\\text{sp}(s_{prior}^{(k)})] \\right) $$\n",
        "        *   Compute total encoder loss: $L_{E\\_total} = L_{LG,E} + \\beta L_{P,E}$.\n",
        "        *   Update $\\theta$: $\\theta \\leftarrow \\theta - \\eta_E \\nabla_{\\theta} L_{E\\_total}$.\n",
        "\n",
        "*(Note: The updates for discriminators and encoder can be simultaneous or alternating. The exact formulation ensures $E$ and $D_{LG}$ co-adapt to maximize the JSD-based MI estimate, and $E$ adapts to fool $D_P$ while $D_P$ adapts to distinguish.)*\n",
        "\n",
        "### VI. Evaluation Phase\n",
        "\n",
        "**A. Loss Functions (During Training)**\n",
        "*   **Local-Global MI Loss (JSD-based)**: $L_{LG}$ (minimized by both $E$ and $D_{LG}$, effectively maximizing the MI lower bound).\n",
        "*   **Prior Matching Loss (JSD-based)**: $L_P$ (minimized by $E$, maximized by $D_P$).\n",
        "These are constructed using the softplus function $\\text{sp}(z) = \\log(1+e^z)$ and discriminator outputs.\n",
        "For practical implementation, these are often translated into binary cross-entropy (BCE) like losses for the discriminators. For $D_{LG}$:\n",
        "$$ L_{D_{LG}}^{BCE} = - \\mathbb{E}_{(M_j, y_{global}) \\sim P_{joint}}[\\log \\sigma(D_{LG}(M_j, y_{global}))] - \\mathbb{E}_{(M_j, y'_{global}) \\sim P_{marginals}}[\\log(1-\\sigma(D_{LG}(M_j, y'_{global})))] $$\n",
        "where $\\sigma(z) = 1/(1+e^{-z})$ is the sigmoid function. The encoder is then trained to maximize $\\mathbb{E}_{(M_j, y_{global}) \\sim P_{joint}}[\\log \\sigma(D_{LG}(M_j, y_{global}))]$. The JSD formulation is generally preferred for stability.\n",
        "\n",
        "**B. Evaluation Metrics (SOTA)**\n",
        "Standard evaluation for self-supervised methods involves assessing feature quality on downstream tasks.\n",
        "\n",
        "1.  **Linear Classification Accuracy**:\n",
        "    *   **Definition**: A linear classifier is trained on top of frozen features extracted by the pre-trained encoder. Accuracy is evaluated on a held-out test set.\n",
        "    *   **Equation**:\n",
        "        $$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$\n",
        "    *   Common benchmarks: CIFAR-10, ImageNet.\n",
        "\n",
        "2.  **Transfer Learning Performance**:\n",
        "    *   **Definition**: The pre-trained encoder is fine-tuned on downstream tasks like object detection or semantic segmentation. Performance is measured using task-specific metrics.\n",
        "    *   Examples: Mean Average Precision (mAP) for object detection, mean Intersection over Union (mIoU) for segmentation.\n",
        "\n",
        "**C. Domain-Specific Metrics**\n",
        "If DIM is applied to other domains (e.g., graphs, audio, text), metrics relevant to those domains are used:\n",
        "*   **Graphs**: Node classification accuracy, link prediction AUC.\n",
        "*   **Audio**: Speaker/sound event classification accuracy.\n",
        "*   **NLP**: Performance on GLUE benchmark tasks after fine-tuning.\n",
        "\n",
        "### VII. Importance and Significance\n",
        "\n",
        "*   **Pioneering Self-Supervised Method**: DIM was one of the early and influential methods demonstrating the power of MI maximization for unsupervised representation learning.\n",
        "*   **Local-Global MI Principle**: Introduced a strong learning signal by focusing on the relationship between local parts and the global context of data.\n",
        "*   **Theoretical Grounding**: Leveraged information-theoretic principles, providing a more formal basis for representation learning compared to some heuristic approaches.\n",
        "*   **Influence on Subsequent Work**: Inspired many follow-up methods in contrastive learning and MI-based representation learning (e.g., CPC, AMDIM, InfoNCE-based methods).\n",
        "\n",
        "### VIII. Pros and Cons\n",
        "\n",
        "**A. Pros**\n",
        "*   **Strong Theoretical Basis**: Grounded in information theory (mutual information maximization).\n",
        "*   **Rich Feature Hierarchies**: Learns representations that capture both local details and global semantic content.\n",
        "*   **Versatility**: Applicable to various data modalities (images, video, potentially graphs, etc.) with appropriate encoder architectures.\n",
        "*   **No Explicit Negative Sampling for Local Features**: Compared to some contrastive methods, the local-global structure provides implicit negative examples through features from different spatial locations or different images.\n",
        "\n",
        "**B. Cons**\n",
        "*   **MI Estimation Challenges**: Estimating and optimizing MI bounds can be difficult and numerically unstable, especially in high dimensions. The quality of learned representations is sensitive to the MI estimator.\n",
        "*   **Discriminator Complexity**: The performance can be sensitive to the architecture and capacity of the discriminator networks.\n",
        "*   **Computational Cost**: Training involves multiple networks (encoder, one or more discriminators), which can be computationally intensive.\n",
        "*   **Performance Relative to Newer Methods**: While foundational, DIM's performance on standard benchmarks might be surpassed by more recent contrastive learning methods (e.g., SimCLR, MoCo) that often utilize stronger data augmentations and more effective negative sampling strategies or InfoNCE-based losses.\n",
        "\n",
        "### IX. Cutting-Edge Advances and Related Work\n",
        "\n",
        "*   **Improved MI Estimators**: Research into more stable and accurate MI estimators (e.g., MINE, NWJ, InfoNCE) has influenced subsequent self-supervised methods. Many newer approaches use InfoNCE due to its connection to contrastive learning and relative stability.\n",
        "*   **Augmented Multiscale Deep InfoMax (AMDIM)**: An extension of DIM that incorporates multiple scales/resolutions and stronger data augmentation, leading to improved performance.\n",
        "*   **Contrastive Predictive Coding (CPC)**: Shares the core idea of predicting future/local information from context using MI maximization, often employing an InfoNCE loss.\n",
        "*   **Graph InfoMax (GIM/DGI)**: Adapts DIM principles for learning node representations in graphs by maximizing MI between local patch representations and a global graph summary.\n",
        "*   **InfoNCE-based Methods (e.g., MoCo, SimCLR)**: While not direct DIM variants, they build on similar ideas of comparing positive (similar) pairs against negative (dissimilar) pairs, often outperforming JSD-based DIM due to large numbers of negative samples and strong augmentations.\n",
        "*   **Variational Information Bottleneck (VIB)**: Related in its use of information theory, but VIB aims to learn a compressed representation of input $X$ that is maximally informative about a target variable $Y$. DIM is unsupervised."
      ],
      "metadata": {
        "id": "f_x3mVBe8aO2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYrbOhn9qYWo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Clustering\n",
        "\n",
        "**I. Definition**\n",
        "\n",
        "Deep Clustering refers to a class of unsupervised learning algorithms that integrate deep neural networks for automated feature representation learning with clustering methodologies. The objective is to simultaneously learn a mapping from the high-dimensional input data to a lower-dimensional feature space and partition these learned features into distinct clusters, without relying on labeled data.\n",
        "\n",
        "**II. Model Architecture, Pre-processing, and Core Mathematical Formulations**\n",
        "\n",
        "A. **Overall Framework:**\n",
        "A typical deep clustering model comprises two main components:\n",
        "1.  A deep neural network $f_{\\theta}(\\cdot)$ (parameterized by $\\theta$) that maps an input data point $x_i \\in \\mathbb{R}^D$ to a lower-dimensional latent representation $z_i = f_{\\theta}(x_i) \\in \\mathbb{R}^d$, where $d \\ll D$.\n",
        "2.  A clustering mechanism or loss function applied to the latent representations $\\{z_i\\}$.\n",
        "\n",
        "B. **Pre-processing Steps:**\n",
        "    *   **Data Normalization:** Input features are typically normalized to have zero mean and unit variance or scaled to a specific range (e.g., [0, 1] or [-1, 1]). For an input feature $x^{(j)}$:\n",
        "  $$ x^{(j)}_{norm} = \\frac{x^{(j)} - \\mu_j}{\\sigma_j} $$\n",
        "        or\n",
        "  $$ x^{(j)}_{scaled} = \\frac{x^{(j)} - \\min(x^{(j)})}{\\max(x^{(j)}) - \\min(x^{(j)})} $$\n",
        "    *   **Data Augmentation (especially for images):** Techniques like random crops, rotations, color jittering are applied to increase robustness and encourage invariance in learned features. If $T(\\cdot)$ is an augmentation function, the input becomes $x'_i = T(x_i)$.\n",
        "\n",
        "C. **Feature Extractor $f_{\\theta}(\\cdot)$:**\n",
        "  - *   **1. Autoencoders (AE):** An AE consists of an encoder $f_{\\theta_e}(\\cdot)$ and a decoder $g_{\\theta_d}(\\cdot)$.\n",
        "        *   Encoder: $z = f_{\\theta_e}(x)$\n",
        "        *   Decoder: $\\hat{x} = g_{\\theta_d}(z)$\n",
        "        *   The parameters $\\theta = (\\theta_e, \\theta_d)$ are learned by minimizing a reconstruction loss, e.g., Mean Squared Error (MSE):\n",
        "            $$ L_{rec} = \\frac{1}{N} \\sum_{i=1}^{N} ||x_i - g_{\\theta_d}(f_{\\theta_e}(x_i))||_2^2 $$\n",
        "    *   **2. Convolutional Neural Networks (CNNs):** For image data, CNNs are commonly used as feature extractors. A typical CNN layer involves:\n",
        "        *   Convolution: $(I * K)(i, j) = \\sum_m \\sum_n I(m, n) K(i-m, j-n)$\n",
        "        *   Activation function (e.g., ReLU): $\\sigma(x) = \\max(0, x)$\n",
        "        *   Pooling (e.g., Max Pooling)\n",
        "\n",
        "D. **Clustering Module/Loss:**\n",
        "  - *   **1. K-means based objective:** Some models incorporate a k-means-like loss on the latent features $z_i$.\n",
        "  Let $C = \\{\\mu_1, \\dots, \\mu_K\\}$ be a set of $K$ cluster centroids in the latent space. The objective is to minimize:\n",
        "  $$ L_{km} = \\sum_{i=1}^{N} \\sum_{k=1}^{K} r_{ik} ||z_i - \\mu_k||_2^2 $$\n",
        "  where $r_{ik} = 1$ if $x_i$ is assigned to cluster $k$, and $0$ otherwise. In deep clustering, $z_i = f_{\\theta}(x_i)$, and $\\mu_k$ are also learnable or updated iteratively.\n",
        "    *   **2. Cluster Assignment Hardening (DEC/IDEC):**\n",
        "        Deep Embedded Clustering (DEC) initializes centroids $\\mu_k$ (e.g., by k-means on initial $z_i$) and then iteratively refines clusters and network parameters. It uses a Student's t-distribution to measure similarity between $z_i$ and $\\mu_k$:\n",
        "        $$ q_{ik} = \\frac{(1 + ||z_i - \\mu_k||^2/\\alpha)^{-\\frac{\\alpha+1}{2}}}{\\sum_{j=1}^{K} (1 + ||z_i - \\mu_j||^2/\\alpha)^{-\\frac{\\alpha+1}{2}}} $$\n",
        "        where $q_{ik}$ is the probability of assigning sample $i$ to cluster $k$, and $\\alpha$ is the degrees of freedom (typically 1). An auxiliary target distribution $p_{ik}$ is derived from $q_{ik}$ to sharpen assignments:\n",
        "        $$ p_{ik} = \\frac{q_{ik}^2 / \\sum_j q_{jk}}{\\sum_{k'} (q_{ik'}^2 / \\sum_j q_{jk'})} $$\n",
        "        The clustering loss is the KL divergence between $P$ and $Q$:\n",
        "        $$ L_c = KL(P||Q) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} p_{ik} \\log \\frac{p_{ik}}{q_{ik}} $$\n",
        "        IDEC (Improved DEC) adds back the AE reconstruction loss: $L = L_c + \\gamma L_{rec}$, where $\\gamma > 0$ is a balancing coefficient.\n",
        "\n",
        "**III. Key Principles**\n",
        "*   **End-to-End Learning:** Feature representations and cluster assignments are learned simultaneously, allowing features to be tailored for the clustering task.\n",
        "*   **Self-Supervision:** Clustering objectives provide a supervisory signal for feature learning without manual labels.\n",
        "*   **Dimensionality Reduction:** Neural networks project data into a lower-dimensional space where clusters are more apparent.\n",
        "*   **Non-linearity:** Deep networks can capture complex, non-linear structures in the data, leading to more effective cluster separation.\n",
        "\n",
        "**IV. Detailed Concept Analysis**\n",
        "Deep clustering methods aim to overcome the limitations of traditional clustering algorithms, which often struggle with high-dimensional data and rely on hand-crafted features or simple distance metrics.\n",
        "*   **Feature Learning:** The core idea is that good features make clustering easier. The DNN learns a representation $z$ where clusters are more compact and well-separated.\n",
        "*   **Joint vs. Alternating Optimization:**\n",
        "    *   **Joint Optimization:** Both network parameters $\\theta$ and cluster parameters (e.g., centroids $\\mu_k$) are optimized with respect to a single, combined objective function. This is conceptually simpler but can be harder to optimize.\n",
        "    *   **Alternating Optimization:** Typically involves iteratively:\n",
        "        1.  Updating cluster assignments or pseudo-labels based on current features $z_i$.\n",
        "        2.  Updating network parameters $\\theta$ using a loss derived from these assignments/pseudo-labels.\n",
        "        This is common in methods like DEC.\n",
        "*   **Avoiding Trivial Solutions:** A common pitfall is the model learning degenerate solutions (e.g., all points mapped to a single point, or empty clusters). Regularization, specific loss formulations (like in DEC), or AE reconstruction loss help prevent this.\n",
        "*   **Initialization Sensitivity:** Performance can be sensitive to the initialization of network weights and cluster centroids. Pre-training the feature extractor (e.g., as an autoencoder) is a common practice.\n",
        "\n",
        "**V. Training Pseudo-algorithms**\n",
        "\n",
        "A. **Algorithm: Deep Embedded Clustering (DEC)**\n",
        "  - *   **Input:** Data $X = \\{x_i\\}_{i=1}^N$, number of clusters $K$.\n",
        "    *   **Output:** Network parameters $\\theta$, cluster centroids $\\{\\mu_k\\}_{k=1}^K$.\n",
        "    1.  **Initialization:**\n",
        "        *   Pre-train an autoencoder $f_{\\theta_e}, g_{\\theta_d}$ on $X$ to minimize $L_{rec}$. Initialize $\\theta$ with $\\theta_e$.\n",
        "        *   Obtain initial latent representations $Z^{(0)} = \\{f_{\\theta}(x_i)\\}_{i=1}^N$.\n",
        "        *   Run k-means on $Z^{(0)}$ to get initial centroids $\\{\\mu_k^{(0)}\\}_{k=1}^K$.\n",
        "    2.  **Iterative Refinement (Epoch $t=1, \\dots, T_{max}$):**\n",
        "        *   **a. Compute soft assignments $Q^{(t)}$:** For each $x_i$:\n",
        "            $$ q_{ik}^{(t)} = \\frac{(1 + ||f_{\\theta^{(t-1)}}(x_i) - \\mu_k^{(t-1)}||^2/\\alpha)^{-\\frac{\\alpha+1}{2}}}{\\sum_{j=1}^{K} (1 + ||f_{\\theta^{(t-1)}}(x_i) - \\mu_j^{(t-1)}||^2/\\alpha)^{-\\frac{\\alpha+1}{2}}} $$\n",
        "        *   **b. Compute target distribution $P^{(t)}$:**\n",
        "            $$ p_{ik}^{(t)} = \\frac{(q_{ik}^{(t)})^2 / \\sum_j q_{jk}^{(t)}}{\\sum_{k'} ((q_{ik'}^{(t)})^2 / \\sum_j q_{jk'}^{(t)})} $$\n",
        "        *   **c. Update network parameters $\\theta$ and centroids $\\mu_k$:** Minimize $L_c^{(t)}$ using Stochastic Gradient Descent (SGD) or its variants:\n",
        "            $$ L_c^{(t)} = \\sum_{i=1}^{N} \\sum_{k=1}^{K} p_{ik}^{(t)} \\log \\frac{p_{ik}^{(t)}}{q_{ik}^{(t)}(z_i, \\mu_k)} $$\n",
        "            Gradients are computed w.r.t. $z_i = f_{\\theta}(x_i)$ and $\\mu_k$.\n",
        "            $$ \\frac{\\partial L_c}{\\partial z_i} = \\frac{\\alpha+1}{\\alpha} \\sum_k p_{ik} (1+||z_i - \\mu_k||^2/\\alpha)^{-1} (z_i - \\mu_k) $$\n",
        "            $$ \\frac{\\partial L_c}{\\partial \\mu_k} = -\\frac{\\alpha+1}{\\alpha} \\sum_i p_{ik} (1+||z_i - \\mu_k||^2/\\alpha)^{-1} (z_i - \\mu_k) $$\n",
        "            Note: Often $\\mu_k$ are not updated via gradient descent but by re-computing centroids based on $p_{ik}$ or $q_{ik}$ for stability.\n",
        "        *   **d. Check for convergence:** If cluster assignments change by less than a tolerance $\\delta$ for a certain number of epochs, stop.\n",
        "    *   **Mathematical Justification:** The KL divergence loss $L_c$ encourages the learned soft assignments $Q$ to be close to the target distribution $P$, which is a \"sharpened\" version of $Q$. This self-training mechanism iteratively refines features and cluster assignments.\n",
        "\n",
        "B. **Algorithm: Autoencoder + K-means (A simpler, often baseline approach)**\n",
        "  - 1.  **Train Autoencoder:**\n",
        "        *   Minimize $L_{rec} = \\frac{1}{N} \\sum_{i=1}^{N} ||x_i - g_{\\theta_d}(f_{\\theta_e}(x_i))||_2^2$ to learn $\\theta_e, \\theta_d$.\n",
        "  - 2.  **Extract Features:**\n",
        "        *   For all $x_i$, compute $z_i = f_{\\theta_e}(x_i)$.\n",
        "  - 3.  **Apply K-means:**\n",
        "        *   Run K-means on the set of latent features $\\{z_i\\}_{i=1}^N$.\n",
        "    *   **Mathematical Justification:** Assumes that good data reconstruction leads to a latent space suitable for clustering. This is a two-stage approach, not end-to-end feature learning *for* clustering.\n",
        "\n",
        "**VI. Post-Training Procedures**\n",
        "*   A. **Cluster Assignment Refinement:** After training, final cluster assignments are typically made by assigning each point $x_i$ to the cluster $k^*$ for which $q_{ik^*}$ is maximal, or by assigning it to the nearest centroid $\\mu_{k^*}$ in the learned latent space.\n",
        "* B. **Model Fine-tuning with Pseudo-Labels (Self-training variant):**\n",
        "    1.  Obtain high-confidence cluster assignments from an initial deep clustering run (e.g., points where $max_k(q_{ik}) > \\text{threshold}$). These are treated as pseudo-labels $y'_i$.\n",
        "    2.  Add a classification loss term to the model, e.g., cross-entropy:\n",
        "$$ L_{cls} = - \\sum_{i \\in \\text{pseudo-labeled set}} \\sum_k y'_{ik} \\log q_{ik} $$\n",
        "    3.  Fine-tune the network $f_{\\theta}$ (and potentially a classifier head) using $L_{cls}$ combined with the original clustering loss and/or reconstruction loss.\n",
        "\n",
        "**VII. Evaluation Phase**\n",
        "\n",
        "A. **Loss Functions (Recap & Further Detail):**\n",
        "  - *   **1. Reconstruction Loss (for AE-based architectures):**\n",
        "        $$ L_{rec}(X, \\hat{X}) = \\frac{1}{N} \\sum_{i=1}^{N} ||x_i - g_{\\theta_d}(f_{\\theta_e}(x_i))||_2^2 \\quad (\\text{MSE}) $$\n",
        "        Or Binary Cross-Entropy (BCE) for binary data:\n",
        "        $$ L_{rec} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^D [x_{ij} \\log \\hat{x}_{ij} + (1-x_{ij}) \\log(1-\\hat{x}_{ij})] $$\n",
        "    *   **2. Clustering-Specific Loss:**\n",
        "        *   **KL Divergence Loss (DEC, IDEC):**\n",
        "            $$ L_c = \\sum_i \\sum_k p_{ik} \\log \\frac{p_{ik}}{q_{ik}} $$\n",
        "        *   **K-means like Loss:**\n",
        "            $$ L_{km} = \\sum_i \\min_k ||z_i - \\mu_k||^2 $$\n",
        "        *   **Contrastive Loss (SCAN, SwAV for clustering):** Used in self-supervised deep clustering, often based on augmented views of data. For a pair of positive views $(z_i, z_j)$ from the same original image, and negative samples $z_l$:\n",
        "            $$ L_{contrastive} = - \\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{l} \\exp(\\text{sim}(z_i, z_l)/\\tau)} $$\n",
        "            where $\\text{sim}(\\cdot, \\cdot)$ is a similarity function (e.g., cosine similarity) and $\\tau$ is a temperature parameter.\n",
        "\n",
        "- B. **Evaluation Metrics (Require Ground-Truth Labels $y$ for evaluation, not training):**\n",
        "    Let $C_{pred}$ be the predicted cluster assignments and $C_{true}$ be the ground-truth class labels.\n",
        "  *   **1. Accuracy (ACC):**\n",
        "        ACC requires finding the best mapping between predicted clusters and true classes. This is often done using the Hungarian algorithm.\n",
        "        $$ ACC = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}(y_i = m(c_i)) $$\n",
        "        where $c_i$ is the predicted cluster for $x_i$, $y_i$ is its true label, and $m$ is the optimal mapping.\n",
        "    *   **2. Normalized Mutual Information (NMI):**\n",
        "        Measures the agreement between two assignments, ignoring permutations.\n",
        "        $$ NMI(Y, C) = \\frac{I(Y; C)}{\\sqrt{H(Y)H(C)}} $$\n",
        "        where $I(Y; C)$ is the mutual information between true labels $Y$ and predicted clusters $C$, and $H(\\cdot)$ is entropy.\n",
        "        $$ I(Y;C) = \\sum_{y \\in Y} \\sum_{c \\in C} p(y,c) \\log \\left( \\frac{p(y,c)}{p(y)p(c)} \\right) $$\n",
        "        $$ H(Y) = - \\sum_{y \\in Y} p(y) \\log p(y) $$\n",
        "    *   **3. Adjusted Rand Index (ARI):**\n",
        "        Measures the similarity between two data clusterings, adjusted for chance.\n",
        "        $$ ARI = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}{\\frac{1}{2} [\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}} $$\n",
        "        where $n_{ij}$ is the number of objects in common between true class $i$ and predicted cluster $j$, $a_i = \\sum_j n_{ij}$, $b_j = \\sum_i n_{ij}$.\n",
        "\n",
        "C. **Domain-Specific Metrics:**\n",
        "    *   For text clustering: Coherence scores within clusters.\n",
        "    *   For image clustering: Visual separation and semantic consistency of clusters.\n",
        "    *   Generally, silhouette score can be computed on the latent space $Z$ if no ground truth is available:\n",
        "        $$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
        "        where $a(i)$ is the mean intra-cluster distance for $z_i$, and $b(i)$ is the mean nearest-cluster distance for $z_i$. Average $s(i)$ over all points.\n",
        "\n",
        "**VIII. Importance**\n",
        "*   Enables unsupervised discovery of structure in complex, high-dimensional datasets where manual labeling is infeasible.\n",
        "*   Powers applications in anomaly detection, customer segmentation, image retrieval, and bioinformatics.\n",
        "*   Serves as a crucial pre-processing step for semi-supervised learning or data exploration.\n",
        "\n",
        "**IX. Pros versus Cons**\n",
        "*   **Pros:**\n",
        "    *   Learns task-relevant features instead of relying on predefined ones.\n",
        "    *   Can model highly non-linear relationships and cluster boundaries.\n",
        "    *   Handles heterogeneous and high-dimensional data more effectively than traditional methods.\n",
        "    *   Potential for end-to-end optimization.\n",
        "*   **Cons:**\n",
        "    *   Computationally more expensive than traditional clustering.\n",
        "    *   Requires careful hyperparameter tuning (network architecture, learning rates, loss balancing).\n",
        "    *   Sensitive to initialization and prone to converging to poor local optima.\n",
        "    *   Determining the number of clusters $K$ remains a challenge (as in traditional clustering).\n",
        "    *   Interpretability of learned features can be difficult.\n",
        "    *   Performance evaluation in a truly unsupervised setting (without ground truth) is challenging.\n",
        "\n",
        "**X. Cutting-Edge Advances**\n",
        "*   **Contrastive Learning for Clustering:** Methods like SCAN (Semantic Clustering by Adopting Nearest neighbors) and PICA (Prototype and Instance-level Contrastive Alignment) leverage instance-wise contrastive learning to learn discriminative features suitable for clustering. This has significantly improved SOTA.\n",
        "*   **Self-Supervised Deep Clustering:** Combining pretext tasks (e.g., predicting rotations, Jigsaw puzzles) with clustering objectives.\n",
        "*   **Graph Neural Networks (GNNs) for Clustering:** Deep clustering on graph-structured data, or constructing graphs from features and then using GNNs.\n",
        "*   **Clustering with Transformers:** Applying Transformer architectures for feature extraction in deep clustering, especially for sequential or set-structured data.\n",
        "*   **Robustness and Fairness:** Research into making deep clustering methods more robust to noise, outliers, and biases in the data.\n",
        "*   **Online Deep Clustering:** Adapting models to streaming data.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "V74ILX7hVH96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Generative Models (DGMs)\n",
        "\n",
        "**I. Definition**\n",
        "Deep Generative Models (DGMs) are a class of neural networks that learn the underlying probability distribution $p_{data}(x)$ of a given training dataset $X = \\{x_1, ..., x_N\\}$. Once trained, these models can generate new data samples $x_{new} \\sim p_{model}(x)$ that are similar to the training data, where $p_{model}(x)$ approximates $p_{data}(x)$.\n",
        "\n",
        "**II. Model Architectures, Pre-processing, and Core Mathematical Formulations**\n",
        "\n",
        "- A. **Pre-processing Steps:**\n",
        "    *   **Data Normalization/Scaling:** Images are often scaled to $[-1, 1]$ (for Tanh output) or $[0, 1]$ (for Sigmoid output). Continuous features are typically standardized.\n",
        "        $$ x_{scaled} = 2 \\cdot \\frac{x - \\min(x)}{\\max(x) - \\min(x)} - 1 \\quad (\\text{for } [-1, 1]) $$\n",
        "    *   **Resizing/Cropping (for images):** Images are usually resized to a fixed resolution (e.g., $64 \\times 64$, $256 \\times 256$).\n",
        "\n",
        "- B. **Variational Autoencoders (VAEs)**\n",
        "    *   VAEs model $p(x)$ by introducing a latent variable $z \\in \\mathbb{R}^d$. They consist of an encoder (inference network) $q_{\\phi}(z|x)$ and a decoder (generative network) $p_{\\theta}(x|z)$.\n",
        "    *   **1. Encoder (Inference Network $q_{\\phi}(z|x)$):**\n",
        "        Maps input $x$ to parameters of a distribution in latent space, typically a Gaussian:\n",
        "        $q_{\\phi}(z|x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\text{diag}(\\sigma^2_{\\phi}(x)))$.\n",
        "        The encoder outputs $\\mu_{\\phi}(x)$ and $\\log \\sigma^2_{\\phi}(x)$.\n",
        "    *   **2. Latent Space ($z$) and Reparameterization Trick:**\n",
        "        To allow backpropagation through the sampling process, $z$ is sampled as:\n",
        "        $$ z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, I) $$\n",
        "        $\\odot$ denotes element-wise product.\n",
        "    *   **3. Decoder (Generative Network $p_{\\theta}(x|z)$):**\n",
        "        Maps latent variable $z$ back to the data space, parameterizing a distribution for $x$.\n",
        "        For continuous data, often $p_{\\theta}(x|z) = \\mathcal{N}(x; \\mu_{\\theta}(z), \\text{diag}(\\sigma^2_{\\theta}(z)))$ (often $\\sigma_{\\theta}$ is fixed).\n",
        "        For binary data, $p_{\\theta}(x|z)$ could be a Bernoulli distribution where the decoder outputs pixel probabilities.\n",
        "    *   **Objective Function (ELBO - Evidence Lower Bound):** VAEs maximize the ELBO:\n",
        "$$ \\mathcal{L}_{VAE}(\\theta, \\phi; x) = \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x) || p(z)) $$\n",
        "The first term is the reconstruction likelihood. The second term is a KL divergence regularizer that pushes $q_{\\phi}(z|x)$ towards a prior $p(z)$ (usually $\\mathcal{N}(0, I)$).\n",
        "\n",
        "- C. **Generative Adversarial Networks (GANs)**\n",
        "  *   GANs involve a two-player minimax game between a Generator $G$ and a Discriminator $D$.\n",
        "  *   **1. Generator ($G_{\\theta_g}$):**\n",
        "  Takes a random noise vector $z \\sim p_z(z)$ (e.g., $z \\sim \\mathcal{N}(0,I)$ or $U(-1,1)$) as input and outputs a synthetic sample $x_{fake} = G_{\\theta_g}(z)$ aiming to resemble real data.\n",
        "  *   **2. Discriminator ($D_{\\theta_d}$):**\n",
        "  A classifier that takes a sample $x$ (either real from $p_{data}$ or fake from $G$) and outputs the probability $D(x)$ that $x$ is real.\n",
        "  *   **Objective Function (Minimax Game):**\n",
        "$$ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] $$\n",
        "$D$ tries to maximize this objective (correctly distinguish real from fake), while $G$ tries to minimize it (fool $D$).\n",
        "\n",
        "- D. **Flow-based Models (e.g., NICE, RealNVP, Glow)**\n",
        "    *   Learn an invertible transformation $f: \\mathcal{X} \\to \\mathcal{Z}$ where $\\mathcal{Z}$ is a latent space with a simple distribution $p_Z(z)$ (e.g., Gaussian).\n",
        "    *   The density of $x$ is given by the change of variables formula:\n",
        "        $$ p_X(x) = p_Z(f(x)) \\left| \\det\\left(\\frac{\\partial f(x)}{\\partial x^T}\\right) \\right| $$\n",
        "    *   The transformation $f$ is typically a composition of simple invertible functions (e.g., coupling layers) whose Jacobians are easy to compute. Training maximizes $\\log p_X(x)$.\n",
        "\n",
        "- E. **Diffusion Models (e.g., DDPM, Score-based Generative Models)**\n",
        "*   Define a forward diffusion process that gradually adds noise to data $x_0$ over $T$ steps, producing $x_1, \\dots, x_T$.\n",
        "$$ q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I) $$\n",
        "where $\\beta_t$ are small positive constants (noise schedule).\n",
        "*   Train a neural network (often a U-Net) $p_{\\theta}(x_{t-1}|x_t)$ to reverse this process (denoising).\n",
        "$$ p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t)) $$\n",
        "*   The model typically predicts the noise $\\epsilon_{\\theta}(x_t, t)$ added at step $t$. The loss is often a simplified objective:\n",
        "$$ L_{simple} = \\mathbb{E}_{t, x_0, \\epsilon_t} [ ||\\epsilon_t - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t, t) ||^2 ] $$\n",
        "where $\\alpha_t = 1-\\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$. Generation starts from $x_T \\sim \\mathcal{N}(0,I)$ and iteratively denoises.\n",
        "\n",
        "### III. Key Principles\n",
        "*   **Learning Data Distribution:** The primary goal is to capture the underlying probability distribution of the training data.\n",
        "*   **Sampling:** A trained model should be able to generate novel samples that are characteristic of the learned distribution.\n",
        "*   **Representation Learning (VAEs, some GANs):** Models often learn a meaningful latent space representation of the data.\n",
        "*   **Adversarial Learning (GANs):** Competitive training between two networks (generator and discriminator) drives improvements in sample quality.\n",
        "*   **Likelihood Maximization (VAEs, Flows, Diffusion):** Many DGMs are trained by directly or indirectly maximizing the likelihood (or a bound on it) of the observed data.\n",
        "\n",
        "## IV. Detailed Concept Analysis\n",
        "-   **Implicit vs. Explicit Density Models:**\n",
        "    *   **Explicit Density Models** (VAEs, Flows, Diffusion Models) define an explicit density function $p_{model}(x)$ that can be evaluated.\n",
        "    *   **Implicit Density Models** (GANs) define a stochastic procedure to generate samples but do not provide an explicit $p_{model}(x)$.\n",
        "*   **Latent Space Structure:**\n",
        "    *   In VAEs, the KL divergence term encourages a smooth, structured latent space (often Gaussian). This allows for meaningful interpolation and manipulation.\n",
        "    *   In GANs, the latent space structure is less explicitly controlled but can also allow for interpolations. Techniques like StyleGAN focus on disentangling latent factors.\n",
        "*   **Training Stability (GANs):** GAN training is notoriously unstable. Common issues include:\n",
        "    *   **Mode Collapse:** Generator produces only a limited variety of samples.\n",
        "    *   **Vanishing Gradients:** Discriminator becomes too good, providing no useful gradient to the generator.\n",
        "    *   Non-convergence.\n",
        "  Solutions involve careful architecture design (e.g., DCGAN), loss function modifications (e.g., Wasserstein GAN), regularization (e.g., gradient penalty), and training heuristics (e.g., two-timescale update rule - TTUR).\n",
        "*   **Sample Quality vs. Diversity:** There's often a trade-off. Some models generate high-fidelity samples but may lack diversity (mode collapse), while others cover the data distribution better but may produce lower-quality samples. Diffusion models currently excel at both.\n",
        "\n",
        "## V. Training Pseudo-algorithms\n",
        "\n",
        "- A. **Variational Autoencoder (VAE)**\n",
        "  *   **Input:** Data $X = \\{x_i\\}_{i=1}^N$, prior $p(z) = \\mathcal{N}(0,I)$.\n",
        "    *   **Output:** Encoder parameters $\\phi$, Decoder parameters $\\theta$.\n",
        "    1.  **Initialize** $\\phi, \\theta$.\n",
        "    2.  **For each training epoch:**\n",
        "      *   For each mini-batch $X_b \\subset X$:\n",
        "        *   **a. Encode:** For each $x \\in X_b$, compute $\\mu_{\\phi}(x)$ and $\\log \\sigma^2_{\\phi}(x)$ using the encoder $q_{\\phi}$.\n",
        "        *   **b. Sample latent vector:** $z = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$.\n",
        "        *   **c. Decode:** Compute parameters of $p_{\\theta}(x|z)$ using the decoder. E.g., $\\mu_{\\theta}(z)$ if $p_{\\theta}(x|z)$ is Gaussian with fixed variance, or pixel probabilities if Bernoulli.\n",
        "        *   **d. Compute Loss (negative ELBO):**\n",
        "$$ L = -(\\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x) || p(z))) $$\n",
        "$$ L = L_{reconstruction} + L_{KL} $$\n",
        "Where, for Gaussian decoder $p_{\\theta}(x|z) = \\mathcal{N}(x; \\mu_{\\theta}(z), \\sigma_{dec}^2 I)$:\n",
        "$$ L_{reconstruction} = \\frac{1}{2\\sigma_{dec}^2N_b}\\sum_{x \\in X_b} ||x - \\mu_{\\theta}(z)||^2 + \\text{const} $$\n",
        "(typically MSE loss is used if $\\sigma_{dec}$ is fixed or absorbed).\n",
        "And for $q_{\\phi}(z|x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\text{diag}(\\sigma^2_{\\phi}(x)))$ and $p(z) = \\mathcal{N}(0,I)$:\n",
        "$$ L_{KL} = \\frac{1}{N_b}\\sum_{x \\in X_b} \\frac{1}{2} \\sum_{j=1}^{d} (\\mu_{\\phi,j}(x)^2 + \\sigma_{\\phi,j}^2(x) - \\log(\\sigma_{\\phi,j}^2(x)) - 1) $$\n",
        "*   **e. Update $\\phi, \\theta$** using gradients $\\nabla_{\\phi} L, \\nabla_{\\theta} L$ via an optimizer (e.g., Adam).\n",
        "*  **Mathematical Justification:** Maximizing ELBO is equivalent to minimizing $D_{KL}(q_{\\phi}(z|x)p_{\\theta}(x|z) || p(x,z))$, pushing the approximate joint posterior towards the true joint. Also, $\\log p(x) \\ge \\mathcal{L}_{VAE}$, so maximizing ELBO indirectly maximizes data log-likelihood.\n",
        "\n",
        "- B. **Generative Adversarial Network (Standard GAN)**\n",
        "    *   **Input:** Data $X = \\{x_i\\}_{i=1}^N$, noise prior $p_z(z)$.\n",
        "    *   **Output:** Generator parameters $\\theta_g$, Discriminator parameters $\\theta_d$.\n",
        "    1.  **Initialize** $\\theta_g, \\theta_d$.\n",
        "    2.  **For each training epoch:**\n",
        "        *   **For $k$ steps (typically $k=1$, but can be >1):**\n",
        "            *   **a. Sample real data:** Mini-batch $\\{x^{(1)}, \\dots, x^{(m)}\\}$ from $p_{data}(x)$.\n",
        "            *   **b. Sample noise:** Mini-batch $\\{z^{(1)}, \\dots, z^{(m)}\\}$ from $p_z(z)$.\n",
        "            *   **c. Generate fake data:** $x_{fake}^{(i)} = G_{\\theta_g}(z^{(i)})$.\n",
        "            *   **d. Update Discriminator $\\theta_d$:** Maximize $V_D = \\frac{1}{m}\\sum_{i=1}^m \\log D_{\\theta_d}(x^{(i)}) + \\frac{1}{m}\\sum_{i=1}^m \\log(1 - D_{\\theta_d}(x_{fake}^{(i)}))$.\n",
        "                $$ \\theta_d \\leftarrow \\theta_d + \\eta_D \\nabla_{\\theta_d} V_D $$\n",
        "        *   **Sample noise:** Mini-batch $\\{z^{(1)}, \\dots, z^{(m)}\\}$ from $p_z(z)$.\n",
        "        *   **Update Generator $\\theta_g$:** Minimize $V_G = \\frac{1}{m}\\sum_{i=1}^m \\log(1 - D_{\\theta_d}(G_{\\theta_g}(z^{(i)})))$.\n",
        "            (Often, for better gradients, maximize $\\frac{1}{m}\\sum_{i=1}^m \\log D_{\\theta_d}(G_{\\theta_g}(z^{(i)}))$ instead - non-saturating loss)\n",
        "            $$ \\theta_g \\leftarrow \\theta_g - \\eta_G \\nabla_{\\theta_g} V_G $$\n",
        "*   **Mathematical Justification:** The training procedure seeks a Nash equilibrium of the minimax game. For a fixed $G$, the optimal $D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$. Substituting $D^*$ into $V(D,G)$, the objective for $G$ becomes minimizing $2 \\cdot JSD(p_{data}||p_g) - 2 \\log 2$, where JSD is Jensen-Shannon Divergence. So $G$ tries to make $p_g$ match $p_{data}$.\n",
        "\n",
        "## VI. Post-Training Procedures\n",
        "- A. **Sample Generation:** Generate new samples by drawing $z \\sim p_z(z)$ and passing it through $G_{\\theta_g}(z)$ (for GANs) or $p_{\\theta}(x|z)$ (for VAEs, Flows, Diffusion).\n",
        "- B. **Latent Space Interpolation/Manipulation:**\n",
        "    *   For $z_1, z_2$ (latent codes of two samples, or random draws), generate samples for $z_{\\alpha} = (1-\\alpha)z_1 + \\alpha z_2$ for $\\alpha \\in [0,1]$. This tests smoothness of latent space.\n",
        "    *   If latent directions corresponding to semantic attributes are found (e.g., \"adding glasses\"), $z_{new} = z_{orig} + \\beta v_{attr}$, where $v_{attr}$ is the attribute vector.\n",
        "*   C. **Reconstruction (VAEs):** Given an input $x$, pass it through encoder $q_{\\phi}(z|x)$ (usually taking $\\mu_{\\phi}(x)$ as $z$) and then decoder $p_{\\theta}(x|z)$ to get $\\hat{x}$.\n",
        "*   D. **Density Estimation (VAEs, Flows, some Diffusion):** For a new point $x$, estimate $p_{model}(x)$ or its lower bound (ELBO for VAEs).\n",
        "\n",
        "## VII. Evaluation Phase\n",
        "\n",
        "- A. **Loss Functions (Recap & Further Detail):**\n",
        "    *   **1. VAE Loss:**\n",
        "        $$ L_{VAE} = L_{reconstruction} + \\beta D_{KL}(q_{\\phi}(z|x) || p(z)) $$\n",
        "        ($\\beta$-VAE uses a weight $\\beta$ on the KL term to encourage disentanglement).\n",
        "    *   **2. GAN Loss:**\n",
        "        *   **Minimax Loss (Binary Cross-Entropy):**\n",
        "$L_D = -(\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))])$\n",
        "$L_G = -\\mathbb{E}_{z \\sim p_z}[\\log D(G(z))]$ (non-saturating version for G)\n",
        "        *   **Wasserstein Loss (WGAN):**\n",
        "$L_D = \\mathbb{E}_{z \\sim p_z}[D(G(z))] - \\mathbb{E}_{x \\sim p_{data}}[D(x)]$\n",
        "$L_G = -\\mathbb{E}_{z \\sim p_z}[D(G(z))]$\n",
        "$D$ must be 1-Lipschitz, enforced by weight clipping or gradient penalty (WGAN-GP):\n",
        "$L_{GP} = \\mathbb{E}_{\\hat{x} \\sim p_{\\hat{x}}}[(||\\nabla_{\\hat{x}} D(\\hat{x})||_2 - 1)^2]$, where $\\hat{x} = \\epsilon x_{real} + (1-\\epsilon)x_{fake}$.\n",
        "    *   **3. Diffusion Model Loss (Simplified DDPM Loss):**\n",
        "$$ L_{simple} = \\mathbb{E}_{t \\sim U(1,T), x_0 \\sim p_{data}, \\epsilon_t \\sim \\mathcal{N}(0,I)} [ ||\\epsilon_t - \\epsilon_{\\theta}(x_t, t) ||^2 ] $$\n",
        "where $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t$.\n",
        "\n",
        "- B. **Evaluation Metrics:**\n",
        "    *   **1. Inception Score (IS):** (Mainly for images)\n",
        "        $$ IS = \\exp(\\mathbb{E}_{x \\sim p_g} [D_{KL}(p(y|x) || p(y))]) $$\n",
        "        $p(y|x)$ is class distribution by a pre-trained Inception network for a generated image $x$. $p(y) = \\int p(y|x) p_g(x) dx$. Higher is better (implies diverse, distinct images).\n",
        "        *Pitfall:* Can be gamed, sensitive to reference classifier.\n",
        "    *   **2. Fréchet Inception Distance (FID):** (Mainly for images)\n",
        "        Measures distance between distributions of Inception activations for real and fake images.\n",
        "        $$ FID(P_r, P_g) = ||\\mu_r - \\mu_g||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}) $$\n",
        "        $(\\mu_r, \\Sigma_r)$ and $(\\mu_g, \\Sigma_g)$ are mean and covariance of activations. Lower is better.\n",
        "        *Best Practice:* Use a large number of samples (e.g., 50k).\n",
        "    *   **3. Precision and Recall (for distributions):** (Kynkäänniemi et al.)\n",
        "        Measures state-of-the-art distributions quality, adapted for DGMs. Precision measures fraction of generated samples in support of real distribution. Recall measures fraction of real samples in support of generated distribution. Based on k-NN in a feature space (e.g., VGG-16).\n",
        "    *   **4. Perceptual Path Length (PPL):** (For GANs, especially StyleGAN)\n",
        "        Measures image dissimilarity (using LPIPS) for small steps in latent space. Lower is better (smoother latent space).\n",
        "        $$ PPL = \\mathbb{E}_{z_1, z_2 \\sim p_z, t \\sim U(0,1)} \\left[ \\frac{1}{\\epsilon^2} d(G(slerp(z_1, z_2; t)), G(slerp(z_1, z_2; t+\\epsilon))) \\right] $$\n",
        "        where $d$ is a perceptual distance.\n",
        "    *   **5. Log-Likelihood (for explicit density models):**\n",
        "        Average log-likelihood on a held-out test set. Hard to compute for GANs. Annealed Importance Sampling (AIS) can be used for VAEs and some diffusion models.\n",
        "    *   **6. (Qualitative) Visual Inspection:** Human evaluation of sample fidelity and diversity.\n",
        "\n",
        "- C. **Domain-Specific Metrics:**\n",
        "    *   **Drug Discovery:** Percentage of valid, unique, and novel molecules generated. QED (Quantitative Estimate of Drug-likeness).\n",
        "    *   **Text Generation:** Perplexity, BLEU score (if reference texts exist), grammatical correctness, coherence.\n",
        "    *   **Audio Generation:** Fréchet Audio Distance (FAD), Mel Cepstral Distortion (MCD).\n",
        "\n",
        "## VIII. Importance\n",
        "*   **Synthetic Data Generation:** Creating realistic data for training other models (data augmentation), privacy preservation, or simulating rare events.\n",
        "*   **Understanding Data:** Learning meaningful representations and underlying factors of variation in data.\n",
        "*   **Creative Applications:** Art generation, music composition, style transfer, text-to-image synthesis.\n",
        "*   **Unsupervised Feature Learning:** Latent representations can be used for downstream tasks.\n",
        "*   **Scientific Discovery:** Generating molecular structures, cosmological simulations, etc.\n",
        "\n",
        "## X. Pros versus Cons\n",
        "*   **Pros (General):**\n",
        "    *   Ability to generate novel, high-fidelity data.\n",
        "    *   Can learn rich, often disentangled, latent representations.\n",
        "    *   Potential for wide-ranging applications across various domains.\n",
        "*   **VAEs:**\n",
        "    *   Pros: Stable training, explicit density model, principled probabilistic framework, interpretable latent space.\n",
        "    *   Cons: Often produce blurrier samples than GANs, ELBO is a lower bound (not true likelihood).\n",
        "*   **GANs:**\n",
        "    *   Pros: Generate sharp, high-fidelity samples (especially images). Implicit density means flexibility.\n",
        "    *   Cons: Unstable training, mode collapse, difficult to evaluate quantitatively, no direct likelihood.\n",
        "*   **Flow-based Models:**\n",
        "    *   Pros: Exact likelihood computation, invertible, learnable latent space.\n",
        "    *   Cons: Restricted architectures (for tractable Jacobians), can be computationally expensive for high-dim data.\n",
        "*   **Diffusion Models:**\n",
        "    *   Pros: State-of-the-art sample quality and diversity, stable training, principled likelihood estimation possible.\n",
        "    *   Cons: Slow sampling (requires many steps), computationally intensive during training and inference (though recent methods improve this).\n",
        "\n",
        "**X. Cutting-Edge Advances**\n",
        "*   **Diffusion Models Dominance:** Models like DALL-E 2, Imagen, Stable Diffusion (Latent Diffusion) have achieved unprecedented text-to-image generation quality. Research focuses on faster sampling (e.g., DDIM, progressive distillation), conditional generation, and applications beyond images.\n",
        "*   **Large-Scale GANs:** Continued improvements in GAN architectures (e.g., StyleGAN3, Projected GANs) leading to better fidelity and control.\n",
        "*   **Generative Transformers:** Applying Transformer architectures for generative tasks across modalities (text, images, audio), e.g., GPT series for text, ViT-VQGAN for images.\n",
        "*   **Neural Fields / Implicit Neural Representations (e.g., NeRF):** Generating and representing complex 3D scenes. While not always DGMs in the classic sense, they share generative principles.\n",
        "*   **Energy-Based Models (EBMs):** Learning unnormalized densities, often trained via MCMC or contrastive divergence. Provide flexibility and can be combined with other DGM frameworks.\n",
        "*   **Controllable Generation:** Fine-grained control over attributes of generated samples through conditioning, latent space manipulation, or text prompts.\n",
        "*   **Ethical AI and DGMs:** Increased focus on mitigating bias, preventing misuse (deepfakes), and ensuring fairness in generative models."
      ],
      "metadata": {
        "id": "sgT9Djd7WpQp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "af5hBJSOWnIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}