{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfaa2035",
   "metadata": {},
   "source": [
    "**BYOL: Bootstrap Your Own Latent**\n",
    "\n",
    "### Definition\n",
    "Bootstrap Your Own Latent (BYOL) is a self-supervised learning algorithm for visual representation learning. It trains an online network to predict the representation generated by a target network for a different augmented view of the same input image. Critically, BYOL achieves strong performance without using negative pairs, relying instead on an asymmetric architecture with a predictor in the online branch and a momentum-updated target network.\n",
    "\n",
    "### Pertinent Equations\n",
    "1.  **Online Network Output (Prediction):**\n",
    "    $$ p_\\theta(z_\\theta) = q_\\theta(g_\\theta(f_\\theta(v))) $$\n",
    "2.  **Target Network Output (Projection):**\n",
    "    $$ z'_\\xi = g_\\xi(f_\\xi(v')) $$\n",
    "3.  **L2 Normalization:**\n",
    "    $$ \\bar{p}_\\theta(z_\\theta) = \\frac{p_\\theta(z_\\theta)}{\\|p_\\theta(z_\\theta)\\|_2} $$\n",
    "    $$ \\bar{z}'_\\xi = \\frac{z'_\\xi}{\\|z'_\\xi\\|_2} $$\n",
    "4.  **Loss Function (Mean Squared Error):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\theta, \\xi}\n",
    "\\;=\\;\n",
    "\\bigl\\|\\bar{p}_\\theta(z_\\theta) \\;-\\; \\text{stop\\_gradient}\\bigl(\\bar{z}'_\\xi\\bigr)\\bigr\\|_2^2\n",
    "\\;=\\;\n",
    "2 \\;-\\; 2 \\cdot \\frac{\\bigl\\langle p_\\theta(z_\\theta),\\,z'_\\xi \\bigr\\rangle}\n",
    "{\\|p_\\theta(z_\\theta)\\|_2 \\,\\cdot\\, \\|z'_\\xi\\|_2}\n",
    "$$\n",
    "5.  **Symmetrized Loss:**\n",
    "    $$ \\mathcal{L}^{\\text{BYOL}} = \\mathcal{L}_{\\theta, \\xi} + \\tilde{\\mathcal{L}}_{\\theta, \\xi} $$\n",
    "    where $\\tilde{\\mathcal{L}}_{\\theta, \\xi}$ is computed by feeding $v'$ to the online network and $v$ to the target network.\n",
    "6.  **Target Network Parameter Update (Exponential Moving Average - EMA):**\n",
    "    $$ \\xi \\leftarrow \\tau \\xi + (1-\\tau) \\theta $$\n",
    "\n",
    "### Key Principles\n",
    "*   **Prediction of Target Projections:** The online network learns by predicting the target network's representation of a different view of the same image.\n",
    "*   **No Negative Pairs:** Unlike contrastive methods (e.g., SimCLR, MoCo), BYOL does not explicitly require negative samples to prevent representational collapse.\n",
    "*   **Momentum Encoder (Target Network):** The target network's parameters are an exponential moving average (EMA) of the online network's parameters. This provides stable targets for the online network to predict.\n",
    "*   **Asymmetric Architecture:** The online network includes an additional predictor MLP ($q_\\theta$) that is not present in the target network. This asymmetry is crucial for preventing collapse.\n",
    "*   **Stop-Gradient:** Gradients are only propagated through the online network parameters $\\theta$. The target network's output $z'_\\xi$ is treated as a constant for the loss calculation w.r.t. $\\theta$.\n",
    "\n",
    "### Detailed Concept Analysis\n",
    "\n",
    "#### Pre-processing Steps: Data Augmentation\n",
    "For an input image $x$, two augmented views, $v$ and $v'$, are generated using a set of stochastic transformations $\\mathcal{T}$ and $\\mathcal{T}'$.\n",
    "1.  Sample $t \\sim \\mathcal{T}$ and $t' \\sim \\mathcal{T}'$.\n",
    "2.  Generate augmented views: $v = t(x)$ and $v' = t'(x)$.\n",
    "Common augmentations for BYOL include:\n",
    "*   Random Resized Crop\n",
    "*   Random Horizontal Flip\n",
    "*   Color Jitter (brightness, contrast, saturation, hue)\n",
    "*   Grayscale Conversion\n",
    "*   Gaussian Blur\n",
    "*   Solarization (used in some BYOL implementations)\n",
    "\n",
    "#### Model Architecture\n",
    "BYOL consists of two neural networks: an online network and a target network. Their core components are an encoder, a projector, and additionally a predictor for the online network.\n",
    "\n",
    "1.  **Online Network (parameterized by $\\theta$)**\n",
    "    *   **Encoder ($f_\\theta$):** A convolutional neural network (e.g., ResNet-50) that extracts image representations $y_\\theta = f_\\theta(v)$.\n",
    "    *   **Projector ($g_\\theta$):** An MLP that maps the representation $y_\\theta$ to a lower-dimensional space, producing latent projections $z_\\theta = g_\\theta(y_\\theta)$. Typically a 2 or 3-layer MLP.\n",
    "        $$ z_\\theta = W^{(g_2)}_\\theta \\text{ReLU}(BN(W^{(g_1)}_\\theta y_\\theta)) $$\n",
    "    *   **Predictor ($q_\\theta$):** Another MLP that transforms the online projection $z_\\theta$ to produce a prediction $p_\\theta(z_\\theta) = q_\\theta(z_\\theta)$. This component is only present in the online network.\n",
    "        $$ p_\\theta(z_\\theta) = W^{(q_2)}_\\theta \\text{ReLU}(BN(W^{(q_1)}_\\theta z_\\theta)) $$\n",
    "        The dimensions of $z_\\theta$ and $p_\\theta(z_\\theta)$ are typically the same (e.g., 256).\n",
    "\n",
    "2.  **Target Network (parameterized by $\\xi$)**\n",
    "    *   **Encoder ($f_\\xi$):** Has the same architecture as $f_\\theta$. Produces $y'_\\xi = f_\\xi(v')$.\n",
    "    *   **Projector ($g_\\xi$):** Has the same architecture as $g_\\theta$. Produces latent projections $z'_\\xi = g_\\xi(y'_\\xi)$.\n",
    "        $$ z'_\\xi = W^{(g_2)}_\\xi \\text{ReLU}(BN(W^{(g_1)}_\\xi y'_\\xi)) $$\n",
    "    *   The parameters $\\xi$ are not updated by gradient descent but are an EMA of $\\theta$.\n",
    "\n",
    "#### Collapse Prevention\n",
    "BYOL avoids trivial solutions (representational collapse) where the network outputs a constant value for all inputs. This is attributed to:\n",
    "*   The predictor $q_\\theta$ in the online network creating an asymmetry.\n",
    "*   The momentum update of the target network $\\xi$, which provides a slowly evolving, stable target.\n",
    "*   Batch Normalization (BN) applied within the MLP heads might implicitly introduce contrastive-like effects or regularize representations.\n",
    "\n",
    "### Training Procedure\n",
    "\n",
    "#### Loss Function\n",
    "The online network is trained to minimize the mean squared error between the L2-normalized online prediction and the L2-normalized target projection:\n",
    "$$ \\mathcal{L}_{\\theta, \\xi} = \\|\\bar{p}_\\theta(z_\\theta) - \\bar{z}'_\\xi \\|_2^2 $$\n",
    "The $\\text{stop_gradient}$ operation is applied to $\\bar{z}'_\\xi$, meaning that $\\xi$ is not updated through this loss.\n",
    "The loss is symmetrized by computing it twice per iteration: once with $v$ fed to the online network and $v'$ to the target network, and once with $v'$ fed to the online network and $v$ to the target network.\n",
    "$$ \\tilde{p}_\\theta(z'_\\theta) = q_\\theta(g_\\theta(f_\\theta(v'))) $$\n",
    "$$ \\tilde{z}_\\xi = g_\\xi(f_\\xi(v)) $$\n",
    "$$ \\tilde{\\mathcal{L}}_{\\theta, \\xi} = \\|\\bar{\\tilde{p}}_\\theta(z'_\\theta) - \\bar{\\tilde{z}}_\\xi \\|_2^2 $$\n",
    "The final loss per mini-batch is:\n",
    "$$ \\mathcal{L}^{\\text{BYOL}} = \\mathcal{L}_{\\theta, \\xi} + \\tilde{\\mathcal{L}}_{\\theta, \\xi} $$\n",
    "\n",
    "#### Momentum Update for Target Network\n",
    "The target network parameters $\\xi$ (which include parameters of $f_\\xi$ and $g_\\xi$) are updated after each training step for $\\theta$ using an EMA:\n",
    "$$ \\xi \\leftarrow \\tau \\xi + (1-\\tau) \\theta $$\n",
    "where $\\tau$ is the target decay rate, typically a value close to 1 (e.g., 0.99 to 0.999). The decay rate $\\tau$ is often scheduled during training, starting from a lower value and increasing towards 1. For example: $\\tau = 1 - (1 - \\tau_{\\text{base}}) \\cdot (\\cos(\\pi k/K) + 1)/2$, where $k$ is current training step and $K$ is total training steps.\n",
    "\n",
    "#### Training Algorithm\n",
    "Let $B$ be the mini-batch size.\n",
    "For each training iteration:\n",
    "1.  **Sample Mini-batch:** Sample a mini-batch of $B$ images $\\{x_j\\}_{j=1}^B$.\n",
    "2.  **Data Augmentation:** For each image $x_j$:\n",
    "    *   Generate two augmented views: $v_j = t(x_j)$ and $v'_j = t'(x_j)$.\n",
    "3.  **Online Network Forward Pass (View 1):**\n",
    "    *   For each $j=1, \\dots, B$:\n",
    "        *   $y_{\\theta,j} = f_\\theta(v_j)$\n",
    "        *   $z_{\\theta,j} = g_\\theta(y_{\\theta,j})$\n",
    "        *   $p_{\\theta,j} = q_\\theta(z_{\\theta,j})$\n",
    "        *   $\\bar{p}_{\\theta,j} = p_{\\theta,j} / \\|p_{\\theta,j}\\|_2$\n",
    "4.  **Target Network Forward Pass (View 2, No Gradient):**\n",
    "    *   For each $j=1, \\dots, B$:\n",
    "        *   $y'_{\\xi,j} = f_\\xi(v'_j)$\n",
    "        *   $z'_{\\xi,j} = g_\\xi(y'_{\\xi,j})$\n",
    "        *   $\\bar{z}'_{\\xi,j} = z'_{\\xi,j} / \\|z'_{\\xi,j}\\|_2$\n",
    "5.  **Compute Loss Term 1:**\n",
    "    $$ \\mathcal{L}_{\\theta, \\xi}^{(1)} = \\frac{1}{B} \\sum_{j=1}^B \\|\\bar{p}_{\\theta,j} - \\text{stop_gradient}(\\bar{z}'_{\\xi,j}) \\|_2^2 $$\n",
    "6.  **Online Network Forward Pass (View 2):**\n",
    "    *   For each $j=1, \\dots, B$:\n",
    "        *   $y'_{\\theta,j} = f_\\theta(v'_j)$\n",
    "        *   $z'_{\\theta,j} = g_\\theta(y'_{\\theta,j})$\n",
    "        *   $p'_{\\theta,j} = q_\\theta(z'_{\\theta,j})$\n",
    "        *   $\\bar{p}'_{\\theta,j} = p'_{\\theta,j} / \\|p'_{\\theta,j}\\|_2$\n",
    "7.  **Target Network Forward Pass (View 1, No Gradient):**\n",
    "    *   For each $j=1, \\dots, B$:\n",
    "        *   $y_{\\xi,j} = f_\\xi(v_j)$\n",
    "        *   $z_{\\xi,j} = g_\\xi(y_{\\xi,j})$\n",
    "        *   $\\bar{z}_{\\xi,j} = z_{\\xi,j} / \\|z_{\\xi,j}\\|_2$\n",
    "8.  **Compute Loss Term 2 (Symmetrized):**\n",
    "    $$ \\mathcal{L}_{\\theta, \\xi}^{(2)} = \\frac{1}{B} \\sum_{j=1}^B \\|\\bar{p}'_{\\theta,j} - \\text{stop_gradient}(\\bar{z}_{\\xi,j}) \\|_2^2 $$\n",
    "9.  **Total Loss:**\n",
    "    $$ \\mathcal{L}^{\\text{BYOL}} = \\mathcal{L}_{\\theta, \\xi}^{(1)} + \\mathcal{L}_{\\theta, \\xi}^{(2)} $$\n",
    "10. **Gradient Update for Online Network:**\n",
    "    *   Compute gradients of $\\mathcal{L}^{\\text{BYOL}}$ with respect to $\\theta$: $\\nabla_{\\theta} \\mathcal{L}^{\\text{BYOL}}$.\n",
    "    *   Update $\\theta$ using an optimizer (e.g., LARS, AdamW): $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}^{\\text{BYOL}}$, where $\\eta$ is the learning rate.\n",
    "11. **Momentum Update for Target Network:**\n",
    "    *   Update $\\xi$: $\\xi \\leftarrow \\tau \\xi + (1-\\tau) \\theta$.\n",
    "\n",
    "### Post-Training Procedures\n",
    "After pre-training, the online encoder $f_\\theta$ is used for downstream tasks.\n",
    "1.  **Linear Evaluation Protocol:**\n",
    "    *   **Procedure:** The weights $\\theta$ of the pre-trained encoder $f_\\theta$ are frozen. A linear classifier is trained on top of the representations $y_\\theta = f_\\theta(x)$.\n",
    "    *   **Mathematical Formulation:** For an input image $x$, features $y_\\theta = f_\\theta(x; \\theta)$ are extracted. A linear layer with weights $W$ and bias $b$ predicts class scores: $s = W^T y_\\theta + b$. The classifier is trained by minimizing cross-entropy loss:\n",
    "        $$ \\mathcal{L}_{\\text{CE}} = -\\sum_{c=1}^{C} y_c^{\\text{label}} \\log(\\text{softmax}(s)_c) $$\n",
    "2.  **Fine-tuning Protocol:**\n",
    "    *   **Procedure:** The pre-trained weights $\\theta$ initialize the encoder $f_\\theta$. The entire network (or parts of it) is then fine-tuned end-to-end on the labeled data of the downstream task.\n",
    "\n",
    "### Evaluation Phase\n",
    "\n",
    "#### Metrics (SOTA and Standard)\n",
    "1.  **ImageNet Linear Classification Accuracy:**\n",
    "    *   **Top-1 Accuracy:**\n",
    "        $$ \\text{Acc}_1 = \\frac{1}{N_{\\text{test}}} \\sum_{i=1}^{N_{\\text{test}}} \\mathbb{I}(\\text{argmax}_c p_{i,c} == y_i^{\\text{label}}) $$\n",
    "    *   **Top-5 Accuracy:**\n",
    "        $$ \\text{Acc}_5 = \\frac{1}{N_{\\text{test}}} \\sum_{i=1}^{N_{\\text{test}}} \\mathbb{I}(y_i^{\\text{label}} \\in \\{\\text{top 5 predicted classes for sample } i\\}) $$\n",
    "    *   BYOL achieved SOTA results on this benchmark for self-supervised methods at the time of its publication.\n",
    "\n",
    "#### Transfer Learning Metrics\n",
    "Evaluations on various downstream tasks:\n",
    "1.  **Object Detection (e.g., PASCAL VOC, COCO):**\n",
    "    *   Mean Average Precision (mAP) at various IoU thresholds.\n",
    "2.  **Semantic Segmentation (e.g., PASCAL VOC, Cityscapes):**\n",
    "    *   Mean Intersection over Union (mIoU).\n",
    "3.  **Other Classification Tasks (e.g., iNaturalist, Places205):** Top-1 Accuracy.\n",
    "\n",
    "#### Loss Function (Monitoring during Training)\n",
    "*   **BYOL Loss Value ($\\mathcal{L}^{\\text{BYOL}}$):** The MSE loss defined above. Monitoring its decrease is essential for diagnosing training progress. Ideally, it should converge to a small value, indicating that the online network effectively predicts the target network's outputs.\n",
    "\n",
    "### Importance\n",
    "*   **Elimination of Negative Pairs:** BYOL demonstrated that high-quality representations can be learned without explicit negative sampling, simplifying the training objective compared to contrastive methods.\n",
    "*   **Robustness to Batch Size:** Performance is less sensitive to batch size variations compared to contrastive methods that rely on in-batch negatives.\n",
    "*   **State-of-the-Art Performance:** BYOL achieved competitive or superior performance compared to previous self-supervised methods on various benchmarks.\n",
    "*   **Stimulated New Research Directions:** It prompted further investigation into non-contrastive self-supervised learning and the mechanisms behind collapse prevention.\n",
    "\n",
    "### Pros versus Cons\n",
    "\n",
    "#### Pros\n",
    "*   **No Need for Negative Pairs:** Simplifies the loss function and avoids issues related to hard negative mining or large batch requirements for sufficient negatives.\n",
    "*   **Stable Training:** Generally exhibits stable training dynamics.\n",
    "*   **Strong Performance:** Achieves excellent results on linear evaluation and transfer learning tasks.\n",
    "*   **Robustness:** Less sensitive to changes in batch size and data augmentation policies compared to some contrastive methods.\n",
    "\n",
    "#### Cons\n",
    "*   **Dependence on Momentum Schedule ($\\tau$):** The performance and stability rely on the careful scheduling of the target decay rate $\\tau$. An improperly set $\\tau$ can lead to slow convergence or even collapse.\n",
    "*   **Potential for Collapse (though empirically rare with proper setup):** While designed to avoid collapse, certain configurations or failure modes could theoretically lead to it. The exact reasons for its empirical success in avoiding collapse without negatives were subjects of later detailed study.\n",
    "*   **Computational Cost:** Requires two forward passes for each view in the symmetrized loss, and two networks (online and target), though the target network does not require gradient computation.\n",
    "*   **Predictor Design:** The necessity and design of the predictor MLP add complexity compared to simpler siamese networks.\n",
    "\n",
    "### Cutting-Edge Advances\n",
    "\n",
    "1.  **SimSiam (Exploring Simple Siamese Representation Learning, 2020):**\n",
    "    *   **Simplification:** Showed that a momentum encoder (target network) is not strictly necessary. SimSiam uses a stop-gradient operation directly on one branch of a siamese network, effectively making one encoder's output the target for the other, without EMA. It also simplified BYOL by removing the momentum encoder and the predictor's batch normalization.\n",
    "    *   **Impact:** Further simplified non-contrastive self-supervised learning, highlighting the critical role of the stop-gradient operation.\n",
    "\n",
    "2.  **Deeper Understanding of BYOL's Mechanisms:**\n",
    "    *   Subsequent research provided theoretical and empirical analyses of why BYOL (and similar methods) avoid collapse. Factors like batch normalization, weight decay, the predictor, and implicit spectral regularization have been implicated. (e.g., \"Understanding Self-Supervised Learning Dynamics without Contrastive Pairs\", \"BYOL works even without Batch Statistics\").\n",
    "\n",
    "3.  **Variants and Extensions:**\n",
    "    *   Exploration of different predictor architectures and loss functions within the BYOL framework.\n",
    "    *   Application of BYOL principles to other modalities like video, audio, and reinforcement learning.\n",
    "\n",
    "4.  **DirectPred (Target Representation learning by predicting reflections, 2022):**\n",
    "    *   An alternative to EMA for target network update, where the target network predicts a reflection of the online network's prediction, aiming for faster convergence and stability.\n",
    "\n",
    "BYOL remains a foundational method in self-supervised learning, with its core ideas influencing subsequent developments in efficient and effective representation learning without explicit negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6657d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
