
# 8.1 Models and Planning

## Definition
- **Model**: In reinforcement learning (RL), a model is a representation of the environmentâ€™s dynamics, typically as a transition function $P(s', r | s, a)$, which gives the probability of transitioning to state $s'$ and receiving reward $r$ after taking action $a$ in state $s$.
- **Planning**: The process of using a model to simulate and evaluate future states and rewards, enabling the agent to improve its policy without direct interaction with the environment.

## Pertinent Equations
- **Transition Model**:  
  $$ P(s', r | s, a) = \Pr\{S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a\} $$
- **Expected Value Update (Bellman Equation)**:  
  $$ V(s) = \sum_{a} \pi(a|s) \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')] $$
- **Action-Value Update**:  
  $$ Q(s, a) = \sum_{s', r} P(s', r | s, a) [r + \gamma \max_{a'} Q(s', a')] $$

## Key Principles
- **Model-Based RL**: Utilizes a learned or given model to simulate experience, enabling planning.
- **Tabular Methods**: State and action spaces are small and discrete, allowing explicit storage of values and models.
- **Planning vs. Learning**: Planning uses the model to update value functions, while learning updates the model from real experience.

## Detailed Concept Analysis
- **Model Learning**: The agent estimates $P(s', r | s, a)$ from observed transitions.
- **Planning Algorithms**: Use the model to perform updates as if the agent had experienced the transitions directly.
- **Interaction**: Real experience updates both the model and value functions; simulated experience (planning) updates only value functions.

## Importance
- **Sample Efficiency**: Planning allows agents to improve policies with fewer real-world interactions.
- **Generalization**: Enables agents to anticipate and prepare for unobserved situations.

## Pros vs. Cons
- **Pros**:
  - Reduces need for costly real-world samples.
  - Enables rapid policy improvement.
- **Cons**:
  - Model inaccuracies can mislead planning.
  - Computational overhead for model learning and planning.

## Recent Developments
- **Hybrid Model-Based/Model-Free Methods**: Combining planning with direct learning for improved performance.
- **Deep Model Learning**: Neural networks for high-dimensional model estimation.
- **Uncertainty-Aware Planning**: Incorporating model uncertainty into planning updates.

---

# 8.2 Dyna: Integrated Planning, Acting, and Learning

## Definition
- **Dyna Architecture**: A framework integrating direct RL (learning from real experience) with planning (learning from simulated experience generated by a model).

## Pertinent Equations
- **Model Update**:  
  $$ \text{Update } \hat{P}(s', r | s, a) \text{ from real experience} $$
- **Value Update (from real or simulated experience)**:  
  $$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] $$

## Key Principles
- **Unified Experience**: Both real and simulated experiences are used to update value functions.
- **Planning Steps**: After each real step, multiple simulated steps are performed using the model.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. Take action in environment, observe $(s, a, r, s')$.
  2. Update model with $(s, a, r, s')$.
  3. Update $Q(s, a)$ with real experience.
  4. For $n$ planning steps:
     - Sample $(\tilde{s}, \tilde{a})$ from past experience.
     - Simulate $(\tilde{r}, \tilde{s}')$ using model.
     - Update $Q(\tilde{s}, \tilde{a})$.

## Importance
- **Efficiency**: Accelerates learning by leveraging both real and simulated data.
- **Flexibility**: Can be adapted to various RL settings.

## Pros vs. Cons
- **Pros**:
  - Dramatically improves sample efficiency.
  - Modular: model, planning, and learning components can be improved independently.
- **Cons**:
  - Model errors can propagate through planning.
  - Requires storage and management of experience.

## Recent Developments
- **Dyna-Q+**: Adds exploration bonuses for less-visited state-action pairs.
- **Deep Dyna**: Uses deep networks for model and value function approximation.

---

# 8.3 When the Model Is Wrong

## Definition
- **Model Error**: Discrepancy between the learned model $\hat{P}$ and the true environment dynamics $P$.

## Pertinent Equations
- **Model Error**:  
  $$ \epsilon(s, a) = \sum_{s', r} |P(s', r | s, a) - \hat{P}(s', r | s, a)| $$

## Key Principles
- **Bias in Planning**: Inaccurate models introduce bias in value estimates.
- **Compounding Error**: Errors accumulate over multiple planning steps.

## Detailed Concept Analysis
- **Sources of Error**: Limited data, function approximation, non-stationarity.
- **Mitigation**: Regular model updates, uncertainty estimation, conservative planning.

## Importance
- **Robustness**: Ensuring reliable performance despite model inaccuracies is critical for real-world deployment.

## Pros vs. Cons
- **Pros**:
  - Awareness of model error can guide safer planning.
- **Cons**:
  - Unchecked, model error can degrade policy quality.

## Recent Developments
- **Ensemble Models**: Use multiple models to estimate uncertainty.
- **Model Predictive Control (MPC)**: Re-plans at each step to mitigate error impact.

---

# 8.4 Prioritized Sweeping

## Definition
- **Prioritized Sweeping**: Planning method that focuses updates on states and actions most likely to affect value estimates, based on the magnitude of their expected change.

## Pertinent Equations
- **Priority Calculation**:  
  $$ p(s, a) = |r + \gamma \max_{a'} Q(s', a') - Q(s, a)| $$

## Key Principles
- **Efficient Planning**: Only high-priority state-action pairs are updated.
- **Backward Focus**: Updates propagate backward to predecessors.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. Compute priority for each $(s, a)$ after real experience.
  2. Insert high-priority pairs into a priority queue.
  3. Pop highest-priority pair, update $Q(s, a)$.
  4. Update priorities for predecessors.

## Importance
- **Computational Efficiency**: Reduces unnecessary updates, focusing computation where it matters most.

## Pros vs. Cons
- **Pros**:
  - Accelerates convergence.
  - Reduces computational cost.
- **Cons**:
  - Requires tracking predecessors and managing a priority queue.

## Recent Developments
- **Scalable Prioritized Sweeping**: Adaptations for large-scale and deep RL.

---

# 8.5 Expected vs. Sample Updates

## Definition
- **Expected Update**: Uses the full expectation over possible next states and rewards.
- **Sample Update**: Uses a single sample transition for the update.

## Pertinent Equations
- **Expected Update**:  
  $$ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ \sum_{s', r} P(s', r | s, a) [r + \gamma \max_{a'} Q(s', a')] - Q(s, a) \right] $$
- **Sample Update**:  
  $$ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] $$

## Key Principles
- **Variance vs. Bias**: Expected updates have lower variance but require full model; sample updates are higher variance but computationally cheaper.

## Detailed Concept Analysis
- **Trade-offs**: Expected updates are more accurate but less scalable; sample updates are scalable but noisier.

## Importance
- **Algorithm Design**: Choice affects convergence speed and computational requirements.

## Pros vs. Cons
- **Expected**:
  - Pros: Lower variance, more stable.
  - Cons: Computationally expensive.
- **Sample**:
  - Pros: Scalable, simple.
  - Cons: Higher variance.

## Recent Developments
- **Hybrid Approaches**: Combining expected and sample updates for efficiency.

---

# 8.6 Trajectory Sampling

## Definition
- **Trajectory Sampling**: Planning method that simulates entire trajectories (sequences of states, actions, rewards) using the model.

## Pertinent Equations
- **Trajectory Return**:  
  $$ G_t = \sum_{k=0}^{T-t-1} \gamma^k r_{t+k+1} $$

## Key Principles
- **Monte Carlo Planning**: Uses sampled trajectories to estimate value functions.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. Simulate trajectory from $(s, a)$ using model.
  2. Compute return $G_t$.
  3. Update value estimates based on $G_t$.

## Importance
- **Long-Term Planning**: Captures long-term effects of actions.

## Pros vs. Cons
- **Pros**:
  - Captures full trajectory effects.
- **Cons**:
  - High variance, computationally intensive.

## Recent Developments
- **Variance Reduction**: Techniques to stabilize trajectory-based planning.

---

# 8.7 Real-time Dynamic Programming (RTDP)

## Definition
- **RTDP**: Online planning algorithm that updates value estimates along simulated or real trajectories, focusing computation on relevant states.

## Pertinent Equations
- **RTDP Update**:  
  $$ V(s) \leftarrow \max_{a} \sum_{s', r} P(s', r | s, a) [r + \gamma V(s')] $$

## Key Principles
- **Focused Updates**: Only updates states encountered along trajectories.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. Start from initial state.
  2. Simulate or follow trajectory.
  3. Update values along the way.

## Importance
- **Scalability**: Efficient for large state spaces where only a subset is relevant.

## Pros vs. Cons
- **Pros**:
  - Focuses computation.
- **Cons**:
  - May miss important states if not explored.

## Recent Developments
- **Heuristic-Guided RTDP**: Uses heuristics to guide exploration.

---

# 8.8 Planning at Decision Time

## Definition
- **Planning at Decision Time**: Planning is performed online, at each decision point, rather than offline.

## Pertinent Equations
- **Online Planning**:  
  $$ a^* = \arg\max_{a} \mathbb{E}[G_t | s, a] $$

## Key Principles
- **Real-Time Constraints**: Planning must be fast enough for real-time action selection.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. At each decision, simulate possible futures.
  2. Select action maximizing expected return.

## Importance
- **Practicality**: Enables deployment in real-world, time-sensitive applications.

## Pros vs. Cons
- **Pros**:
  - Adapts to current situation.
- **Cons**:
  - Limited by computational budget.

## Recent Developments
- **Anytime Algorithms**: Planning algorithms that can return a valid action at any time.

---

# 8.9 Heuristic Search

## Definition
- **Heuristic Search**: Planning method using heuristic functions to guide search through the state space.

## Pertinent Equations
- **Heuristic Function**:  
  $$ h(s) \approx V^*(s) $$

## Key Principles
- **Guided Exploration**: Heuristics focus search on promising states.

## Detailed Concept Analysis
- **Algorithm Examples**: A*, AO*, LAO*.

## Importance
- **Efficiency**: Reduces search space, accelerates planning.

## Pros vs. Cons
- **Pros**:
  - Efficient for large spaces.
- **Cons**:
  - Quality depends on heuristic accuracy.

## Recent Developments
- **Learning Heuristics**: Using RL to learn effective heuristics.

---

# 8.10 Rollout Algorithms

## Definition
- **Rollout Algorithm**: Uses a base policy to simulate trajectories (rollouts) from the current state, evaluating actions by averaging returns.

## Pertinent Equations
- **Rollout Value**:  
  $$ Q^{\text{rollout}}(s, a) = \frac{1}{N} \sum_{i=1}^N G_t^{(i)} $$

## Key Principles
- **Policy Improvement**: Rollouts provide action value estimates for policy improvement.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. For each action, simulate $N$ rollouts using base policy.
  2. Average returns to estimate $Q(s, a)$.
  3. Select action with highest estimated value.

## Importance
- **Policy Evaluation**: Effective for evaluating and improving policies.

## Pros vs. Cons
- **Pros**:
  - Simple, effective.
- **Cons**:
  - Computationally expensive for large $N$.

## Recent Developments
- **Parallel Rollouts**: Using parallel computation to scale rollout algorithms.

---

# 8.11 Monte Carlo Tree Search (MCTS)

## Definition
- **MCTS**: Planning algorithm that builds a search tree by sampling trajectories, balancing exploration and exploitation.

## Pertinent Equations
- **UCT (Upper Confidence Bound for Trees)**:  
  $$ a^* = \arg\max_{a} \left[ Q(s, a) + c \sqrt{\frac{\ln N(s)}{N(s, a)}} \right] $$

## Key Principles
- **Tree Expansion**: Expands tree nodes based on simulated rollouts.
- **Exploration-Exploitation Tradeoff**: UCT balances between trying new actions and exploiting known good actions.

## Detailed Concept Analysis
- **Algorithm Steps**:
  1. **Selection**: Traverse tree using UCT.
  2. **Expansion**: Add new node to tree.
  3. **Simulation**: Simulate rollout from new node.
  4. **Backpropagation**: Update values up the tree.

## Importance
- **State-of-the-Art Planning**: Powers top-performing agents in complex domains (e.g., AlphaGo).

## Pros vs. Cons
- **Pros**:
  - Scalable, effective in large spaces.
- **Cons**:
  - Computationally intensive.

## Recent Developments
- **Neural MCTS**: Integrates deep learning for value and policy estimation within MCTS.
