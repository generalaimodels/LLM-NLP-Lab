---

# Chapter 3â€ƒShallow Latent Variable Models

---

## Principal Component Analysis (PCA)

### Definition

Principal Component Analysis (PCA) is a linear dimensionality reduction technique that projects data onto a lower-dimensional subspace such that the variance of the projected data is maximized. It is a shallow latent variable model where the observed data is assumed to be generated by a linear transformation of lower-dimensional latent variables plus isotropic Gaussian noise.

---

### Mathematical Formulation

Given a dataset $X \in \mathbb{R}^{n \times d}$ (with $n$ samples and $d$ features), PCA seeks a projection matrix $W \in \mathbb{R}^{d \times k}$ ($k < d$) such that the projected data $Z = XW$ maximizes the variance.

#### Objective Function

Maximize the variance of the projected data:
$$
\max_{W \in \mathbb{R}^{d \times k}} \operatorname{Tr}(W^\top S W)
$$
subject to
$$
W^\top W = I_k
$$
where $S = \frac{1}{n} X^\top X$ is the empirical covariance matrix.

#### Eigenvalue Decomposition

The optimal $W$ consists of the top $k$ eigenvectors of $S$ corresponding to the largest $k$ eigenvalues.

#### Probabilistic PCA

Assume a generative model:
$$
x = Wz + \mu + \epsilon
$$
where
- $z \sim \mathcal{N}(0, I_k)$ (latent variable)
- $\epsilon \sim \mathcal{N}(0, \sigma^2 I_d)$ (isotropic noise)
- $W \in \mathbb{R}^{d \times k}$ (loading matrix)
- $\mu \in \mathbb{R}^d$ (mean)

The marginal likelihood:
$$
p(x) = \mathcal{N}(x \mid \mu, WW^\top + \sigma^2 I_d)
$$

---

### Step-by-Step Explanation

1. **Data Centering**: Subtract the mean $\mu$ from each data point.
2. **Covariance Matrix Computation**: Compute $S = \frac{1}{n} X^\top X$.
3. **Eigenvalue Decomposition**: Solve $S v = \lambda v$ for eigenvalues $\lambda$ and eigenvectors $v$.
4. **Projection Matrix Construction**: Select $k$ eigenvectors with largest eigenvalues to form $W$.
5. **Data Projection**: Compute $Z = XW$ for reduced representation.
6. **Reconstruction (optional)**: Approximate original data as $\hat{X} = ZW^\top + \mu$.
7. **Probabilistic Interpretation**: Fit $W$ and $\sigma^2$ by maximizing the likelihood of the observed data under the probabilistic PCA model.

---

### Key Properties

- **Orthogonality**: Principal components are orthogonal.
- **Variance Maximization**: Each principal component captures maximal remaining variance.
- **Linear Transform**: PCA is strictly linear.
- **Latent Variable Model**: Latent variables $z$ are uncorrelated and have unit variance.

---

## Factor Analysis (FA)

### Definition

Factor Analysis (FA) is a generative latent variable model that explains observed variables as linear combinations of latent factors plus independent Gaussian noise, where the noise can have different variances for each observed variable.

---

### Mathematical Formulation

Generative model:
$$
x = \Lambda z + \mu + \epsilon
$$
where
- $z \sim \mathcal{N}(0, I_k)$ (latent factors)
- $\epsilon \sim \mathcal{N}(0, \Psi)$, $\Psi = \operatorname{diag}(\psi_1, \ldots, \psi_d)$ (diagonal noise covariance)
- $\Lambda \in \mathbb{R}^{d \times k}$ (factor loading matrix)
- $\mu \in \mathbb{R}^d$ (mean)

Marginal distribution:
$$
p(x) = \mathcal{N}(x \mid \mu, \Lambda \Lambda^\top + \Psi)
$$

---

### Step-by-Step Explanation

1. **Data Centering**: Subtract mean $\mu$ from each data point.
2. **Model Specification**: Assume $x = \Lambda z + \mu + \epsilon$.
3. **Parameter Estimation**:
   - **Expectation-Maximization (EM) Algorithm**:
     - **E-step**: Compute posterior $p(z \mid x)$ using current parameters.
     - **M-step**: Update $\Lambda$ and $\Psi$ to maximize expected complete-data log-likelihood.
   - **Closed-form Updates** (in EM):
     - Posterior mean: $E[z \mid x] = M^{-1} \Lambda^\top \Psi^{-1} (x - \mu)$, where $M = I_k + \Lambda^\top \Psi^{-1} \Lambda$.
     - Update $\Lambda$ and $\Psi$ using sufficient statistics from the E-step.
4. **Covariance Structure**: $\operatorname{Cov}(x) = \Lambda \Lambda^\top + \Psi$.
5. **Interpretation**: Each observed variable is modeled as a linear combination of latent factors plus unique noise.

---

### Key Properties

- **Noise Model**: Unlike PCA, FA allows for variable-specific noise variances.
- **Identifiability**: Factor loadings are not unique; rotation ambiguity exists.
- **Latent Variable Model**: Latent factors are uncorrelated and have unit variance.
- **Probabilistic**: FA is a fully probabilistic model, suitable for maximum likelihood estimation.

---

## Independent Component Analysis (ICA)

### Definition

Independent Component Analysis (ICA) is a latent variable model that seeks a linear transformation of observed data such that the resulting components are statistically independent and non-Gaussian. ICA is primarily used for blind source separation.

---

### Mathematical Formulation

Assume observed data $x \in \mathbb{R}^d$ is generated as:
$$
x = As
$$
where
- $A \in \mathbb{R}^{d \times d}$ (mixing matrix)
- $s \in \mathbb{R}^d$ (source vector with independent, non-Gaussian components)

Goal: Estimate $A$ (or its inverse $W = A^{-1}$) such that $s = Wx$ are as independent as possible.

---

#### Independence Criterion

Maximize statistical independence of components $s_i$:
$$
p(s) = \prod_{i=1}^d p_i(s_i)
$$

#### Non-Gaussianity Maximization

Common objective: maximize non-Gaussianity (e.g., kurtosis, negentropy) of $s_i$.

- **Kurtosis**: $\operatorname{Kurt}(y) = E[y^4] - 3(E[y^2])^2$
- **Negentropy**: $J(y) = H(y_{\text{gauss}}) - H(y)$, where $H$ is differential entropy.

---

### Step-by-Step Explanation

1. **Data Centering**: Subtract mean from $x$.
2. **Whitening**: Transform $x$ so that $\operatorname{Cov}(x) = I$ (e.g., via PCA).
3. **Estimate Unmixing Matrix**: Find $W$ such that $s = Wx$ are independent.
   - **FastICA Algorithm**:
     - Initialize $w$ randomly.
     - Iteratively update:
       $$
       w \leftarrow E[x g(w^\top x)] - E[g'(w^\top x)] w
       $$
       where $g$ is a non-linear function (e.g., $g(y) = \tanh(y)$).
     - Normalize $w$.
     - Decorrelate $w$ vectors.
4. **Recover Sources**: $s = Wx$.
5. **Ambiguities**:
   - **Scaling**: Each $s_i$ can be scaled arbitrarily.
   - **Permutation**: Order of $s_i$ is indeterminate.

---

### Key Properties

- **Independence**: Components are statistically independent.
- **Non-Gaussianity**: ICA exploits non-Gaussianity for identifiability.
- **Linear Transform**: ICA assumes a linear mixing model.
- **Ambiguities**: Scaling and permutation of sources are unidentifiable.

---

## Summary Table

| Model | Generative Assumption | Noise | Latent Variable | Identifiability | Objective |
|-------|----------------------|-------|-----------------|-----------------|-----------|
| PCA   | $x = Wz + \mu + \epsilon$ | Isotropic ($\sigma^2 I$) | Uncorrelated, unit variance | Unique up to rotation | Maximize variance |
| FA    | $x = \Lambda z + \mu + \epsilon$ | Diagonal ($\Psi$) | Uncorrelated, unit variance | Rotation ambiguity | Maximize likelihood |
| ICA   | $x = As$ | None | Independent, non-Gaussian | Scaling, permutation | Maximize independence |

---