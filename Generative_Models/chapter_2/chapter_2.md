
# Chapter 2â€ƒClassical Statistical Generative Models

---

## Gaussian Mixture Models (GMM)

### Definition

A **Gaussian Mixture Model (GMM)** is a probabilistic model that assumes all data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.

---

### Mathematical Formulation

- Let $x \in \mathbb{R}^d$ be a $d$-dimensional data point.
- The GMM models the probability density as:
  $$
  p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)
  $$
  where:
  - $K$ = number of mixture components
  - $\pi_k$ = mixture weight for component $k$, $\sum_{k=1}^K \pi_k = 1$, $\pi_k \geq 0$
  - $\mathcal{N}(x \mid \mu_k, \Sigma_k)$ = multivariate Gaussian:
    $$
    \mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu)^\top \Sigma^{-1} (x-\mu) \right)
    $$

---

### Step-by-Step Explanation

#### 1. **Model Specification**
   - Each data point is assumed to be generated by first selecting a component $k$ with probability $\pi_k$, then sampling $x$ from $\mathcal{N}(\mu_k, \Sigma_k)$.

#### 2. **Latent Variable Representation**
   - Introduce latent variable $z \in \{1, \ldots, K\}$ indicating the component.
   - Joint distribution:
     $$
     p(x, z) = \pi_z \mathcal{N}(x \mid \mu_z, \Sigma_z)
     $$
   - Marginalize $z$ to recover $p(x)$.

#### 3. **Parameter Estimation (EM Algorithm)**
   - **Goal:** Estimate $\{\pi_k, \mu_k, \Sigma_k\}_{k=1}^K$ given data $\{x_i\}_{i=1}^N$.
   - **E-step:** Compute responsibilities (posterior probabilities):
     $$
     \gamma_{ik} = p(z_i = k \mid x_i) = \frac{\pi_k \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}
     $$
   - **M-step:** Update parameters:
     $$
     N_k = \sum_{i=1}^N \gamma_{ik}
     $$
     $$
     \pi_k = \frac{N_k}{N}
     $$
     $$
     \mu_k = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} x_i
     $$
     $$
     \Sigma_k = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^\top
     $$
   - Iterate E and M steps until convergence.

#### 4. **Inference**
   - For a new $x$, compute posterior over components:
     $$
     p(z = k \mid x) = \frac{\pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x \mid \mu_j, \Sigma_j)}
     $$

#### 5. **Model Selection**
   - Use criteria such as BIC/AIC to select $K$.

#### 6. **Applications**
   - Clustering, density estimation, anomaly detection, speaker identification.

---

## Hidden Markov Models (HMM)

### Definition

A **Hidden Markov Model (HMM)** is a generative probabilistic model for sequential data, where the system is assumed to be a Markov process with unobserved (hidden) states.

---

### Mathematical Formulation

- Let $X = (x_1, \ldots, x_T)$ be the observed sequence.
- Let $Z = (z_1, \ldots, z_T)$ be the hidden state sequence, $z_t \in \{1, \ldots, K\}$.
- Model parameters:
  - Initial state distribution: $\pi_k = p(z_1 = k)$
  - Transition matrix: $A_{ij} = p(z_{t+1} = j \mid z_t = i)$
  - Emission distribution: $B_k(x) = p(x_t = x \mid z_t = k)$

- Joint probability:
  $$
  p(X, Z) = \pi_{z_1} B_{z_1}(x_1) \prod_{t=2}^T A_{z_{t-1}, z_t} B_{z_t}(x_t)
  $$

---

### Step-by-Step Explanation

#### 1. **Model Specification**
   - At each time $t$:
     - Hidden state $z_t$ evolves according to Markov property: $p(z_t \mid z_{t-1})$.
     - Observation $x_t$ is generated from emission distribution conditioned on $z_t$.

#### 2. **Parameter Estimation (Baum-Welch / EM Algorithm)**
   - **Goal:** Estimate $\{\pi, A, B\}$ given observed sequences.
   - **E-step:** Compute expected sufficient statistics using the forward-backward algorithm.
     - **Forward variable:**
       $$
       \alpha_t(k) = p(x_1, \ldots, x_t, z_t = k)
       $$
     - **Backward variable:**
       $$
       \beta_t(k) = p(x_{t+1}, \ldots, x_T \mid z_t = k)
       $$
     - **Posterior:**
       $$
       \gamma_t(k) = p(z_t = k \mid X) = \frac{\alpha_t(k) \beta_t(k)}{\sum_{j=1}^K \alpha_t(j) \beta_t(j)}
       $$
     - **Transition posterior:**
       $$
       \xi_t(i, j) = p(z_t = i, z_{t+1} = j \mid X)
       $$
   - **M-step:** Update parameters:
     $$
     \pi_k = \gamma_1(k)
     $$
     $$
     A_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i, j)}{\sum_{t=1}^{T-1} \gamma_t(i)}
     $$
     $$
     B_k(x) = \frac{\sum_{t=1}^T \gamma_t(k) \mathbb{I}[x_t = x]}{\sum_{t=1}^T \gamma_t(k)}
     $$
   - Iterate until convergence.

#### 3. **Inference**
   - **Decoding (Viterbi Algorithm):** Find most probable state sequence:
     $$
     Z^* = \arg\max_Z p(Z \mid X)
     $$
   - **Likelihood Computation:** Use forward algorithm to compute $p(X)$.

#### 4. **Extensions**
   - Continuous emissions (e.g., Gaussian HMMs)
   - Higher-order HMMs
   - Input-output HMMs

#### 5. **Applications**
   - Speech recognition, POS tagging, bioinformatics, time series modeling.

---

## Probabilistic Context-Free Grammars (PCFG)

### Definition

A **Probabilistic Context-Free Grammar (PCFG)** is a context-free grammar where each production rule is assigned a probability, defining a probability distribution over all possible derivations.

---

### Mathematical Formulation

- A PCFG is a 5-tuple $(N, \Sigma, R, S, P)$:
  - $N$: set of non-terminal symbols
  - $\Sigma$: set of terminal symbols
  - $R$: set of production rules $A \rightarrow \alpha$, $A \in N$, $\alpha \in (N \cup \Sigma)^*$
  - $S$: start symbol, $S \in N$
  - $P$: rule probabilities, $P(A \rightarrow \alpha)$, with $\sum_{\alpha} P(A \rightarrow \alpha) = 1$ for each $A$

- Probability of a derivation (parse tree) $T$:
  $$
  P(T) = \prod_{r \in T} P(r)
  $$
  where $r$ ranges over all rules used in $T$.

- Probability of a string $w$:
  $$
  P(w) = \sum_{T \in \text{parses}(w)} P(T)
  $$

---

### Step-by-Step Explanation

#### 1. **Model Specification**
   - Each production rule $A \rightarrow \alpha$ is associated with a probability $P(A \rightarrow \alpha)$.
   - The sum of probabilities of all rules expanding a given non-terminal $A$ is 1.

#### 2. **Parsing and Inference**
   - **Parsing:** Find all possible parse trees for a string $w$.
   - **Inside Algorithm:** Dynamic programming to compute $P(w)$.
     - Let $X_{i,j}^A$ be the probability that non-terminal $A$ generates substring $w_i \ldots w_j$:
       $$
       X_{i,j}^A = \sum_{A \rightarrow BC} P(A \rightarrow BC) \sum_{k=i}^{j-1} X_{i,k}^B X_{k+1,j}^C
       $$
       For terminals:
       $$
       X_{i,i}^A = P(A \rightarrow w_i)
       $$
   - **Viterbi Algorithm:** Find the most probable parse tree.

#### 3. **Parameter Estimation (Inside-Outside Algorithm)**
   - **Goal:** Estimate rule probabilities $P(A \rightarrow \alpha)$ from a corpus.
   - **E-step:** Compute expected counts of each rule using inside-outside probabilities.
   - **M-step:** Update rule probabilities:
     $$
     P(A \rightarrow \alpha) = \frac{\text{expected count of } A \rightarrow \alpha}{\sum_{\alpha'} \text{expected count of } A \rightarrow \alpha'}
     $$

#### 4. **Applications**
   - Natural language parsing, syntax modeling, biological sequence analysis.

---

# Summary Table

| Model | Latent Structure | Data Type | Inference | Learning | Applications |
|-------|------------------|-----------|-----------|----------|--------------|
| GMM   | Mixture component | Vectors   | EM        | EM       | Clustering, density estimation |
| HMM   | State sequence    | Sequences | Forward-Backward, Viterbi | EM (Baum-Welch) | Speech, time series |
| PCFG  | Parse tree        | Strings   | Inside, Viterbi | Inside-Outside | Parsing, syntax |

---