---

# Probabilistic Context-Free Grammars (PCFG)

---

## Definition

A **Probabilistic Context-Free Grammar (PCFG)** is a generative model that extends context-free grammars (CFGs) by associating a probability with each production rule. This defines a probability distribution over all possible derivations (parse trees) and, consequently, over all strings generated by the grammar.

---

## Mathematical Formulation

A PCFG is defined as a 5-tuple:

$$
G = (N, \Sigma, R, S, P)
$$

where:
- $N$: finite set of non-terminal symbols.
- $\Sigma$: finite set of terminal symbols.
- $R$: finite set of production rules of the form $A \rightarrow \alpha$, where $A \in N$, $\alpha \in (N \cup \Sigma)^*$.
- $S \in N$: start symbol.
- $P$: rule probability function, $P(A \rightarrow \alpha)$, with $\sum_{\alpha: (A \rightarrow \alpha) \in R} P(A \rightarrow \alpha) = 1$ for each $A \in N$.

**Probability of a derivation (parse tree) $T$:**

$$
P(T) = \prod_{r \in T} P(r)
$$

where $r$ ranges over all production rules used in $T$.

**Probability of a string $w$:**

$$
P(w) = \sum_{T \in \text{parses}(w)} P(T)
$$

---

## Step-by-Step Explanation

### 1. **Model Specification**

- Each non-terminal $A$ can be expanded using one of its production rules $A \rightarrow \alpha$ with probability $P(A \rightarrow \alpha)$.
- The sum of probabilities for all rules expanding a given non-terminal is 1:
  $$
  \sum_{\alpha} P(A \rightarrow \alpha) = 1, \quad \forall A \in N
  $$
- The generative process recursively applies production rules, starting from $S$, until a string of terminals is produced.

---

### 2. **Parsing and Inference**

#### a. **Parsing: Enumerating Parse Trees**

- For a given string $w = w_1 w_2 \ldots w_n$, there may be multiple parse trees (derivations) under the grammar.
- The probability of $w$ is the sum of the probabilities of all its parse trees.

#### b. **Inside Algorithm (Dynamic Programming for $P(w)$)**

- Let $X_{i,j}^A$ denote the probability that non-terminal $A$ generates substring $w_i \ldots w_j$.
- **Base case (length 1):**
  $$
  X_{i,i}^A = P(A \rightarrow w_i)
  $$
- **Recursive case (length $l > 1$):**
  $$
  X_{i,j}^A = \sum_{A \rightarrow BC} P(A \rightarrow BC) \sum_{k=i}^{j-1} X_{i,k}^B X_{k+1,j}^C
  $$
- **Total probability:**
  $$
  P(w) = X_{1,n}^S
  $$

#### c. **Viterbi Algorithm (Most Probable Parse Tree)**

- Replace summations with maximizations to find the most probable parse tree:
  $$
  V_{i,j}^A = \max_{A \rightarrow BC} \max_{k=i}^{j-1} \left[ P(A \rightarrow BC) V_{i,k}^B V_{k+1,j}^C \right]
  $$
- Backtrack to recover the highest-probability parse.

---

### 3. **Parameter Estimation (Inside-Outside Algorithm / EM for PCFGs)**

Given a corpus of sentences (possibly unparsed), estimate rule probabilities $P(A \rightarrow \alpha)$.

#### a. **E-Step: Expected Rule Counts**

- Compute expected number of times each rule $A \rightarrow \alpha$ is used in generating the corpus, using inside and outside probabilities.
- **Inside probability:** $X_{i,j}^A$ as above.
- **Outside probability:** $Y_{i,j}^A$ is the probability that $A$ spans $w_i \ldots w_j$ as part of the derivation of $w$.
- **Expected count for rule $A \rightarrow BC$ at span $(i, j)$ split at $k$:**
  $$
  \text{Count}(A \rightarrow BC) = \frac{Y_{i,j}^A \cdot P(A \rightarrow BC) \cdot X_{i,k}^B \cdot X_{k+1,j}^C}{P(w)}
  $$

#### b. **M-Step: Update Rule Probabilities**

- For each non-terminal $A$:
  $$
  P(A \rightarrow \alpha) = \frac{\text{expected count of } A \rightarrow \alpha}{\sum_{\alpha'} \text{expected count of } A \rightarrow \alpha'}
  $$

- Iterate E and M steps until convergence.

---

### 4. **Properties and Limitations**

- **Expressiveness:** PCFGs can model hierarchical, recursive structures with probabilistic ambiguity.
- **Context-Freeness:** PCFGs cannot capture context-sensitive dependencies.
- **Independence Assumptions:** Rule applications are independent given the non-terminal.
- **Ambiguity:** Multiple parse trees for a string; PCFGs provide a principled way to resolve ambiguity probabilistically.

---

### 5. **Extensions**

- **Lexicalized PCFGs:** Condition rule probabilities on headwords or lexical items.
- **Parent Annotation:** Condition on parent non-terminal to capture more context.
- **Nonparametric PCFGs:** Use Bayesian nonparametrics (e.g., Dirichlet Process) for infinite rule sets.
- **Discriminative Training:** Maximize conditional likelihood $P(T \mid w)$ instead of joint.

---

### 6. **Applications**

- **Natural Language Parsing:** Syntactic analysis of sentences.
- **Syntax-Based Machine Translation:** Tree-to-tree translation models.
- **Biological Sequence Analysis:** RNA secondary structure prediction.
- **Program Analysis:** Parsing programming languages with probabilistic ambiguity.

---

## Summary Table

| Component | Description |
|-----------|-------------|
| $N$ | Non-terminal symbols |
| $\Sigma$ | Terminal symbols |
| $R$ | Production rules $A \rightarrow \alpha$ |
| $S$ | Start symbol |
| $P(A \rightarrow \alpha)$ | Probability of rule $A \rightarrow \alpha$ |
| $X_{i,j}^A$ | Inside probability for $A$ spanning $w_i \ldots w_j$ |
| $Y_{i,j}^A$ | Outside probability for $A$ spanning $w_i \ldots w_j$ |

---