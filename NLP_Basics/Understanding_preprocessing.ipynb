{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNaEGj7KFdys4oRFYZhQ7A2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["- **Author:** **Hemanth K**\n","- **✉** **speechcodehemanth2@gmail.com**\n","- Ai Researcher\n"],"metadata":{"id":"wyE6yprCTa09"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-64bdFCeG8XB"}},{"cell_type":"markdown","source":["# Word Structure and Subword Models in Natural Language Processing\n","\n","In this exposition, we delve into the concept of word structure and subword models, which are foundational to modern Natural Language Processing (NLP) systems, particularly in the context of Large Language Models (LLMs) and Generative AI (Gen-AI). These models are crucial for handling linguistic diversity, out-of-vocabulary (OOV) words, and efficient representation learning. We will cover definitions, mathematical formulations, and detailed explanations of the topic, ensuring a comprehensive understanding for AI researchers and scientists.\n","\n","---\n","\n","## 1. Definition of Word Structure and Subword Models\n","\n","### 1.1 Word Structure\n","Word structure refers to the morphological composition of words, encompassing their roots, prefixes, suffixes, and other subcomponents. In linguistics, words are often decomposed into smaller meaningful units called *morphemes*.\n","- For example, the word \"unhappiness\" can be broken down into \"un-\" (prefix), \"happy\" (root), and \"-ness\" (suffix). Understanding word structure is critical for tasks like part-of-speech tagging, machine translation, and semantic understanding.\n","\n","### 1.2 Subword Models\n","Subword models are techniques in NLP that represent words as sequences of smaller, linguistically or statistically derived units, rather than treating words as atomic entities. `These models aim to balance the trade-off between vocabulary size and generalization`, especially for morphologically rich languages (e.g., German, Finnish) or rare words. Subword models are widely used in modern LLMs, such as BERT, GPT, and T5, to tokenize and encode text efficiently.\n","\n","---\n","\n","## 2. Importance of Subword Models in NLP\n","\n","**Subword models address several challenges in traditional word-based or character-based representations**:\n","- **Out-of-Vocabulary (OOV) Words**: Traditional word-based models struggle with unseen words, requiring large vocabularies. Subword models mitigate this by breaking words into smaller, reusable units.\n","- **Morphological Richness**: Languages with complex morphology (e.g., agglutinative languages) benefit from subword units that capture shared morphemes.\n","- **Efficiency**: Subword models reduce vocabulary size while maintaining expressiveness, optimizing memory and computation during training and inference.\n","- **Generalization**: Subword units enable models to generalize to unseen words by learning patterns in subword composition.\n","\n","---\n","\n","## 3. Mathematical Foundations of Subword Models\n","\n","Subword models rely on `probabilistic, statistical, or optimization-based techniques` to decompose words into subword units. Below, we outline the mathematical principles underlying key subword algorithms, such as Byte-Pair Encoding (BPE) and WordPiece.\n","\n","### 3.1 Tokenization as an Optimization Problem\n","The goal of subword tokenization is to represent a corpus $C$ as a sequence of tokens $T = \\{t_1, t_2, \\dots, t_n\\}$, where each token $t_i$ is either a word, subword, or character. **The objective is to minimize the vocabulary size $V$ while maximizing the coverage of the corpus, subject to constraints on model complexity**.\n","\n","\n","\n","\n","\n","Mathematically, the problem can be framed as an optimization problem:\n","$$\n","\\text{minimize } |V| \\text{ subject to } P(C|T, V) \\geq \\epsilon\n","$$\n","where:\n","- $ |V| $ is the size of the vocabulary.\n","- $ P(C|T, V) $ is the probability of reconstructing the corpus $C$ given the token sequence $T$ and vocabulary $V$.\n","- $ \\epsilon $ is a threshold for acceptable coverage.\n","\n","### 3.2 Probability of a Corpus\n","The probability of a corpus $C$ given a tokenization $T$ can be modeled using a language model. For a sequence of tokens $T = \\{t_1, t_2, \\dots, t_n\\}$, the joint probability is:\n","$$\n","P(T) = \\prod_{i=1}^n P(t_i | t_1, t_2, \\dots, t_{i-1})\n","$$\n","Subword models aim to maximize this probability by selecting a vocabulary $V$ that captures frequent and meaningful patterns in the data.\n","\n","---\n","\n","## 4. Subword Model Algorithms\n","\n","Subword models employ various algorithms to generate subword units. Below, we discuss the most widely used approaches: Byte-Pair Encoding (BPE), WordPiece, and Unigram Language Model.\n","\n","### 4.1 Byte-Pair Encoding (BPE)\n","\n","#### 4.1.1 Definition\n","BPE is a data compression technique adapted for NLP to iteratively merge frequent pairs of characters or subword units into a single token. It starts with a character-level representation of the corpus and builds a vocabulary of subword units.\n","\n","#### 4.1.2 Algorithm\n","1. **Initialization**: Start with the corpus $C$ represented as a sequence of characters, with each word separated by a special end-of-word symbol (e.g., `_`).\n","2. **Frequency Counting**: Compute the frequency of all adjacent pairs of tokens in the corpus.\n","3. **Merging**: Identify the most frequent pair $ (a, b) $ and merge it into a single token $ ab $.\n","4. **Iteration**: Repeat steps 2–3 for a fixed number of iterations or until a desired vocabulary size is reached.\n","5. **Tokenization**: Use the final vocabulary to tokenize new text by greedily matching the longest possible subword units.\n","\n","#### 4.1.3 Mathematical Formulation\n","Let $ F(a, b) $ denote the frequency of the pair $ (a, b) $ in the corpus. At each iteration, BPE selects the pair that maximizes:\n","$$\n","(a^*, b^*) = \\arg \\max_{a, b} F(a, b)\n","$$\n","The merged token $ ab $ is added to the vocabulary, and all occurrences of $ (a, b) $ in the corpus are replaced with $ ab $.\n","\n","#### 4.1.4 Example\n","Consider the corpus: `low`, `lowest`, `new`, `newer`.  \n","- Initial representation: `l o w _`, `l o w e s t _`, `n e w _`, `n e w e r _`.\n","- Step 1: Merge `e w` (most frequent pair) → Vocabulary: `{e, w, ew}`.\n","- Step 2: Merge `l o` → Vocabulary: `{e, w, ew, l, o, lo}`.\n","- Continue until the desired vocabulary size is reached.\n","\n","### 4.2 WordPiece\n","\n","#### 4.2.1 Definition\n","WordPiece is a subword tokenization algorithm that selects subword units based on their likelihood of improving the language model's performance. It is used in models like BERT.\n","\n","#### 4.2.2 Algorithm\n","1. **Initialization**: Start with a character-level vocabulary.\n","2. **Scoring**: For each potential merge of two tokens $ (a, b) $ into $ ab $, compute the increase in the likelihood of the corpus under a language model.\n","3. **Merging**: Select the merge that maximizes the likelihood and add the new token $ ab $ to the vocabulary.\n","4. **Iteration**: Repeat steps 2–3 until the desired vocabulary size is reached.\n","5. **Tokenization**: Use the final vocabulary to tokenize text by greedily matching the longest possible subword units.\n","\n","#### 4.2.3 Mathematical Formulation\n","WordPiece maximizes the likelihood of the corpus $C$ under a unigram language model. For a token $t$, its probability is:\n","$$\n","P(t) = \\frac{\\text{freq}(t)}{\\sum_{t' \\in V} \\text{freq}(t')}\n","$$\n","The likelihood of the corpus is the product of the probabilities of its tokens. At each iteration, WordPiece selects the merge $ (a, b) \\rightarrow ab $ that maximizes the increase in likelihood:\n","$$\n","\\Delta L = L(C|V \\cup \\{ab\\}) - L(C|V)\n","$$\n","where $ L(C|V) $ is the log-likelihood of the corpus given the current vocabulary $V$.\n","\n","#### 4.2.4 Example\n","For the same corpus as above, WordPiece might prioritize merges like `lo` or `est` if they improve the language model's likelihood more than pairs like `ew`.\n","\n","### 4.3 Unigram Language Model\n","\n","#### 4.3.1 Definition\n","The Unigram Language Model is a probabilistic approach to subword tokenization, used in models like SentencePiece. It assumes that tokens are generated independently and selects a vocabulary that maximizes the likelihood of the corpus.\n","\n","#### 4.3.2 Algorithm\n","1. **Initialization**: Start with a large seed vocabulary (e.g., all possible character sequences up to a certain length).\n","2. **Scoring**: Compute the unigram probability of each token in the vocabulary.\n","3. **Pruning**: Remove the least probable tokens from the vocabulary, keeping only the top $k$ tokens that maximize the corpus likelihood.\n","4. **Iteration**: Repeat steps 2–3 until the desired vocabulary size is reached.\n","5. **Tokenization**: Use the Viterbi algorithm to find the most probable tokenization of a word given the final vocabulary.\n","\n","#### 4.3.3 Mathematical Formulation\n","The probability of a word $w$ being tokenized as a sequence of subword units $ T = \\{t_1, t_2, \\dots, t_m\\} $ is:\n","$$\n","P(w|T) = \\prod_{i=1}^m P(t_i)\n","$$\n","The goal is to find the vocabulary $V$ and tokenization $T$ that maximize the likelihood of the corpus $C$:\n","$$\n","V^* = \\arg \\max_V \\prod_{w \\in C} \\max_T P(w|T, V)\n","$$\n","\n","#### 4.3.4 Example\n","For the word `lowest`, the Unigram model might consider tokenizations like `low est`, `lo west`, or `l o w e s t`, and select the one with the highest probability based on the learned token probabilities.\n","\n","---\n","\n","## 5. Subword Models in Practice\n","\n","### 5.1 Integration with Neural Networks\n","Subword models are typically used as a preprocessing step in NLP pipelines. The tokenized subword units are mapped to embeddings, which are then fed into neural networks (e.g., Transformers). The embedding of a token $t$ is represented as a vector $ \\mathbf{e}_t \\in \\mathbb{R}^d $, where $d$ is the embedding dimension.\n","\n","### 5.2 Handling OOV Words\n","For a new word $w$, subword models tokenize it into a sequence of subword units $ T = \\{t_1, t_2, \\dots, t_m\\} $. The word's representation is often the sum or average of the embeddings of its subword units:\n","$$\n","\\mathbf{e}_w = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{e}_{t_i}\n","$$\n","\n","### 5.3 Evaluation Metrics\n","Subword models are evaluated based on:\n","- **Vocabulary Size**: Smaller vocabularies are preferred for efficiency.\n","- **Coverage**: The percentage of the corpus that can be tokenized without OOV tokens.\n","- **Perplexity**: The perplexity of a language model trained on the tokenized corpus.\n","- **Downstream Task Performance**: Accuracy, F1-score, or BLEU score on tasks like machine translation or text classification.\n","\n","---\n","\n","## 6. Challenges and Limitations\n","\n","- **Language Dependence**: Subword models may struggle with languages that have irregular morphology or lack clear morpheme boundaries.\n","- **Over-Segmentation**: Rare words may be tokenized into too many subword units, leading to loss of meaning.\n","- **Under-Segmentation**: Frequent words may not be decomposed, missing morphological insights.\n","- **Computational Cost**: Training subword models on large corpora requires significant computational resources.\n","\n","---\n","\n","## 7. Advanced Topics in Subword Models\n","\n","### 7.1 Multilingual Subword Models\n","For multilingual models, subword vocabularies are shared across languages. The challenge is to balance the representation of high-resource and low-resource languages. Techniques like temperature sampling are used to adjust the frequency of tokens during vocabulary construction:\n","$$\n","P(t) \\propto \\text{freq}(t)^{1/\\tau}\n","$$\n","where $ \\tau $ is the temperature parameter.\n","\n","### 7.2 Subword Regularization\n","To improve robustness, subword regularization introduces randomness into the tokenization process during training. For a word $w$, multiple tokenizations $T_1, T_2, \\dots, T_k$ are sampled, and the model is trained on these variations to improve generalization.\n","\n","### 7.3 Subword Models in Speech and Audio Processing\n","In speech processing, subword models are used to tokenize phonetic transcriptions or align audio features with text. For example, in automatic speech recognition (ASR), subword units help handle pronunciation variations and OOV words in spoken language.\n","\n","---\n","\n","## 8. Conclusion\n","\n","Subword models are a cornerstone of modern NLP, enabling efficient, scalable, and generalizable representations of text. By decomposing words into smaller units, these models address challenges like OOV words, morphological richness, and vocabulary size. Algorithms like BPE, WordPiece, and Unigram Language Model provide robust frameworks for subword tokenization, each with unique strengths and trade-offs. Understanding the mathematical foundations and practical applications of these models is essential for researchers and AI scientists working on cutting-edge NLP systems."],"metadata":{"id":"hbBmt3-KN05z"}},{"cell_type":"code","source":["import nltk\n","nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"id":"bGa482NgNQHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Word Structure and Subword Models in NLP\n","\n","This module explains and implements various tokenization strategies\n","with a focus on subword models used in modern NLP systems.\n","\"\"\"\n","\n","import re\n","import collections\n","from typing import Dict, List, Tuple, Set, Optional\n","\n","\n","# ============================================================================ #\n","#                             TOKENIZATION CONCEPTS                            #\n","# ============================================================================ #\n","\"\"\"\n","WORD STRUCTURE AND TOKENIZATION\n","\n","In Natural Language Processing (NLP), tokenization is the process of breaking\n","text into smaller units called tokens. Traditionally, this meant splitting text\n","into words, but modern approaches use more sophisticated methods:\n","\n","1. Word-level tokenization: Splits text into words\n","   - Simple but has issues with out-of-vocabulary (OOV) words\n","   - Example: \"I love programming\" → [\"I\", \"love\", \"programming\"]\n","\n","2. Character-level tokenization: Splits text into individual characters\n","   - No OOV issues but loses word-level semantics\n","   - Example: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n","\n","3. Subword tokenization: A middle ground approach\n","   - Breaks words into meaningful subword units\n","   - Handles rare words and morphologically rich languages better\n","   - Example: \"unhappiness\" → [\"un\", \"happiness\"] or [\"un\", \"happy\", \"ness\"]\n","\n","Subword models are crucial for modern NLP architectures like BERT, GPT, and\n","T5, as they balance vocabulary size and semantic representation.\n","\"\"\"\n","\n","\n","# ============================================================================ #\n","#                               WORD TOKENIZATION                              #\n","# ============================================================================ #\n","\n","def word_tokenizer(text: str) -> List[str]:\n","    \"\"\"\n","    A basic word-level tokenizer that splits text on whitespace and removes punctuation.\n","\n","    Args:\n","        text: Input text string\n","\n","    Returns:\n","        List of word tokens\n","\n","    Examples:\n","        >>> word_tokenizer(\"Hello, world! How are you?\")\n","        ['Hello', 'world', 'How', 'are', 'you']\n","    \"\"\"\n","    # Remove punctuation and split on whitespace\n","    words = re.sub(r'[^\\w\\s]', '', text).split()\n","    return words\n","\n","\n","def word_tokenizer_with_punctuation(text: str) -> List[str]:\n","    \"\"\"\n","    A word tokenizer that preserves punctuation as separate tokens.\n","\n","    Args:\n","        text: Input text string\n","\n","    Returns:\n","        List of word and punctuation tokens\n","\n","    Examples:\n","        >>> word_tokenizer_with_punctuation(\"Hello, world! How are you?\")\n","        ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n","    \"\"\"\n","    # This regex splits on word boundaries, keeping punctuation as tokens\n","    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n","    return tokens\n","\n","\n","# ============================================================================ #\n","#                            CHARACTER TOKENIZATION                            #\n","# ============================================================================ #\n","\n","def char_tokenizer(text: str) -> List[str]:\n","    \"\"\"\n","    A character-level tokenizer that splits text into individual characters.\n","\n","    Args:\n","        text: Input text string\n","\n","    Returns:\n","        List of character tokens\n","\n","    Examples:\n","        >>> char_tokenizer(\"Hello\")\n","        ['H', 'e', 'l', 'l', 'o']\n","    \"\"\"\n","    return list(text)\n","\n","\n","def char_tokenizer_with_whitespace(text: str) -> List[str]:\n","    \"\"\"\n","    A character-level tokenizer that keeps whitespace as separate tokens.\n","\n","    Args:\n","        text: Input text string\n","\n","    Returns:\n","        List of character tokens including whitespace\n","\n","    Examples:\n","        >>> char_tokenizer_with_whitespace(\"Hi there\")\n","        ['H', 'i', ' ', 't', 'h', 'e', 'r', 'e']\n","    \"\"\"\n","    return list(text)\n","\n","\n","# ============================================================================ #\n","#                              SUBWORD TOKENIZATION                            #\n","# ============================================================================ #\n","\n","\"\"\"\n","SUBWORD MODELS\n","\n","Subword models break words into smaller meaningful units, offering a balance\n","between word and character tokenization. The main approaches are:\n","\n","1. Byte Pair Encoding (BPE):\n","   - Iteratively merges the most frequent pair of bytes or characters\n","   - Used in GPT models and RoBERTa\n","\n","2. WordPiece:\n","   - Similar to BPE but uses a different selection criteria based on likelihood\n","   - Used in BERT and DistilBERT\n","\n","3. Unigram Language Model:\n","   - Probabilistic approach that starts with a large vocabulary and prunes it\n","   - Used in XLNet and AlBERT\n","\n","4. SentencePiece:\n","   - Language-agnostic tokenizer that treats the input as a raw stream of Unicode characters\n","   - Used in multilingual models like mBERT and XLM-RoBERTa\n","\n","These methods have revolutionized NLP by reducing vocabulary size while effectively\n","handling unseen words, morphologically rich languages, and multilingual scenarios.\n","\"\"\"\n","\n","\n","# ============================================================================ #\n","#                           BYTE PAIR ENCODING (BPE)                           #\n","# ============================================================================ #\n","\n","class BytePairEncoder:\n","    \"\"\"\n","    Implementation of the Byte Pair Encoding (BPE) algorithm for subword tokenization.\n","\n","    BPE is a data compression algorithm that iteratively replaces the most frequent\n","    pair of consecutive bytes (or characters) with a single, unused byte (or a new symbol).\n","    In NLP, it's used to learn subword units from a corpus.\n","\n","    Attributes:\n","        vocab_size: The target vocabulary size\n","        merges: List of character/subword merges in order of frequency\n","        vocab: Dictionary mapping tokens to IDs\n","    \"\"\"\n","\n","    def __init__(self, vocab_size: int = 10000):\n","        \"\"\"\n","        Initialize the BPE tokenizer.\n","\n","        Args:\n","            vocab_size: Target size of the vocabulary\n","        \"\"\"\n","        self.vocab_size = vocab_size\n","        self.merges = []\n","        self.vocab = {}\n","\n","    def _get_stats(self, vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n","        \"\"\"\n","        Count frequency of adjacent symbol pairs in the vocabulary.\n","\n","        Args:\n","            vocab: Dictionary mapping sequences to their frequency\n","\n","        Returns:\n","            Dictionary of adjacent symbol pairs and their frequencies\n","        \"\"\"\n","        pairs = collections.defaultdict(int)\n","        for word, freq in vocab.items():\n","            symbols = word.split()\n","            for i in range(len(symbols) - 1):\n","                pairs[(symbols[i], symbols[i + 1])] += freq\n","        return pairs\n","\n","    def _merge_vocab(self, vocab: Dict[str, int], pair: Tuple[str, str]) -> Dict[str, int]:\n","        \"\"\"\n","        Merge all occurrences of a symbol pair in the vocabulary.\n","\n","        Args:\n","            vocab: Dictionary mapping sequences to their frequency\n","            pair: The pair of symbols to merge\n","\n","        Returns:\n","            Updated vocabulary with the pair merged\n","        \"\"\"\n","        new_vocab = {}\n","        bigram = ' '.join(pair)\n","        replacement = ''.join(pair)\n","\n","        for word, freq in vocab.items():\n","            # Split the word into symbols (parts)\n","            parts = word.split()\n","\n","            # Keep track of where we are in the word\n","            i = 0\n","            new_parts = []\n","\n","            # Iterate through the parts to find and merge the pair\n","            while i < len(parts):\n","                # If we found the pair and we're not at the end\n","                if i < len(parts) - 1 and parts[i] == pair[0] and parts[i + 1] == pair[1]:\n","                    new_parts.append(replacement)\n","                    i += 2\n","                else:\n","                    new_parts.append(parts[i])\n","                    i += 1\n","\n","            # Create the new word and add it to the vocabulary\n","            new_word = ' '.join(new_parts)\n","            new_vocab[new_word] = freq\n","\n","        return new_vocab\n","\n","    def fit(self, texts: List[str], num_merges: Optional[int] = None) -> None:\n","        \"\"\"\n","        Learn BPE merges from a list of texts.\n","\n","        Args:\n","            texts: List of text samples to learn from\n","            num_merges: Number of merge operations (if None, uses vocab_size)\n","        \"\"\"\n","        # Initialize vocabulary with character-split words\n","        vocab = {}\n","        for text in texts:\n","            # Simple word tokenization\n","            words = re.findall(r'\\b\\w+\\b', text.lower())\n","            for word in words:\n","                # Split each word into characters with spaces between\n","                char_word = ' '.join(list(word))\n","                if char_word in vocab:\n","                    vocab[char_word] += 1\n","                else:\n","                    vocab[char_word] = 1\n","\n","        # Determine number of merges\n","        if num_merges is None:\n","            # Calculate initial vocab size (unique characters)\n","            unique_chars = set()\n","            for word in vocab:\n","                unique_chars.update(word.split())\n","\n","            # Number of merges needed to reach target vocab size\n","            num_merges = min(self.vocab_size - len(unique_chars), 10000)\n","\n","        # Perform merge operations\n","        for i in range(num_merges):\n","            pairs = self._get_stats(vocab)\n","            if not pairs:\n","                break\n","\n","            # Find the most frequent pair\n","            best_pair = max(pairs, key=pairs.get)\n","            self.merges.append(best_pair)\n","\n","            # Merge the pair in the vocabulary\n","            vocab = self._merge_vocab(vocab, best_pair)\n","\n","            if i % 100 == 0:\n","                print(f\"Merge {i}: {best_pair} -> {''.join(best_pair)}\")\n","\n","        # Build the final vocabulary\n","        self._build_vocab(vocab)\n","\n","    def _build_vocab(self, vocab: Dict[str, int]) -> None:\n","        \"\"\"\n","        Build the vocabulary from learned merges.\n","\n","        Args:\n","            vocab: The current vocabulary after merges\n","        \"\"\"\n","        # Get all unique tokens\n","        tokens = set()\n","        for word in vocab:\n","            tokens.update(word.split())\n","\n","        # Assign IDs to tokens\n","        for i, token in enumerate(sorted(tokens)):\n","            self.vocab[token] = i\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        \"\"\"\n","        Tokenize text using learned BPE merges.\n","\n","        Args:\n","            text: Text to tokenize\n","\n","        Returns:\n","            List of subword tokens\n","        \"\"\"\n","        # Simple word tokenization\n","        words = re.findall(r'\\b\\w+\\b', text.lower())\n","        result = []\n","\n","        for word in words:\n","            # Start with characters\n","            chars = ' '.join(list(word))\n","\n","            # Apply merges in the learned order\n","            for pair in self.merges:\n","                chars = chars.replace(' '.join(pair), ''.join(pair))\n","\n","            # Add resulting tokens to output\n","            result.extend(chars.split())\n","\n","        return result\n","\n","    def encode(self, text: str) -> List[int]:\n","        \"\"\"\n","        Encode text into token IDs.\n","\n","        Args:\n","            text: Text to encode\n","\n","        Returns:\n","            List of token IDs\n","        \"\"\"\n","        tokens = self.tokenize(text)\n","        return [self.vocab.get(token, self.vocab.get('<unk>', len(self.vocab))) for token in tokens]\n","\n","    def decode(self, ids: List[int]) -> str:\n","        \"\"\"\n","        Decode token IDs back to text.\n","\n","        Args:\n","            ids: List of token IDs\n","\n","        Returns:\n","            Decoded text\n","        \"\"\"\n","        # Create a reverse mapping from IDs to tokens\n","        id_to_token = {v: k for k, v in self.vocab.items()}\n","\n","        # Convert IDs to tokens\n","        tokens = [id_to_token.get(id, '<unk>') for id in ids]\n","\n","        # Join tokens (this simplistic join doesn't handle spacing properly)\n","        return ''.join(tokens)\n","\n","\n","# ============================================================================ #\n","#                           WORDPIECE IMPLEMENTATION                           #\n","# ============================================================================ #\n","\n","class WordPiece:\n","    \"\"\"\n","    Implementation of WordPiece tokenization algorithm.\n","\n","    WordPiece is similar to BPE but uses a different selection criterion.\n","    It chooses the pair that maximizes the likelihood of the training data\n","    after the merge.\n","\n","    The main difference from BPE is that WordPiece marks subword units\n","    differently. Most subwords are prefixed with \"##\" to indicate they are\n","    part of a word.\n","\n","    Attributes:\n","        vocab_size: The target vocabulary size\n","        vocab: Dictionary mapping tokens to IDs\n","    \"\"\"\n","\n","    def __init__(self, vocab_size: int = 10000):\n","        \"\"\"\n","        Initialize the WordPiece tokenizer.\n","\n","        Args:\n","            vocab_size: Target size of the vocabulary\n","        \"\"\"\n","        self.vocab_size = vocab_size\n","        self.vocab = {}\n","        # Special tokens\n","        self.special_tokens = {\n","            '<unk>': 0,  # Unknown token\n","            '<s>': 1,    # Start of sequence\n","            '</s>': 2,   # End of sequence\n","            '<pad>': 3,  # Padding token\n","        }\n","\n","    def _get_word_counts(self, texts: List[str]) -> Dict[str, int]:\n","        \"\"\"\n","        Count word frequencies in the corpus.\n","\n","        Args:\n","            texts: List of text samples\n","\n","        Returns:\n","            Dictionary of words and their frequencies\n","        \"\"\"\n","        word_counts = collections.Counter()\n","        for text in texts:\n","            # Simple word tokenization\n","            words = re.findall(r'\\b\\w+\\b', text.lower())\n","            word_counts.update(words)\n","        return word_counts\n","\n","    def _split_word_to_chars(self, word: str) -> str:\n","        \"\"\"\n","        Split a word into character level representation for WordPiece.\n","\n","        Args:\n","            word: Input word\n","\n","        Returns:\n","            WordPiece character representation with ## prefix\n","        \"\"\"\n","        chars = list(word)\n","        # First character doesn't get ## prefix\n","        result = [chars[0]]\n","        # Rest get ## prefix\n","        result.extend([f\"##{c}\" for c in chars[1:]])\n","        return ' '.join(result)\n","\n","    def _compute_pair_scores(self, vocab: Dict[str, int]) -> Dict[Tuple[str, str], float]:\n","        \"\"\"\n","        Compute scores for each pair based on the WordPiece scoring function.\n","\n","        Args:\n","            vocab: Dictionary mapping sequences to their frequency\n","\n","        Returns:\n","            Dictionary of adjacent symbol pairs and their scores\n","        \"\"\"\n","        # Count frequencies of adjacent pairs\n","        pair_counts = collections.defaultdict(int)\n","        total_pairs = 0\n","\n","        for word, freq in vocab.items():\n","            symbols = word.split()\n","            for i in range(len(symbols) - 1):\n","                pair = (symbols[i], symbols[i + 1])\n","                pair_counts[pair] += freq\n","                total_pairs += freq\n","\n","        # Calculate scores (simplified version of WordPiece scoring)\n","        scores = {}\n","        for pair, count in pair_counts.items():\n","            # WordPiece uses a more complex likelihood-based score\n","            # This is a simplified version that approximates the original algorithm\n","            a, b = pair\n","            if a.startswith('##') and b.startswith('##'):\n","                # Encourage merging subword units\n","                scores[pair] = count * 1.1 / total_pairs\n","            else:\n","                scores[pair] = count / total_pairs\n","\n","        return scores\n","\n","    def fit(self, texts: List[str], num_merges: Optional[int] = None) -> None:\n","        \"\"\"\n","        Learn WordPiece vocabulary from a list of texts.\n","\n","        Args:\n","            texts: List of text samples to learn from\n","            num_merges: Number of merge operations (if None, uses vocab_size)\n","        \"\"\"\n","        # Get word counts\n","        word_counts = self._get_word_counts(texts)\n","\n","        # Initialize vocabulary with character-split words\n","        vocab = {}\n","        for word, count in word_counts.items():\n","            char_word = self._split_word_to_chars(word)\n","            vocab[char_word] = count\n","\n","        # Determine number of merges\n","        if num_merges is None:\n","            # Calculate initial vocab size (unique characters and special tokens)\n","            unique_tokens = set()\n","            for word in vocab:\n","                unique_tokens.update(word.split())\n","\n","            # Number of merges needed to reach target vocab size\n","            num_merges = min(self.vocab_size - len(unique_tokens) - len(self.special_tokens), 10000)\n","\n","        # Perform merge operations\n","        merges = []\n","        for i in range(num_merges):\n","            # Calculate scores for all pairs\n","            scores = self._compute_pair_scores(vocab)\n","\n","            if not scores:\n","                break\n","\n","            # Find the best pair to merge\n","            best_pair = max(scores, key=scores.get)\n","            merges.append(best_pair)\n","\n","            # Merge the pair in all words\n","            vocab = self._merge_vocab(vocab, best_pair)\n","\n","            if i % 100 == 0:\n","                print(f\"Merge {i}: {best_pair} -> {''.join(best_pair)}\")\n","\n","        # Build final vocabulary\n","        self._build_vocab(vocab, merges)\n","\n","    def _merge_vocab(self, vocab: Dict[str, int], pair: Tuple[str, str]) -> Dict[str, int]:\n","        \"\"\"\n","        Merge all occurrences of a symbol pair in the vocabulary.\n","\n","        Args:\n","            vocab: Dictionary mapping sequences to their frequency\n","            pair: The pair of symbols to merge\n","\n","        Returns:\n","            Updated vocabulary with the pair merged\n","        \"\"\"\n","        new_vocab = {}\n","        bigram = ' '.join(pair)\n","\n","        # Handle special case for ## prefix\n","        if pair[1].startswith('##'):\n","            # When merging with a ## token, remove the ## from the merged result\n","            replacement = pair[0] + pair[1][2:]\n","        else:\n","            replacement = ''.join(pair)\n","\n","        for word, freq in vocab.items():\n","            # Replace the pair throughout the word\n","            new_word = word.replace(bigram, replacement)\n","            new_vocab[new_word] = freq\n","\n","        return new_vocab\n","\n","    def _build_vocab(self, vocab: Dict[str, int], merges: List[Tuple[str, str]]) -> None:\n","        \"\"\"\n","        Build the vocabulary from learned merges.\n","\n","        Args:\n","            vocab: The current vocabulary after merges\n","            merges: List of merge operations\n","        \"\"\"\n","        # Start with special tokens\n","        final_vocab = self.special_tokens.copy()\n","        next_id = len(final_vocab)\n","\n","        # Get all unique tokens\n","        tokens = set()\n","        for word in vocab:\n","            tokens.update(word.split())\n","\n","        # Add tokens to vocabulary\n","        for token in sorted(tokens):\n","            final_vocab[token] = next_id\n","            next_id += 1\n","\n","        self.vocab = final_vocab\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        \"\"\"\n","        Tokenize text using learned WordPiece vocabulary.\n","\n","        Args:\n","            text: Text to tokenize\n","\n","        Returns:\n","            List of subword tokens\n","        \"\"\"\n","        # Simple word tokenization\n","        words = re.findall(r'\\b\\w+\\b', text.lower())\n","        result = []\n","\n","        for word in words:\n","            # Start with character-level representation\n","            current = self._split_word_to_chars(word)\n","            current_tokens = current.split()\n","\n","            # Try to merge tokens greedily\n","            changed = True\n","            while changed:\n","                changed = False\n","                for i in range(len(current_tokens) - 1):\n","                    # Check if this pair can be merged\n","                    pair = (current_tokens[i], current_tokens[i + 1])\n","                    merged = current_tokens[i]\n","\n","                    # Handle ## prefix when merging\n","                    if current_tokens[i + 1].startswith('##'):\n","                        merged += current_tokens[i + 1][2:]\n","                    else:\n","                        merged += current_tokens[i + 1]\n","\n","                    # If the merged token is in our vocabulary, merge it\n","                    if merged in self.vocab:\n","                        current_tokens[i] = merged\n","                        current_tokens.pop(i + 1)\n","                        changed = True\n","                        break\n","\n","            # Add resulting tokens to output\n","            result.extend(current_tokens)\n","\n","        return result\n","\n","    def encode(self, text: str) -> List[int]:\n","        \"\"\"\n","        Encode text into token IDs.\n","\n","        Args:\n","            text: Text to encode\n","\n","        Returns:\n","            List of token IDs\n","        \"\"\"\n","        tokens = self.tokenize(text)\n","        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n","\n","    def decode(self, ids: List[int]) -> str:\n","        \"\"\"\n","        Decode token IDs back to text.\n","\n","        Args:\n","            ids: List of token IDs\n","\n","        Returns:\n","            Decoded text\n","        \"\"\"\n","        # Create a reverse mapping from IDs to tokens\n","        id_to_token = {v: k for k, v in self.vocab.items()}\n","\n","        # Convert IDs to tokens\n","        tokens = [id_to_token.get(id, '<unk>') for id in ids]\n","\n","        # Join tokens, handling ## prefix\n","        text = ''\n","        for token in tokens:\n","            if token.startswith('##'):\n","                # Remove ## and append without space\n","                text += token[2:]\n","            elif token in self.special_tokens.keys():\n","                # Skip special tokens\n","                continue\n","            else:\n","                # Add space before regular tokens (except at the beginning)\n","                if text:\n","                    text += ' '\n","                text += token\n","\n","        return text\n","\n","\n","# ============================================================================ #\n","#                          UNIGRAM LANGUAGE MODEL                              #\n","# ============================================================================ #\n","\n","class UnigramLM:\n","    \"\"\"\n","    Implementation of the Unigram Language Model for subword tokenization.\n","\n","    The Unigram model is a probabilistic approach that starts with a large\n","    vocabulary and iteratively removes tokens to maximize the likelihood\n","    of the training data.\n","\n","    Attributes:\n","        vocab_size: The target vocabulary size\n","        vocab: Dictionary mapping tokens to IDs\n","        token_probs: Probability for each token\n","    \"\"\"\n","\n","    def __init__(self, vocab_size: int = 10000):\n","        \"\"\"\n","        Initialize the Unigram Language Model tokenizer.\n","\n","        Args:\n","            vocab_size: Target size of the vocabulary\n","        \"\"\"\n","        self.vocab_size = vocab_size\n","        self.vocab = {}\n","        self.token_probs = {}\n","        # Special tokens\n","        self.special_tokens = {\n","            '<unk>': 0,\n","            '<s>': 1,\n","            '</s>': 2,\n","            '<pad>': 3,\n","        }\n","\n","    def _initialize_vocab(self, texts: List[str], initial_vocab_size: int = 50000) -> Dict[str, float]:\n","        \"\"\"\n","        Initialize a large vocabulary for pruning.\n","\n","        Args:\n","            texts: List of text samples\n","            initial_vocab_size: Initial size of the vocabulary before pruning\n","\n","        Returns:\n","            Dictionary mapping tokens to their probabilities\n","        \"\"\"\n","        # Start with character-level tokenization\n","        char_vocab = collections.Counter()\n","        for text in texts:\n","            char_vocab.update(list(text.lower()))\n","\n","        # Add all characters to the vocabulary with their frequencies\n","        vocab = {c: count for c, count in char_vocab.items()}\n","\n","        # Generate all substrings of length 2-8 from the corpus\n","        substrings = collections.Counter()\n","        for text in texts:\n","            text = text.lower()\n","            for length in range(2, 9):\n","                for i in range(len(text) - length + 1):\n","                    substrings[text[i:i+length]] += 1\n","\n","        # Add most common substrings to initial vocabulary\n","        max_substrs = initial_vocab_size - len(vocab) - len(self.special_tokens)\n","        for substr, count in substrings.most_common(max_substrs):\n","            vocab[substr] = count\n","\n","        # Convert frequencies to probabilities\n","        total = sum(vocab.values())\n","        vocab_probs = {token: count / total for token, count in vocab.items()}\n","\n","        return vocab_probs\n","\n","    def _viterbi_segment(self, word: str, token_probs: Dict[str, float]) -> List[str]:\n","        \"\"\"\n","        Segment a word into the most likely sequence of subwords using Viterbi algorithm.\n","\n","        Args:\n","            word: Word to segment\n","            token_probs: Dictionary of token probabilities\n","\n","        Returns:\n","            List of subword tokens\n","        \"\"\"\n","        # Dynamic programming approach\n","        # best_score[i] = best score for segmenting word[:i]\n","        best_score = [0] * (len(word) + 1)\n","        best_score[0] = 1.0\n","\n","        # best_edge[i] = best previous position for segmenting word[:i]\n","        best_edge = [0] * (len(word) + 1)\n","\n","        for i in range(1, len(word) + 1):\n","            best_score[i] = 0\n","            for j in range(i):\n","                # Try segmenting at each possible position\n","                substr = word[j:i]\n","                if substr in token_probs:\n","                    # Score is the product of probabilities (log domain would be better)\n","                    score = best_score[j] * token_probs[substr]\n","                    if score > best_score[i]:\n","                        best_score[i] = score\n","                        best_edge[i] = j\n","\n","        # Backtrack to get the segmentation\n","        tokens = []\n","        current = len(word)\n","        while current > 0:\n","            prev = best_edge[current]\n","            tokens.append(word[prev:current])\n","            current = prev\n","\n","        # Reverse to get tokens in the correct order\n","        return tokens[::-1]\n","\n","    def fit(self, texts: List[str]) -> None:\n","        \"\"\"\n","        Learn the Unigram vocabulary from a list of texts.\n","\n","        Args:\n","            texts: List of text samples to learn from\n","        \"\"\"\n","        # Initialize with a large vocabulary\n","        token_probs = self._initialize_vocab(texts)\n","\n","        # Iteratively prune the vocabulary\n","        while len(token_probs) + len(self.special_tokens) > self.vocab_size:\n","            # Calculate loss contribution of each token\n","            token_scores = {}\n","            for token, prob in token_probs.items():\n","                # Skip single character tokens (to avoid degenerate solutions)\n","                if len(token) == 1:\n","                    continue\n","\n","                # Calculate approximate loss change if we remove this token\n","                # This is a simplified version of the actual algorithm\n","                token_scores[token] = prob * len(token)\n","\n","            # Remove the token that contributes least to the model\n","            if token_scores:\n","                worst_token = min(token_scores, key=token_scores.get)\n","                del token_probs[worst_token]\n","            else:\n","                # If there are no more tokens to remove, break\n","                break\n","\n","            # Adjust probabilities to sum to 1\n","            total = sum(token_probs.values())\n","            token_probs = {token: prob / total for token, prob in token_probs.items()}\n","\n","        self.token_probs = token_probs\n","\n","        # Build final vocabulary\n","        self._build_vocab()\n","\n","    def _build_vocab(self) -> None:\n","        \"\"\"Build the vocabulary from token probabilities.\"\"\"\n","        # Start with special tokens\n","        vocab = self.special_tokens.copy()\n","        next_id = len(vocab)\n","\n","        # Add tokens sorted by probability\n","        for token, _ in sorted(self.token_probs.items(), key=lambda x: x[1], reverse=True):\n","            vocab[token] = next_id\n","            next_id += 1\n","\n","        self.vocab = vocab\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        \"\"\"\n","        Tokenize text using the Unigram model.\n","\n","        Args:\n","            text: Text to tokenize\n","\n","        Returns:\n","            List of subword tokens\n","        \"\"\"\n","        result = []\n","        # Simple word tokenization first\n","        words = re.findall(r'\\b\\w+\\b', text.lower())\n","\n","        for word in words:\n","            # Use Viterbi algorithm to segment each word\n","            tokens = self._viterbi_segment(word, self.token_probs)\n","            result.extend(tokens)\n","\n","        return result\n","\n","    def encode(self, text: str) -> List[int]:\n","        \"\"\"\n","        Encode text into token IDs.\n","\n","        Args:\n","            text: Text to encode\n","\n","        Returns:\n","            List of token IDs\n","        \"\"\"\n","        tokens = self.tokenize(text)\n","        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n","\n","    def decode(self, ids: List[int]) -> str:\n","        \"\"\"\n","        Decode token IDs back to text.\n","\n","        Args:\n","            ids: List of token IDs\n","\n","        Returns:\n","            Decoded text\n","        \"\"\"\n","        # Create a reverse mapping from IDs to tokens\n","        id_to_token = {v: k for k, v in self.vocab.items()}\n","\n","        # Convert IDs to tokens\n","        tokens = [id_to_token.get(id, '<unk>') for id in ids]\n","\n","        # Simply join tokens (this is a simplification)\n","        return ''.join(tokens)\n","\n","\n","# ============================================================================ #\n","#                        EXAMPLES AND DEMONSTRATIONS                           #\n","# ============================================================================ #\n","\n","def demonstrate_word_tokenization() -> None:\n","    \"\"\"Demonstrate word-level tokenization.\"\"\"\n","    text = \"Hello world! This is an example of word-level tokenization. It doesn't handle contractions like don't very well.\"\n","\n","    print(\"\\nWORD TOKENIZATION EXAMPLE:\")\n","    print(\"Original text:\", text)\n","    print(\"Simple word tokenization:\", word_tokenizer(text))\n","    print(\"Word tokenization with punctuation:\", word_tokenizer_with_punctuation(text))\n","\n","\n","def demonstrate_char_tokenization() -> None:\n","    \"\"\"Demonstrate character-level tokenization.\"\"\"\n","    text = \"Hello world!\"\n","\n","    print(\"\\nCHARACTER TOKENIZATION EXAMPLE:\")\n","    print(\"Original text:\", text)\n","    print(\"Character tokens:\", char_tokenizer(text))\n","    print(\"Character tokens with whitespace:\", char_tokenizer_with_whitespace(text))\n","\n","\n","def demonstrate_bpe() -> None:\n","    \"\"\"Demonstrate Byte Pair Encoding tokenization.\"\"\"\n","    # Training corpus\n","    corpus = [\n","        \"I love to program in Python\",\n","        \"Python programming is fun\",\n","        \"The Python programming language is versatile\",\n","        \"I enjoy learning new programming languages\",\n","        \"Programming helps solve complex problems\",\n","        \"Python has many programming libraries\",\n","        \"Learning to program is a valuable skill\"\n","    ]\n","\n","    # Test text\n","    test_text = \"I love programming in Python language\"\n","\n","    print(\"\\nBYTE PAIR ENCODING EXAMPLE:\")\n","    print(\"Training corpus size:\", len(corpus))\n","\n","    # Train BPE\n","    bpe = BytePairEncoder(vocab_size=100)\n","    bpe.fit(corpus, num_merges=50)\n","\n","    # Tokenize and encode test text\n","    tokens = bpe.tokenize(test_text)\n","    ids = bpe.encode(test_text)\n","\n","    print(\"BPE tokens for '\", test_text, \"':\", tokens)\n","    print(\"BPE token IDs:\", ids)\n","\n","    # Show how BPE handles unseen words\n","    unseen_text = \"programmer programs programmable\"\n","    unseen_tokens = bpe.tokenize(unseen_text)\n","    print(\"BPE tokens for unseen text '\", unseen_text, \"':\", unseen_tokens)\n","\n","\n","def demonstrate_wordpiece() -> None:\n","    \"\"\"Demonstrate WordPiece tokenization.\"\"\"\n","    # Training corpus\n","    corpus = [\n","        \"I love to program in Python\",\n","        \"Python programming is fun\",\n","        \"The Python programming language is versatile\",\n","        \"I enjoy learning new programming languages\",\n","        \"Programming helps solve complex problems\",\n","        \"Python has many programming libraries\",\n","        \"Learning to program is a valuable skill\"\n","    ]\n","\n","    # Test text\n","    test_text = \"I love programming in Python language\"\n","\n","    print(\"\\nWORDPIECE EXAMPLE:\")\n","\n","    # Train WordPiece\n","    wp = WordPiece(vocab_size=100)\n","    wp.fit(corpus, num_merges=50)\n","\n","    # Tokenize and encode test text\n","    tokens = wp.tokenize(test_text)\n","    ids = wp.encode(test_text)\n","\n","    print(\"WordPiece tokens for '\", test_text, \"':\", tokens)\n","    print(\"WordPiece token IDs:\", ids)\n","\n","    # Show how WordPiece handles unseen words\n","    unseen_text = \"programmer programs programmable\"\n","    unseen_tokens = wp.tokenize(unseen_text)\n","    print(\"WordPiece tokens for unseen text '\", unseen_text, \"':\", unseen_tokens)\n","\n","\n","def demonstrate_unigram() -> None:\n","    \"\"\"Demonstrate Unigram Language Model tokenization.\"\"\"\n","    # Training corpus\n","    corpus = [\n","        \"I love to program in Python\",\n","        \"Python programming is fun\",\n","        \"The Python programming language is versatile\",\n","        \"I enjoy learning new programming languages\",\n","        \"Programming helps solve complex problems\",\n","        \"Python has many programming libraries\",\n","        \"Learning to program is a valuable skill\"\n","    ]\n","\n","    # Test text\n","    test_text = \"I love programming in Python language\"\n","\n","    print(\"\\nUNIGRAM LANGUAGE MODEL EXAMPLE:\")\n","\n","    # Train Unigram model\n","    unigram = UnigramLM(vocab_size=100)\n","    unigram.fit(corpus)\n","\n","    # Tokenize and encode test text\n","    tokens = unigram.tokenize(test_text)\n","    ids = unigram.encode(test_text)\n","\n","    print(\"Unigram tokens for '\", test_text, \"':\", tokens)\n","    print(\"Unigram token IDs:\", ids)\n","\n","    # Show how Unigram handles unseen words\n","    unseen_text = \"programmer programs programmable\"\n","    unseen_tokens = unigram.tokenize(unseen_text)\n","    print(\"Unigram tokens for unseen text '\", unseen_text, \"':\", unseen_tokens)\n","\n","\n","def compare_models() -> None:\n","    \"\"\"Compare different tokenization approaches on the same text.\"\"\"\n","    text = \"The transformer architecture revolutionized natural language processing.\"\n","\n","    print(\"\\nCOMPARISON OF TOKENIZATION APPROACHES:\")\n","    print(\"Original text:\", text)\n","\n","    # Word tokenization\n","    print(\"Word tokens:\", word_tokenizer(text))\n","\n","    # Character tokenization\n","    print(\"Character tokens:\", char_tokenizer(text))\n","\n","    # Train simple models on a small corpus\n","    corpus = [\n","        \"The transformer architecture revolutionized natural language processing.\",\n","        \"Natural language processing models use transformers.\",\n","        \"Transformers process text effectively.\",\n","        \"Language models help with text generation.\",\n","        \"Processing natural language is complex.\"\n","    ]\n","\n","    # BPE\n","    bpe = BytePairEncoder(vocab_size=50)\n","    bpe.fit(corpus, num_merges=20)\n","    print(\"BPE tokens:\", bpe.tokenize(text))\n","\n","    # WordPiece\n","    wp = WordPiece(vocab_size=50)\n","    wp.fit(corpus, num_merges=20)\n","    print(\"WordPiece tokens:\", wp.tokenize(text))\n","\n","    # Unigram\n","    unigram = UnigramLM(vocab_size=50)\n","    unigram.fit(corpus)\n","    print(\"Unigram tokens:\", unigram.tokenize(text))\n","\n","\n","def real_world_examples() -> None:\n","    \"\"\"Show how subword models are used in real-world scenarios.\"\"\"\n","    print(\"\\nREAL-WORLD APPLICATIONS OF SUBWORD MODELS:\")\n","\n","    print(\"\\n1. Handling Out-of-Vocabulary Words:\")\n","    print(\"   - Traditional word-level tokenization: 'unhappiness' -> ['<unk>'] (if not in vocab)\")\n","    print(\"   - BPE tokenization: 'unhappiness' -> ['un', 'happiness'] (more meaningful)\")\n","\n","    print(\"\\n2. Morphologically Rich Languages:\")\n","    print(\"   - Finnish word 'epäjärjestelmällistyttämättömyydelläänsäkäänköhän'\")\n","    print(\"   - Would be broken into meaningful subwords rather than a single OOV token\")\n","\n","    print(\"\\n3. Code-Switching and Multilingual Text:\")\n","    print(\"   - Text: 'I love machine learning. C'est très intéressant.'\")\n","    print(\"   - Handles multiple languages with the same vocabulary\")\n","\n","    print(\"\\n4. Emojis and Special Characters:\")\n","    print(\"   - Text with emojis: 'I love 🐱 and 🐶'\")\n","    print(\"   - Tokenized properly as individual units\")\n","\n","    print(\"\\n5. Technical Terms and Domain-Specific Vocabulary:\")\n","    print(\"   - Medical terms like 'electroencephalography'\")\n","    print(\"   - Broken into meaningful subword units: 'electro', 'encephalo', 'graphy'\")\n","\n","\n","def advanced_use_cases() -> None:\n","    \"\"\"Demonstrate some advanced use cases for subword tokenization.\"\"\"\n","    print(\"\\nADVANCED USE CASES:\")\n","\n","    print(\"\\n1. Cross-Lingual Transfer Learning:\")\n","    print(\"   - Models trained on one language can be applied to similar languages\")\n","    print(\"   - Example: English → German, Spanish → Portuguese\")\n","\n","    print(\"\\n2. Handling Technical Vocabulary:\")\n","    text = \"The hyperparameter optimization of the convolutional neural network improved F1-score.\"\n","    print(\"   Original:\", text)\n","\n","    # Create small corpus with technical terms\n","    corpus = [\n","        \"The hyperparameter optimization improved the model.\",\n","        \"Convolutional neural networks process images.\",\n","        \"F1-score is a metric for classification.\",\n","        \"Neural networks have hyperparameters.\",\n","        \"Optimization of models requires careful tuning.\"\n","    ]\n","\n","    # Train BPE\n","    bpe = BytePairEncoder(vocab_size=50)\n","    bpe.fit(corpus, num_merges=20)\n","    print(\"   BPE tokens:\", bpe.tokenize(text))\n","\n","    print(\"\\n3. Spelling Errors and Typos:\")\n","    text_with_typo = \"The transfomer architecture is powerful.\"  # Misspelled \"transformer\"\n","    print(\"   Original with typo:\", text_with_typo)\n","    print(\"   BPE tokens:\", bpe.tokenize(text_with_typo))\n","\n","    print(\"\\n4. Computational Efficiency:\")\n","    print(\"   - Smaller vocabulary size (compared to word-level) leads to:\")\n","    print(\"     * Reduced memory footprint\")\n","    print(\"     * Faster training\")\n","    print(\"     * More efficient matrix operations\")\n","\n","\n","# ============================================================================ #\n","#                               MAIN FUNCTION                                  #\n","# ============================================================================ #\n","\n","def main() -> None:\n","    \"\"\"\n","    Main function demonstrating word structure and subword models in NLP.\n","    \"\"\"\n","    print(\"WORD STRUCTURE AND SUBWORD MODELS IN NLP\\n\")\n","\n","    # Demonstrate different tokenization methods\n","    demonstrate_word_tokenization()\n","    demonstrate_char_tokenization()\n","    demonstrate_bpe()\n","    demonstrate_wordpiece()\n","    demonstrate_unigram()\n","\n","    # Compare models on the same text\n","    compare_models()\n","\n","    # Show real-world examples\n","    real_world_examples()\n","\n","    # Advanced use cases\n","    advanced_use_cases()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"hmPKrLRHNtRY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"7srdpRO49S8v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibskgZ1eMCM8"},"outputs":[],"source":["\"\"\"\n","Word Structure and Subword Models: A Comprehensive Guide\n","\n","This file provides an in-depth exploration of word structure analysis and subword modeling techniques,\n","which are fundamental in Natural Language Processing (NLP), text processing, and machine learning.\n","We will cover word tokenization, stemming, lemmatization, and advanced subword modeling techniques\n","like Byte-Pair Encoding (BPE) and WordPiece. The code is written with PEP-8 standards, is highly\n","modular, and includes detailed explanations and examples.\n","\n","Topics Covered:\n","1. Word Structure Basics\n","   - Tokenization\n","   - Stemming\n","   - Lemmatization\n","2. Subword Modeling\n","   - Byte-Pair Encoding (BPE)\n","   - WordPiece\n","3. Exception Handling and Edge Cases\n","4. Practical Examples and Use Cases\n","\n","Each section includes detailed explanations, code implementations, and examples to ensure clarity.\n","\"\"\"\n","\n","# Import necessary libraries\n","import re\n","from collections import Counter, defaultdict\n","from typing import List, Dict, Set, Tuple\n","\n","# For lemmatization, we use NLTK (ensure to install: pip install nltk)\n","import nltk\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","# Download required NLTK data (run once)\n","try:\n","    nltk.download('wordnet')\n","    nltk.download('averaged_perceptron_tagger')\n","except Exception as e:\n","    print(f\"Error downloading NLTK data: {e}\")\n","\n","\n","# ============================================================================\n","# SECTION 1: WORD STRUCTURE BASICS\n","# ============================================================================\n","\n","\"\"\"\n","Word Structure Basics:\n","Words are the fundamental units of language. Understanding their structure involves breaking them\n","into meaningful components (tokens, stems, lemmas, etc.). This section covers:\n","1. Tokenization: Splitting text into words or tokens.\n","2. Stemming: Reducing words to their root form (e.g., 'running' -> 'run').\n","3. Lemmatization: Reducing words to their dictionary form (e.g., 'better' -> 'good').\n","\"\"\"\n","\n","class WordStructureAnalyzer:\n","    \"\"\"\n","    A class to handle basic word structure analysis including tokenization,\n","    stemming, and lemmatization.\n","    \"\"\"\n","\n","    def __init__(self) -> None:\n","        \"\"\"Initialize stemmer and lemmatizer.\"\"\"\n","        self.stemmer = PorterStemmer()\n","        self.lemmatizer = WordNetLemmatizer()\n","\n","    def tokenize(self, text: str) -> List[str]:\n","        \"\"\"\n","        Tokenize text into words using regex.\n","\n","        Args:\n","            text (str): Input text to tokenize.\n","\n","        Returns:\n","            List[str]: List of tokens.\n","\n","        Example:\n","            >>> analyzer = WordStructureAnalyzer()\n","            >>> analyzer.tokenize(\"Hello, world! How are you?\")\n","            ['Hello', 'world', 'How', 'are', 'you']\n","        \"\"\"\n","        # Use regex to split on non-word characters, preserving apostrophes\n","        tokens = re.findall(r'\\w+\\'?\\w*|\\w+', text)\n","        return tokens\n","\n","    def stem(self, word: str) -> str:\n","        \"\"\"\n","        Apply stemming to a word.\n","\n","        Args:\n","            word (str): Input word to stem.\n","\n","        Returns:\n","            str: Stemmed word.\n","\n","        Example:\n","            >>> analyzer = WordStructureAnalyzer()\n","            >>> analyzer.stem(\"running\")\n","            'run'\n","        \"\"\"\n","        return self.stemmer.stem(word)\n","\n","    def lemmatize(self, word: str, pos: str = 'n') -> str:\n","        \"\"\"\n","        Apply lemmatization to a word.\n","\n","        Args:\n","            word (str): Input word to lemmatize.\n","            pos (str): Part of speech ('n' for noun, 'v' for verb, etc.).\n","\n","        Returns:\n","            str: Lemmatized word.\n","\n","        Example:\n","            >>> analyzer = WordStructureAnalyzer()\n","            >>> analyzer.lemmatize(\"better\", pos='a')\n","            'good'\n","        \"\"\"\n","        return self.lemmatizer.lemmatize(word, pos=pos)\n","\n","    def get_wordnet_pos(self, word: str) -> str:\n","        \"\"\"\n","        Helper function to map POS tag to WordNet POS tag for lemmatization.\n","\n","        Args:\n","            word (str): Input word to tag.\n","\n","        Returns:\n","            str: WordNet POS tag.\n","        \"\"\"\n","        tag = nltk.pos_tag([word])[0][1][0].upper()\n","        tag_dict = {\n","            'J': wordnet.ADJ,\n","            'N': wordnet.NOUN,\n","            'V': wordnet.VERB,\n","            'R': wordnet.ADV\n","        }\n","        return tag_dict.get(tag, wordnet.NOUN)\n","\n","\n","# Example usage of WordStructureAnalyzer\n","def demonstrate_word_structure() -> None:\n","    \"\"\"Demonstrate word structure analysis with examples.\"\"\"\n","    analyzer = WordStructureAnalyzer()\n","\n","    # Example text\n","    text = \"Running faster is better than walking slowly!\"\n","\n","    # Tokenization\n","    print(\"\\n=== Tokenization ===\")\n","    tokens = analyzer.tokenize(text)\n","    print(f\"Original text: {text}\")\n","    print(f\"Tokens: {tokens}\")\n","\n","    # Stemming\n","    print(\"\\n=== Stemming ===\")\n","    stemmed_words = [analyzer.stem(token) for token in tokens]\n","    print(f\"Original tokens: {tokens}\")\n","    print(f\"Stemmed tokens: {stemmed_words}\")\n","\n","    # Lemmatization\n","    print(\"\\n=== Lemmatization ===\")\n","    lemmatized_words = [\n","        analyzer.lemmatize(token, analyzer.get_wordnet_pos(token))\n","        for token in tokens\n","    ]\n","    print(f\"Original tokens: {tokens}\")\n","    print(f\"Lemmatized tokens: {lemmatized_words}\")\n","\n","\n","# ============================================================================\n","# SECTION 2: SUBWORD MODELING\n","# ============================================================================\n","\n","\"\"\"\n","Subword Modeling:\n","Subword modeling is a technique used in modern NLP models (e.g., BERT, GPT) to handle large\n","vocabularies and out-of-vocabulary (OOV) words. Instead of treating words as atomic units,\n","subword models break words into smaller, meaningful units (subwords). This section covers:\n","1. Byte-Pair Encoding (BPE): A data compression technique adapted for NLP.\n","2. WordPiece: A subword tokenization algorithm used in models like BERT.\n","\"\"\"\n","\n","class SubwordModel:\n","    \"\"\"\n","    A class to implement subword modeling techniques like BPE and WordPiece.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size: int = 1000) -> None:\n","        \"\"\"\n","        Initialize subword model.\n","\n","        Args:\n","            vocab_size (int): Maximum size of the vocabulary.\n","        \"\"\"\n","        self.vocab_size = vocab_size\n","        self.vocab: Set[str] = set()\n","        self.merges: List[Tuple[str, str]] = []\n","\n","    def train_bpe(self, corpus: List[str]) -> None:\n","        \"\"\"\n","        Train a Byte-Pair Encoding (BPE) model on a corpus.\n","\n","        Args:\n","            corpus (List[str]): List of words in the corpus.\n","\n","        Example:\n","            >>> model = SubwordModel(vocab_size=10)\n","            >>> corpus = ['low', 'lower', 'lowest', 'widest']\n","            >>> model.train_bpe(corpus)\n","        \"\"\"\n","        # Step 1: Initialize vocabulary with character-level tokens\n","        word_freqs = Counter(corpus)\n","        word_splits: Dict[str, List[str]] = {}\n","        for word, freq in word_freqs.items():\n","            chars = list(word) + ['</w>']  # Add end-of-word symbol\n","            word_splits[word] = chars\n","            self.vocab.update(chars)\n","\n","        # Step 2: Perform BPE merges until vocab size is reached\n","        while len(self.vocab) < self.vocab_size:\n","            # Count pairs of adjacent symbols\n","            pair_freqs = defaultdict(int)\n","            for word, splits in word_splits.items():\n","                freq = word_freqs[word]\n","                for i in range(len(splits) - 1):\n","                    pair = (splits[i], splits[i + 1])\n","                    pair_freqs[pair] += freq\n","\n","            if not pair_freqs:\n","                break\n","\n","            # Find most frequent pair\n","            most_frequent_pair = max(pair_freqs, key=pair_freqs.get)\n","            self.merges.append(most_frequent_pair)\n","\n","            # Merge the most frequent pair in the vocabulary\n","            new_subword = ''.join(most_frequent_pair)\n","            self.vocab.add(new_subword)\n","\n","            # Update word splits by merging the pair\n","            for word in word_splits:\n","                splits = word_splits[word]\n","                new_splits = []\n","                i = 0\n","                while i < len(splits):\n","                    if (\n","                        i < len(splits) - 1\n","                        and (splits[i], splits[i + 1]) == most_frequent_pair\n","                    ):\n","                        new_splits.append(new_subword)\n","                        i += 2\n","                    else:\n","                        new_splits.append(splits[i])\n","                        i += 1\n","                word_splits[word] = new_splits\n","\n","    def tokenize_bpe(self, word: str) -> List[str]:\n","        \"\"\"\n","        Tokenize a word using trained BPE merges.\n","\n","        Args:\n","            word (str): Input word to tokenize.\n","\n","        Returns:\n","            List[str]: List of subword tokens.\n","\n","        Example:\n","            >>> model = SubwordModel(vocab_size=10)\n","            >>> model.tokenize_bpe(\"lowest\")\n","            ['low', 'est']\n","        \"\"\"\n","        if not self.merges:\n","            return list(word) + ['</w>']\n","\n","        # Start with character-level split\n","        splits = list(word) + ['</w>']\n","        for pair in self.merges:\n","            new_subword = ''.join(pair)\n","            new_splits = []\n","            i = 0\n","            while i < len(splits):\n","                if (\n","                    i < len(splits) - 1\n","                    and (splits[i], splits[i + 1]) == pair\n","                ):\n","                    new_splits.append(new_subword)\n","                    i += 2\n","                else:\n","                    new_splits.append(splits[i])\n","                    i += 1\n","            splits = new_splits\n","        return splits\n","\n","    def train_wordpiece(self, corpus: List[str]) -> None:\n","        \"\"\"\n","        Train a WordPiece model on a corpus (simplified version).\n","\n","        Args:\n","            corpus (List[str]): List of words in the corpus.\n","        \"\"\"\n","        # Implementation of WordPiece is complex and often requires likelihood scoring.\n","        # Here, we simulate a simplified version by reusing BPE with a different merging strategy.\n","        # In practice, WordPiece uses a language model to score merges.\n","        self.train_bpe(corpus)  # Placeholder for actual WordPiece implementation\n","\n","    def tokenize_wordpiece(self, word: str) -> List[str]:\n","        \"\"\"\n","        Tokenize a word using trained WordPiece model.\n","\n","        Args:\n","            word (str): Input word to tokenize.\n","\n","        Returns:\n","            List[str]: List of subword tokens.\n","        \"\"\"\n","        # Placeholder: In practice, WordPiece uses a different strategy\n","        return self.tokenize_bpe(word)\n","\n","\n","# Example usage of SubwordModel\n","def demonstrate_subword_modeling() -> None:\n","    \"\"\"Demonstrate subword modeling with examples.\"\"\"\n","    model = SubwordModel(vocab_size=15)\n","\n","    # Example corpus\n","    corpus = ['low', 'lower', 'lowest', 'widest', 'new', 'newer', 'newest']\n","\n","    # Train BPE\n","    print(\"\\n=== Byte-Pair Encoding (BPE) ===\")\n","    model.train_bpe(corpus)\n","    print(f\"Learned vocabulary: {model.vocab}\")\n","    print(f\"Merges: {model.merges}\")\n","\n","    # Tokenize a new word\n","    word = \"lowest\"\n","    tokens = model.tokenize_bpe(word)\n","    print(f\"Word: {word}\")\n","    print(f\"BPE Tokens: {tokens}\")\n","\n","    # Train WordPiece (simplified)\n","    print(\"\\n=== WordPiece (Simplified) ===\")\n","    model.train_wordpiece(corpus)\n","    tokens = model.tokenize_wordpiece(word)\n","    print(f\"Word: {word}\")\n","    print(f\"WordPiece Tokens: {tokens}\")\n","\n","\n","# ============================================================================\n","# SECTION 3: EXCEPTION HANDLING AND EDGE CASES\n","# ============================================================================\n","\n","\"\"\"\n","Exception Handling and Edge Cases:\n","When dealing with word structure and subword models, several edge cases must be handled:\n","1. Empty or invalid input text.\n","2. Special characters, numbers, or punctuation.\n","3. Out-of-vocabulary (OOV) words in subword models.\n","4. Language-specific nuances (e.g., contractions, compound words).\n","\"\"\"\n","\n","class RobustWordAnalyzer:\n","    \"\"\"\n","    A robust version of WordStructureAnalyzer and SubwordModel with exception handling.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size: int = 1000) -> None:\n","        \"\"\"Initialize analyzer with error handling.\"\"\"\n","        self.analyzer = WordStructureAnalyzer()\n","        self.subword_model = SubwordModel(vocab_size=vocab_size)\n","\n","    def tokenize_with_exceptions(self, text: str) -> List[str]:\n","        \"\"\"\n","        Tokenize text with exception handling.\n","\n","        Args:\n","            text (str): Input text to tokenize.\n","\n","        Returns:\n","            List[str]: List of tokens or empty list if invalid.\n","\n","        Raises:\n","            ValueError: If input is not a string or is empty.\n","        \"\"\"\n","        if not isinstance(text, str):\n","            raise ValueError(\"Input must be a string.\")\n","        if not text.strip():\n","            raise ValueError(\"Input text cannot be empty.\")\n","        try:\n","            return self.analyzer.tokenize(text)\n","        except Exception as e:\n","            print(f\"Error during tokenization: {e}\")\n","            return []\n","\n","    def subword_tokenize_with_exceptions(self, word: str) -> List[str]:\n","        \"\"\"\n","        Tokenize a word into subwords with exception handling.\n","\n","        Args:\n","            word (str): Input word to tokenize.\n","\n","        Returns:\n","            List[str]: List of subword tokens or empty list if invalid.\n","\n","        Raises:\n","            ValueError: If input is not a string or is empty.\n","        \"\"\"\n","        if not isinstance(word, str):\n","            raise ValueError(\"Input must be a string.\")\n","        if not word.strip():\n","            raise ValueError(\"Input word cannot be empty.\")\n","        try:\n","            return self.subword_model.tokenize_bpe(word)\n","        except Exception as e:\n","            print(f\"Error during subword tokenization: {e}\")\n","            return []\n","\n","\n","# Example usage of RobustWordAnalyzer\n","def demonstrate_exception_handling() -> None:\n","    \"\"\"Demonstrate exception handling with examples.\"\"\"\n","    robust_analyzer = RobustWordAnalyzer(vocab_size=15)\n","\n","    print(\"\\n=== Exception Handling ===\")\n","\n","    # Valid case\n","    try:\n","        tokens = robust_analyzer.tokenize_with_exceptions(\"Hello, world!\")\n","        print(f\"Valid tokenization: {tokens}\")\n","    except ValueError as e:\n","        print(f\"Error: {e}\")\n","\n","    # Empty input\n","    try:\n","        tokens = robust_analyzer.tokenize_with_exceptions(\"\")\n","        print(f\"Tokens: {tokens}\")\n","    except ValueError as e:\n","        print(f\"Error: {e}\")\n","\n","    # Invalid input type\n","    try:\n","        tokens = robust_analyzer.tokenize_with_exceptions(123)\n","        print(f\"Tokens: {tokens}\")\n","    except ValueError as e:\n","        print(f\"Error: {e}\")\n","\n","    # Subword tokenization of OOV word\n","    try:\n","        tokens = robust_analyzer.subword_tokenize_with_exceptions(\"unseenword\")\n","        print(f\"Subword tokens for 'unseenword': {tokens}\")\n","    except ValueError as e:\n","        print(f\"Error: {e}\")\n","\n","\n","# ============================================================================\n","# SECTION 4: PRACTICAL EXAMPLES AND USE CASES\n","# ============================================================================\n","\n","\"\"\"\n","Practical Examples and Use Cases:\n","This section provides real-world examples of how word structure analysis and subword modeling\n","can be applied in NLP tasks, such as text preprocessing, search engines, and machine translation.\n","\"\"\"\n","\n","def practical_examples() -> None:\n","    \"\"\"Demonstrate practical use cases with examples.\"\"\"\n","    analyzer = WordStructureAnalyzer()\n","    subword_model = SubwordModel(vocab_size=20)\n","\n","    print(\"\\n=== Practical Examples ===\")\n","\n","    # Use Case 1: Text Preprocessing for Search Engine\n","    print(\"\\nUse Case 1: Text Preprocessing for Search Engine\")\n","    query = \"Running faster is better than walking slowly!\"\n","    tokens = analyzer.tokenize(query)\n","    stemmed_tokens = [analyzer.stem(token) for token in tokens]\n","    print(f\"Query: {query}\")\n","    print(f\"Stemmed tokens for indexing: {stemmed_tokens}\")\n","\n","    # Use Case 2: Subword Modeling for Machine Translation\n","    print(\"\\nUse Case 2: Subword Modeling for Machine Translation\")\n","    corpus = ['play', 'playing', 'played', 'plays', 'player']\n","    subword_model.train_bpe(corpus)\n","    word = \"players\"\n","    subword_tokens = subword_model.tokenize_bpe(word)\n","    print(f\"Word: {word}\")\n","    print(f\"Subword tokens for translation model: {subword_tokens}\")\n","\n","    # Use Case 3: Handling Compound Words\n","    print(\"\\nUse Case 3: Handling Compound Words\")\n","    compound_word = \"icecream\"\n","    subword_tokens = subword_model.tokenize_bpe(compound_word)\n","    print(f\"Compound word: {compound_word}\")\n","    print(f\"Subword tokens: {subword_tokens}\")\n","\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    \"\"\"\n","    Main execution block to run all demonstrations.\n","    \"\"\"\n","    print(\"=== Word Structure and Subword Models Demonstration ===\")\n","\n","    # Demonstrate word structure analysis\n","    demonstrate_word_structure()\n","\n","    # Demonstrate subword modeling\n","    demonstrate_subword_modeling()\n","\n","    # Demonstrate exception handling\n","    demonstrate_exception_handling()\n","\n","    # Demonstrate practical examples\n","    practical_examples()\n","\n","\"\"\"\n","Key Takeaways:\n","1. Word structure analysis (tokenization, stemming, lemmatization) is essential for text preprocessing.\n","2. Subword modeling (BPE, WordPiece) is crucial for handling large vocabularies and OOV words in NLP.\n","3. Robust exception handling ensures code reliability in real-world applications.\n","4. Practical use cases demonstrate the importance of these techniques in NLP tasks.\n","\n","This implementation adheres to PEP-8 standards, is modular, and includes detailed explanations\n","and examples for next-generation coders to learn from.\n","\"\"\""]},{"cell_type":"code","source":[],"metadata":{"id":"fIksfGtJNSO2"},"execution_count":null,"outputs":[]}]}