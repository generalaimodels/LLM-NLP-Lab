{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO2v3vhTW3Ym/Z7gTKSAIf4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Byte Pair Encoding (BPE), Unigram Language Model, and WordPiece Tokenization\n","\n","## Byte Pair Encoding (BPE)\n","\n","### Definition\n","Byte Pair Encoding is a subword tokenization algorithm originally developed as a data compression technique by Gage and Gale (1994) and adapted for NLP by Sennrich et al. (2016). BPE iteratively merges the most frequent pairs of adjacent tokens to build a vocabulary that effectively handles rare words while maintaining a fixed vocabulary size.\n","\n","### Mathematical Formulation\n","In BPE, we begin with a vocabulary of individual characters and incrementally merge the most frequent adjacent token pairs.\n","\n","Let:\n","- $V$ be the vocabulary of tokens\n","- $C(xy)$ be the count of occurrences of adjacent tokens $x$ and $y$ in the corpus\n","\n","The merge operation selects the pair that maximizes:\n","\n","$$\\arg\\max_{(x,y) \\in V \\times V} C(xy)$$\n","\n","The probability of a token $t$ in the final vocabulary is estimated as:\n","\n","$$P(t) = \\frac{C(t)}{\\sum_{t' \\in V} C(t')}$$\n","\n","### Core Principles\n","1. Initialize the vocabulary with individual characters/bytes\n","2. Count frequencies of all adjacent token pairs in the corpus\n","3. Iteratively merge the most frequent pair and add the new token to the vocabulary\n","4. Update the corpus by replacing all occurrences of the merged pair\n","5. Continue until reaching the desired vocabulary size or merge operations limit\n","\n","### Pseudo-Algorithm\n","```\n","function BPE(corpus, num_merges):\n","    # Initialize vocabulary with individual characters\n","    vocab = set of all characters in corpus\n","    \n","    # Initialize each word as a sequence of characters\n","    words = {(word, frequency): list of characters in word for word, frequency in corpus}\n","    \n","    for i = 1 to num_merges:\n","        # Count all pairs\n","        pairs = {}\n","        for (word, freq), tokens in words.items():\n","            for j in range(len(tokens) - 1):\n","                pair = (tokens[j], tokens[j+1])\n","                pairs[pair] = pairs.get(pair, 0) + freq\n","        \n","        # Find most frequent pair\n","        best_pair = max(pairs, key=pairs.get)\n","        \n","        # Create new merged token\n","        new_token = best_pair[0] + best_pair[1]\n","        vocab.add(new_token)\n","        \n","        # Update words by replacing the pair\n","        new_words = {}\n","        for (word, freq), tokens in words.items():\n","            new_tokens = []\n","            i = 0\n","            while i < len(tokens):\n","                if i < len(tokens) - 1 and tokens[i] == best_pair[0] and tokens[i+1] == best_pair[1]:\n","                    new_tokens.append(new_token)\n","                    i += 2\n","                else:\n","                    new_tokens.append(tokens[i])\n","                    i += 1\n","            new_words[(word, freq)] = new_tokens\n","        words = new_words\n","    \n","    return vocab\n","```\n","\n","### Importance\n","BPE is vital in NLP because it:\n","- Solves the out-of-vocabulary problem by representing unknown words as sequences of subwords\n","- Enables effective handling of morphologically rich languages\n","- Provides a compact vocabulary that balances common words and subword units\n","- Forms the foundation for tokenization in many state-of-the-art language models\n","\n","### Pros and Cons\n","\n","#### Pros\n","- Simple and efficient implementation\n","- Handles morphologically rich languages effectively\n","- Creates a fixed-size vocabulary that minimizes out-of-vocabulary tokens\n","- Language-agnostic approach that works for any script\n","- Produces interpretable subword units\n","\n","#### Cons\n","- Merges are based solely on frequency, not linguistic meaning\n","- Can create unintuitive or linguistically meaningless subword units\n","- No consideration of context when creating tokens\n","- Original implementation doesn't directly optimize for likelihood of training data\n","\n","### Recent Advancements\n","- **BPE-Dropout**: Introduces stochastic segmentation during training to improve robustness\n","- **Regularized BPE**: Incorporates regularization terms to prevent overfitting\n","- **Multilingual BPE**: Shared vocabulary across multiple languages enabling cross-lingual transfer\n","- **SentencePiece BPE**: Language-agnostic implementation that treats text as Unicode characters\n","\n","## Unigram Language Model\n","\n","### Definition\n","The Unigram Language Model tokenization is a probabilistic subword segmentation method introduced by Kudo (2018) that treats tokenization as a statistical inference problem, aiming to find the subword vocabulary that maximizes the likelihood of the training corpus.\n","\n","### Mathematical Formulation\n","The Unigram model defines the probability of a sentence $X$ as:\n","\n","$$P(X) = \\sum_{x \\in S(X)} P(x)$$\n","\n","where $S(X)$ is the set of all possible segmentations, and $P(x)$ is the probability of a specific segmentation $x = (x_1, x_2, ..., x_m)$ given by:\n","\n","$$P(x) = \\prod_{i=1}^{m} P(x_i)$$\n","\n","The training objective is to find the vocabulary $V$ and token probabilities $P(x_i)$ that maximize:\n","\n","$$\\mathcal{L} = \\sum_{s \\in D} \\log \\left( \\sum_{x \\in S(s)} \\prod_{i=1}^{|x|} P(x_i) \\right)$$\n","\n","where $D$ is the training corpus.\n","\n","The Expectation-Maximization (EM) algorithm is used for optimization:\n","\n","**E-step**: Compute expected counts for each subword token:\n","$$c(w) = \\sum_{s \\in D} \\sum_{x \\in S(s)} P(x|s) \\cdot count(w, x)$$\n","\n","**M-step**: Update probabilities:\n","$$P(w) = \\frac{c(w)}{\\sum_{w' \\in V} c(w')}$$\n","\n","### Core Principles\n","1. Start with a large initial vocabulary (often generated by BPE)\n","2. Assign probabilities to each token based on occurrence frequency\n","3. Iteratively prune the vocabulary by removing tokens that contribute least to the corpus likelihood\n","4. Estimate the likelihood loss from removing each token\n","5. Use EM algorithm to re-estimate token probabilities after pruning\n","6. For tokenization, find the most probable segmentation using the Viterbi algorithm\n","\n","### Pseudo-Algorithm\n","\n","```\n","function UnigramTraining(corpus, initial_vocab_size, target_vocab_size):\n","    # Initialize with a large vocabulary\n","    vocab = Initialize_Vocabulary(corpus, initial_vocab_size)\n","    \n","    while size(vocab) > target_vocab_size:\n","        # Calculate token probabilities\n","        probs = {}\n","        total_count = 0\n","        for token in vocab:\n","            count = Count_Token_Occurrences(token, corpus)\n","            probs[token] = count\n","            total_count += count\n","        \n","        for token in vocab:\n","            probs[token] /= total_count\n","        \n","        # Compute loss for each token if removed\n","        token_losses = {}\n","        for token in vocab:\n","            loss = Compute_Loss_Without_Token(token, vocab, probs, corpus)\n","            token_losses[token] = loss\n","        \n","        # Sort tokens by loss impact\n","        sorted_tokens = Sort_By_Loss(token_losses)\n","        \n","        # Remove p% of tokens with lowest impact\n","        to_remove = sorted_tokens[0:int(0.2 * len(vocab))]\n","        for token in to_remove:\n","            vocab.remove(token)\n","    \n","    return vocab, Calculate_Final_Probabilities(vocab, corpus)\n","\n","function UnigramTokenization(text, vocab, probs):\n","    # Viterbi algorithm for optimal segmentation\n","    n = length(text)\n","    best_path = array of size n+1\n","    best_score = array of size n+1, initialized with -infinity\n","    best_score[0] = 0\n","    \n","    for i = 0 to n-1:\n","        for j = i+1 to min(i+MAX_TOKEN_LENGTH, n):\n","            if text[i:j] in vocab:\n","                score = best_score[i] + log(probs[text[i:j]])\n","                if score > best_score[j]:\n","                    best_score[j] = score\n","                    best_path[j] = i\n","    \n","    # Backtrack to find tokens\n","    tokens = []\n","    pos = n\n","    while pos > 0:\n","        prev_pos = best_path[pos]\n","        tokens.insert(0, text[prev_pos:pos])\n","        pos = prev_pos\n","    \n","    return tokens\n","```\n","\n","### Importance\n","The Unigram model is significant because it:\n","- Introduces a probabilistic framework for subword segmentation\n","- Directly optimizes the likelihood of the training data\n","- Allows for multiple possible segmentations of a word\n","- Provides a principled approach to vocabulary pruning\n","- Captures the statistical properties of language more effectively\n","\n","### Pros and Cons\n","\n","#### Pros\n","- Based on sound statistical principles\n","- Produces linguistically meaningful subword units\n","- Handles ambiguity through probabilistic segmentation\n","- Optimizes segmentation for likelihood of the training data\n","- Enables efficient pruning of ineffective tokens\n","\n","#### Cons\n","- More computationally intensive than BPE\n","- Requires careful initialization of the vocabulary\n","- More complex implementation than frequency-based methods\n","- Sensitive to hyperparameter choices\n","\n","### Recent Advancements\n","- **SentencePiece**: End-to-end text tokenization with Unigram model\n","- **Subword Regularization**: Training with multiple tokenization candidates to improve robustness\n","- **Unigram Mixture Model**: Extensions to handle multiple languages or domains\n","- **Dynamic Programming Optimizations**: Faster training and inference algorithms\n","\n","## WordPiece Tokenization\n","\n","### Definition\n","WordPiece is a subword tokenization algorithm developed by Schuster and Nakajima (2012) at Google and later used in BERT and other transformer models. It's similar to BPE but uses a likelihood-based criterion for merging tokens rather than raw frequency.\n","\n","### Mathematical Formulation\n","WordPiece selects the pair that maximizes the likelihood of the training data after the merge. For pairs $(x,y)$, the merge criterion is:\n","\n","$$\\arg\\max_{(x,y) \\in V \\times V} \\frac{freq(xy)}{freq(x) \\times freq(y)}$$\n","\n","This is equivalent to maximizing the log-likelihood gain:\n","\n","$$\\arg\\max_{(x,y) \\in V \\times V} \\left[ freq(xy) \\times \\log\\frac{freq(xy)}{freq(x) \\times freq(y)} \\right]$$\n","\n","The probability of a sequence $x = (x_1, x_2, ..., x_m)$ is:\n","\n","$$P(x) = \\prod_{i=1}^{m} P(x_i)$$\n","\n","where $P(x_i)$ is estimated from the corpus frequencies.\n","\n","### Core Principles\n","1. Initialize vocabulary with basic units (usually characters)\n","2. Calculate likelihood gain for each potential merge\n","3. Select the merge that maximizes the likelihood gain\n","4. Add the new merged token to the vocabulary\n","5. Iterate until desired vocabulary size is reached\n","6. Use special WordPiece convention: prefix subwords with ## if they don't start a word\n","\n","### Pseudo-Algorithm\n","```\n","function WordPiece(corpus, num_merges):\n","    # Initialize vocabulary with individual characters\n","    vocab = set of all characters in corpus\n","    \n","    # Count token frequencies\n","    token_counts = Count_Token_Frequencies(corpus, vocab)\n","    \n","    for i = 1 to num_merges:\n","        best_score = -infinity\n","        best_pair = None\n","        \n","        # Evaluate all potential merges\n","        for each pair (x, y) where x and y appear adjacently in corpus:\n","            freq_xy = Count_Adjacent_Occurrences(x, y, corpus)\n","            freq_x = token_counts[x]\n","            freq_y = token_counts[y]\n","            \n","            # Calculate likelihood gain\n","            score = freq_xy / (freq_x * freq_y)\n","            \n","            if score > best_score:\n","                best_score = score\n","                best_pair = (x, y)\n","        \n","        # Merge the best pair\n","        new_token = best_pair[0] + best_pair[1]\n","        vocab.add(new_token)\n","        \n","        # Update corpus by replacing occurrences of best_pair with new_token\n","        corpus = Replace_Token_Pair(corpus, best_pair, new_token)\n","        \n","        # Update token counts\n","        token_counts = Count_Token_Frequencies(corpus, vocab)\n","    \n","    # Apply WordPiece formatting (prefix with ##)\n","    formatted_vocab = Format_WordPiece_Vocabulary(vocab)\n","    \n","    return formatted_vocab\n","```\n","\n","### Importance\n","WordPiece is important because:\n","- It optimizes directly for likelihood of the training data\n","- It creates more linguistically motivated subword units\n","- It has been used in highly successful models like BERT, ALBERT, and other Google models\n","- It bridges the gap between character-level and word-level tokenization\n","- Its ## prefix convention clearly distinguishes word-initial from word-internal subwords\n","\n","### Pros and Cons\n","\n","#### Pros\n","- Creates linguistically more meaningful subwords than BPE\n","- Directly optimizes for data likelihood\n","- Effective for morphologically rich languages\n","- Clear marking of word-internal subwords improves readability\n","- Demonstrated effectiveness in state-of-the-art models\n","\n","#### Cons\n","- More computationally expensive than BPE\n","- Implementation details less documented (Google has not fully open-sourced it)\n","- May require more hyperparameter tuning\n","- Less flexible than fully probabilistic approaches like Unigram\n","\n","### Recent Advancements\n","- **Multilingual WordPiece**: Used in mBERT with shared vocabulary across languages\n","- **Efficiency optimizations**: Faster implementations for large-scale training\n","- **Dynamic WordPiece**: Adaptive vocabulary selection based on domain\n","- **WordPiece with contextual information**: Incorporating surrounding context for better segmentation\n","\n","## Comparative Analysis\n","\n","### Mathematical Differences\n","\n","| Algorithm | Merge Criterion | Optimization Target |\n","|-----------|-----------------|---------------------|\n","| BPE | $$\\arg\\max_{(x,y)} C(xy)$$ | Frequency of token pairs |\n","| WordPiece | $$\\arg\\max_{(x,y)} \\frac{freq(xy)}{freq(x) \\times freq(y)}$$ | Likelihood gain per merge |\n","| Unigram | $$\\arg\\max_{V, P} \\sum_{s \\in D} \\log \\left( \\sum_{x \\in S(s)} P(x) \\right)$$ | Overall corpus likelihood |\n","\n","### Tokenization Approaches\n","\n","| Algorithm | Building Direction | Approach | Segmentation Strategy |\n","|-----------|-------------------|----------|------------------------|\n","| BPE | Bottom-up | Greedy, deterministic | Maximum frequency merging |\n","| WordPiece | Bottom-up | Greedy, deterministic | Maximum likelihood gain |\n","| Unigram | Top-down | Probabilistic, iterative pruning | Multiple segmentation candidates with EM |\n","\n","### Implementation and Usage\n","\n","| Algorithm | Computational Complexity | Implementation Difficulty | Notable Models |\n","|-----------|-------------------------|---------------------------|----------------|\n","| BPE | Lower | Simpler | GPT, RoBERTa, XLM |\n","| WordPiece | Medium | Medium | BERT, ALBERT, DistilBERT |\n","| Unigram | Higher | Complex | T5, SentencePiece applications |"],"metadata":{"id":"s_JIPd77CDi4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0roWTDzdCC86"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"yH2zt4e2GWGj"},"execution_count":null,"outputs":[]}]}