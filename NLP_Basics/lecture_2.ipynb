{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOBv7xHskBtEWv6a2sTy9U8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Neural Computation: Mathematical Foundations and Technical Implementation\n","\n","## Neural Computation\n","\n","Neural computation refers to information processing systems inspired by biological neural networks. Mathematically, neural computation implements function approximation through distributed representations and parallel processing.\n","\n","A neural computational system can be defined as:\n","\n","$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$$\n","\n","Where the function $f$ maps input space $\\mathbb{R}^n$ to output space $\\mathbb{R}^m$ through a series of transformations. The fundamental computational unit transforms input vector $\\mathbf{x} \\in \\mathbb{R}^n$ via:\n","\n","$$y = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\n","\n","Where $w_i$ represents weights, $b$ is bias, and $\\sigma$ is a non-linear activation function.\n","\n","## Binary Logistic Regression Unit as a Neuron\n","\n","A binary logistic regression unit implements a mapping $f: \\mathbb{R}^n \\rightarrow [0,1]$ that models the conditional probability:\n","\n","$$P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$$\n","\n","Where $\\sigma$ is the logistic function:\n","\n","$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n","\n","This directly parallels a biological neuron where:\n","- Input features $\\mathbf{x}$ correspond to dendritic inputs\n","- Weights $\\mathbf{w}$ correspond to synaptic strengths\n","- Bias $b$ corresponds to activation threshold\n","- Sigmoid function $\\sigma$ corresponds to firing rate response\n","\n","The decision boundary is defined by:\n","\n","$$\\mathbf{w}^T\\mathbf{x} + b = 0$$\n","\n","Creating a hyperplane in the feature space that separates the two classes.\n","\n","## Neural Network as Multiple Logistic Regressions\n","\n","A neural network extends this concept by implementing multiple logistic regression units running simultaneously with interconnections. For a network with $L$ layers, each layer $l$ computes:\n","\n","$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n","$$\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})$$\n","\n","Where:\n","- $\\mathbf{a}^{[l-1]}$ is the activation from the previous layer\n","- $\\mathbf{W}^{[l]}$ is the weight matrix for layer $l$\n","- $\\mathbf{b}^{[l]}$ is the bias vector for layer $l$\n","- $\\sigma^{[l]}$ is the activation function for layer $l$\n","\n","The composite function represented by the entire network is:\n","\n","$$f(\\mathbf{x}) = \\sigma^{[L]}(\\mathbf{W}^{[L]}\\sigma^{[L-1]}(...\\sigma^{[1]}(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]})...) + \\mathbf{b}^{[L]})$$\n","\n","Each unit effectively performs logistic regression, but their interconnected nature enables the modeling of complex, non-linear relationships.\n","\n","## Matrix Notation for a Layer\n","\n","For a layer with $n^{[l-1]}$ input units and $n^{[l]}$ output units, the computation can be efficiently expressed in matrix form:\n","\n","$$\\mathbf{Z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{A}^{[l-1]} + \\mathbf{b}^{[l]}$$\n","$$\\mathbf{A}^{[l]} = \\sigma^{[l]}(\\mathbf{Z}^{[l]})$$\n","\n","Where:\n","- $\\mathbf{W}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ is the weight matrix\n","- $\\mathbf{A}^{[l-1]} \\in \\mathbb{R}^{n^{[l-1]} \\times m}$ contains activations for $m$ samples\n","- $\\mathbf{b}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times 1}$ is the bias vector\n","- $\\mathbf{Z}^{[l]}$ is the pre-activation output\n","\n","For a single sample $\\mathbf{x}^{(i)}$, the computation becomes:\n","\n","$$\\mathbf{z}^{[l](i)} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1](i)} + \\mathbf{b}^{[l]}$$\n","\n","This matrix formulation enables vectorization, critical for efficient computation on modern hardware architectures.\n","\n","## Non-linearities: Mathematical Necessity\n","\n","Non-linear activation functions are mathematically essential in neural networks. Consider a network with linear activations:\n","\n","$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n","$$\\mathbf{a}^{[l]} = \\mathbf{z}^{[l]}$$\n","\n","For a two-layer network:\n","$$\\mathbf{a}^{[2]} = \\mathbf{W}^{[2]}(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]}) + \\mathbf{b}^{[2]}$$\n","$$= \\mathbf{W}^{[2]}\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{W}^{[2]}\\mathbf{b}^{[1]} + \\mathbf{b}^{[2]}$$\n","$$= \\mathbf{W}'\\mathbf{x} + \\mathbf{b}'$$\n","\n","Where $\\mathbf{W}' = \\mathbf{W}^{[2]}\\mathbf{W}^{[1]}$ and $\\mathbf{b}' = \\mathbf{W}^{[2]}\\mathbf{b}^{[1]} + \\mathbf{b}^{[2]}$\n","\n","This demonstrates that multiple linear layers collapse mathematically into a single linear transformation, severely limiting modeling capacity. By introducing non-linearities $\\sigma$:\n","\n","$$\\mathbf{a}^{[1]} = \\sigma(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]})$$\n","$$\\mathbf{a}^{[2]} = \\mathbf{W}^{[2]}\\mathbf{a}^{[1]} + \\mathbf{b}^{[2]} = \\mathbf{W}^{[2]}\\sigma(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]}) + \\mathbf{b}^{[2]}$$\n","\n","This enables the network to model non-linear relationships and approximate arbitrary continuous functions per the Universal Approximation Theorem.\n","\n","Common non-linearities include:\n","- Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n","- Hyperbolic tangent: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n","- ReLU: $\\text{ReLU}(z) = \\max(0, z)$\n","\n","Each introduces different properties regarding gradient flow, computational efficiency, and representational capacity."],"metadata":{"id":"gpqj3NrQxIfe"}},{"cell_type":"markdown","source":["# Gradients, Jacobian Matrices, and Backpropagation in Neural Networks\n","\n","## Gradients\n","\n","The gradient represents the multi-dimensional generalization of the derivative for scalar-valued functions of multiple variables. For a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, the gradient $\\nabla f$ is defined as the vector of partial derivatives:\n","\n","$$\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n","\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\\\\n","\\frac{\\partial f}{\\partial x_2}(\\mathbf{x}) \\\\\n","\\vdots \\\\\n","\\frac{\\partial f}{\\partial x_n}(\\mathbf{x})\n","\\end{bmatrix}$$\n","\n","The gradient has fundamental mathematical properties:\n","1. It points in the direction of steepest ascent of $f$\n","2. The magnitude $\\|\\nabla f(\\mathbf{x})\\|$ indicates the rate of change in that direction\n","3. For any unit vector $\\mathbf{u}$, the directional derivative is given by $\\nabla f(\\mathbf{x}) \\cdot \\mathbf{u}$\n","\n","In optimization applications, we update parameters iteratively using:\n","\n","$$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha \\nabla f(\\mathbf{x}_t)$$\n","\n","where $\\alpha$ is the learning rate.\n","\n","## Jacobian Matrix: Generalization of the Gradient\n","\n","The Jacobian matrix extends the gradient concept to vector-valued functions. For a function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with component functions $f_1, f_2, \\ldots, f_m$, the Jacobian $\\mathbf{J}_\\mathbf{f}$ is defined as:\n","\n","$$\\mathbf{J}_\\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix}\n","\\frac{\\partial f_1}{\\partial x_1}(\\mathbf{x}) & \\frac{\\partial f_1}{\\partial x_2}(\\mathbf{x}) & \\cdots & \\frac{\\partial f_1}{\\partial x_n}(\\mathbf{x}) \\\\\n","\\frac{\\partial f_2}{\\partial x_1}(\\mathbf{x}) & \\frac{\\partial f_2}{\\partial x_2}(\\mathbf{x}) & \\cdots & \\frac{\\partial f_2}{\\partial x_n}(\\mathbf{x}) \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","\\frac{\\partial f_m}{\\partial x_1}(\\mathbf{x}) & \\frac{\\partial f_m}{\\partial x_2}(\\mathbf{x}) & \\cdots & \\frac{\\partial f_m}{\\partial x_n}(\\mathbf{x})\n","\\end{bmatrix}$$\n","\n","The Jacobian $\\mathbf{J}_\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^{m \\times n}$ represents the best linear approximation of $\\mathbf{f}$ near $\\mathbf{x}$:\n","\n","$$\\mathbf{f}(\\mathbf{x} + \\mathbf{h}) \\approx \\mathbf{f}(\\mathbf{x}) + \\mathbf{J}_\\mathbf{f}(\\mathbf{x})\\mathbf{h}$$\n","\n","When $m = 1$, the Jacobian reduces to the gradient (transposed):\n","\n","$$\\mathbf{J}_f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^T$$\n","\n","## Chain Rule\n","\n","The chain rule enables the computation of derivatives for composite functions. For scalar-valued functions, if $y = g(u)$ and $u = h(x)$, then:\n","\n","$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n","\n","For vector-valued functions, given $\\mathbf{y} = \\mathbf{g}(\\mathbf{u})$ and $\\mathbf{u} = \\mathbf{h}(\\mathbf{x})$, the chain rule becomes:\n","\n","$$\\mathbf{J}_{\\mathbf{g} \\circ \\mathbf{h}}(\\mathbf{x}) = \\mathbf{J}_\\mathbf{g}(\\mathbf{h}(\\mathbf{x})) \\cdot \\mathbf{J}_\\mathbf{h}(\\mathbf{x})$$\n","\n","Mathematically, if $\\mathbf{g}: \\mathbb{R}^p \\rightarrow \\mathbb{R}^m$ and $\\mathbf{h}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^p$, then the Jacobian matrix of their composition has dimensions $\\mathbb{R}^{m \\times n}$ and is computed through matrix multiplication of Jacobians.\n","\n","## Example Jacobian: Elementwise Activation Function\n","\n","Consider an elementwise activation function $\\sigma: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ where each component $\\sigma_i(z) = \\sigma(z_i)$. The Jacobian matrix has a diagonal structure:\n","\n","$$\\mathbf{J}_\\sigma(\\mathbf{z}) = \\begin{bmatrix}\n","\\sigma'(z_1) & 0 & \\cdots & 0 \\\\\n","0 & \\sigma'(z_2) & \\cdots & 0 \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","0 & 0 & \\cdots & \\sigma'(z_n)\n","\\end{bmatrix} = \\text{diag}(\\sigma'(z_1), \\sigma'(z_2), \\ldots, \\sigma'(z_n))$$\n","\n","For specific activation functions:\n","\n","1. Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n","   $$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n","\n","2. ReLU: $\\sigma(z) = \\max(0, z)$\n","   $$\\sigma'(z) = \\begin{cases}\n","   1 & \\text{if } z > 0 \\\\\n","   0 & \\text{if } z \\leq 0\n","   \\end{cases}$$\n","\n","3. Tanh: $\\sigma(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n","   $$\\sigma'(z) = 1 - \\tanh^2(z)$$\n","\n","## Other Jacobians\n","\n","1. **Linear Transformation**: For $\\mathbf{f}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$ where $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$:\n","   $$\\mathbf{J}_\\mathbf{f}(\\mathbf{x}) = \\mathbf{W}$$\n","\n","2. **Matrix-Vector Product**: For $\\mathbf{f}(\\mathbf{W}) = \\mathbf{W}\\mathbf{x}$ with fixed $\\mathbf{x}$:\n","   $$\\frac{\\partial (\\mathbf{W}\\mathbf{x})_i}{\\partial W_{jk}} = \\begin{cases}\n","   x_k & \\text{if } i = j \\\\\n","   0 & \\text{otherwise}\n","   \\end{cases}$$\n","\n","3. **Element-wise Operations**: For $\\mathbf{f}(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\odot \\mathbf{y}$ (Hadamard product):\n","   $$\\frac{\\partial f_i}{\\partial x_j} = \\begin{cases}\n","   y_i & \\text{if } i = j \\\\\n","   0 & \\text{otherwise}\n","   \\end{cases}$$\n","\n","## Back to our Neural Net!\n","\n","In a neural network, the forward pass for layer $l$ typically computes:\n","$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n","$$\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})$$\n","\n","### 1. Breaking up equations into simple pieces\n","\n","We decompose these operations:\n","- Linear transformation: $\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$\n","- Non-linear activation: $\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})$\n","\n","### 2. Applying the chain rule\n","\n","Consider a loss function $L$ dependent on the network output. To compute $\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}}$, we apply the chain rule:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} \\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{W}^{[l]}}$$\n","\n","For multiple layers, the recursion expands:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\frac{\\partial \\mathbf{a}^{[l]}}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot \\sigma'^{[l]}(\\mathbf{z}^{[l]})$$\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{a}^{[l-1]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} \\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{a}^{[l-1]}} = (\\mathbf{W}^{[l]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$\n","\n","### 3. Writing out the Jacobians\n","\n","For an element-wise activation function:\n","\n","$$\\frac{\\partial \\mathbf{a}^{[l]}}{\\partial \\mathbf{z}^{[l]}} = \\text{diag}(\\sigma'^{[l]}(\\mathbf{z}^{[l]}_1), \\sigma'^{[l]}(\\mathbf{z}^{[l]}_2), \\ldots, \\sigma'^{[l]}(\\mathbf{z}^{[l]}_n))$$\n","\n","For a linear transformation:\n","$$\\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{W}^{[l]}_{ij}} = \\begin{cases}\n","a^{[l-1]}_j & \\text{for element } z^{[l]}_i \\\\\n","0 & \\text{otherwise}\n","\\end{cases}$$\n","\n","## Re-using Computation\n","\n","During backpropagation, we can reuse computations from the forward pass. Define $\\boldsymbol{\\delta}^{[l]} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$. Then:\n","\n","$$\\boldsymbol{\\delta}^{[l]} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot \\sigma'^{[l]}(\\mathbf{z}^{[l]})$$\n","\n","$$\\boldsymbol{\\delta}^{[l-1]} = (\\mathbf{W}^{[l]})^T \\boldsymbol{\\delta}^{[l]} \\odot \\sigma'^{[l-1]}(\\mathbf{z}^{[l-1]})$$\n","\n","The gradient with respect to weights becomes:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\boldsymbol{\\delta}^{[l]} (\\mathbf{a}^{[l-1]})^T$$\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\boldsymbol{\\delta}^{[l]}$$\n","\n","## Derivative with respect to Matrix: Output shape\n","\n","For a scalar function $L$ with respect to a matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, the derivative $\\frac{\\partial L}{\\partial \\mathbf{W}}$ has the same dimensions $\\mathbb{R}^{m \\times n}$. Specifically:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\begin{bmatrix}\n","\\frac{\\partial L}{\\partial W_{11}} & \\frac{\\partial L}{\\partial W_{12}} & \\cdots & \\frac{\\partial L}{\\partial W_{1n}} \\\\\n","\\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{22}} & \\cdots & \\frac{\\partial L}{\\partial W_{2n}} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","\\frac{\\partial L}{\\partial W_{m1}} & \\frac{\\partial L}{\\partial W_{m2}} & \\cdots & \\frac{\\partial L}{\\partial W_{mn}}\n","\\end{bmatrix}$$\n","\n","## Deriving local input gradient in backprop\n","\n","The local input gradient for layer $l$ computes how the loss changes with respect to the input of that layer. For input $\\mathbf{a}^{[l-1]}$ to layer $l$:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{a}^{[l-1]}} = (\\mathbf{W}^{[l]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = (\\mathbf{W}^{[l]})^T \\boldsymbol{\\delta}^{[l]}$$\n","\n","This expression quantifies how changes in the activations of layer $l-1$ affect the overall loss, forming the critical recursive relationship that enables efficient backpropagation through the network.\n","\n","The complete backpropagation algorithm is therefore:\n","\n","1. Perform forward pass to compute all $\\mathbf{z}^{[l]}$ and $\\mathbf{a}^{[l]}$\n","2. Compute output layer error: $\\boldsymbol{\\delta}^{[L]} = \\nabla_{\\mathbf{a}^{[L]}}L \\odot \\sigma'^{[L]}(\\mathbf{z}^{[L]})$\n","3. Backpropagate error: $\\boldsymbol{\\delta}^{[l-1]} = (\\mathbf{W}^{[l]})^T \\boldsymbol{\\delta}^{[l]} \\odot \\sigma'^{[l-1]}(\\mathbf{z}^{[l-1]})$\n","4. Compute gradients: $\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\boldsymbol{\\delta}^{[l]} (\\mathbf{a}^{[l-1]})^T$, $\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\boldsymbol{\\delta}^{[l]}$\n","\n"],"metadata":{"id":"vTsy4d_pzOX1"}},{"cell_type":"markdown","source":["# Backpropagation and Computation Graphs: Mathematical Foundations\n","\n","##  Backpropagation\n","\n","Backpropagation is an efficient algorithm for computing gradients in parameterized computational models through recursive application of the chain rule of differentiation. Formally, given a scalar loss function $L: \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that depends on the output of a composite function $f(\\mathbf{x};\\boldsymbol{\\theta})$ with parameters $\\boldsymbol{\\theta}$, backpropagation computes $\\nabla_{\\boldsymbol{\\theta}}L$ with computational complexity proportional to the forward evaluation of $f$.\n","\n","The mathematical foundation of backpropagation derives from the chain rule for computing derivatives of composite functions. For scalar functions, if $y = g(h(x))$, then:\n","\n","$$\\frac{dy}{dx} = \\frac{dy}{dh} \\cdot \\frac{dh}{dx}$$\n","\n","This generalizes to vector-valued functions through the Jacobian formulation:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$$\n","\n","Where $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ is the Jacobian matrix $\\mathbf{J}$ with elements $J_{ij} = \\frac{\\partial y_i}{\\partial x_j}$.\n","\n","## Computation Graphs and Backpropagation\n","\n","A computation graph $G = (V, E)$ is a directed acyclic graph (DAG) where:\n","- Vertices $v \\in V$ represent variables or operations\n","- Edges $(u, v) \\in E$ represent dependencies between variables\n","- Input nodes have in-degree zero\n","- Output nodes produce the final computation result\n","- Intermediate nodes represent operations or transformations\n","\n","Each node $v_i$ computes a function $f_i$ of its inputs:\n","\n","$$v_i = f_i(\\text{Parents}(v_i))$$\n","\n","Mathematically, the computation graph encodes the decomposition of a complex function into primitive operations, enabling the systematic application of the chain rule.\n","\n","### 1. Fprop: Visit Nodes in Topological Sort Order\n","\n","Forward propagation traverses the graph in topological order, ensuring all inputs to a node are computed before the node itself:\n","\n","$$v_i = f_i(v_{j_1}, v_{j_2}, ..., v_{j_k})$$\n","\n","where $v_{j_1}, v_{j_2}, ..., v_{j_k}$ are the parent nodes of $v_i$.\n","\n","The topological ordering $\\pi$ satisfies the property that for every edge $(v_i, v_j) \\in E$, $\\pi(v_i) < \\pi(v_j)$, guaranteeing that all dependencies are resolved before computation.\n","\n","For a node representing a primitive operation $v_i = f_i(v_{j_1}, v_{j_2}, ..., v_{j_k})$, we compute and store:\n","1. The output value $v_i$\n","2. Additional information required for gradient computation (intermediate values)\n","\n","The forward pass has computational complexity $O(|E|)$ where $|E|$ is the number of edges in the graph.\n","\n","### 2. Bprop: Backward Gradient Computation\n","\n","Backward propagation computes gradients by applying the chain rule recursively through the graph in reverse topological order:\n","\n","1. Initialize output gradient $\\frac{\\partial L}{\\partial v_{\\text{output}}} = 1$ for the output node\n","2. For each node $v_i$ in reverse topological order:\n","   - Compute gradient with respect to each input $v_j$ using:\n","   \n","   $$\\frac{\\partial L}{\\partial v_j} += \\frac{\\partial L}{\\partial v_i} \\cdot \\frac{\\partial v_i}{\\partial v_j}$$\n","   \n","   - The += operator indicates accumulation of gradients when a node affects multiple downstream computations\n","\n","The mathematical justification follows from the multivariate chain rule. For a node $v_j$ that influences multiple nodes $v_{i_1}, v_{i_2}, ..., v_{i_m}$:\n","\n","$$\\frac{\\partial L}{\\partial v_j} = \\sum_{k=1}^{m} \\frac{\\partial L}{\\partial v_{i_k}} \\cdot \\frac{\\partial v_{i_k}}{\\partial v_j}$$\n","\n","Each local derivative $\\frac{\\partial v_i}{\\partial v_j}$ depends on the specific operation at node $v_i$. For common operations:\n","\n","1. Addition $(v_i = v_j + v_k)$: $\\frac{\\partial v_i}{\\partial v_j} = 1$\n","2. Multiplication $(v_i = v_j \\cdot v_k)$: $\\frac{\\partial v_i}{\\partial v_j} = v_k$\n","3. Function application $(v_i = f(v_j))$: $\\frac{\\partial v_i}{\\partial v_j} = f'(v_j)$\n","\n","The backward pass systematically computes all required partial derivatives, eventually yielding $\\frac{\\partial L}{\\partial \\theta_i}$ for each parameter $\\theta_i$ in the model.\n","\n","When implemented correctly, the backpropagation algorithm has the same asymptotic complexity as forward propagation, specifically $O(|E|)$. This equivalence derives from the chain rule structure: each edge in the computation graph corresponds to exactly one multiplication and addition operation during the backward pass.\n","\n","For neural networks with regular layer structures, the computation graph exhibits specific patterns that enable efficient matrix-based implementations. Consider a neural network layer:\n","\n","$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n","$$\\mathbf{a}^{[l]} = \\sigma(\\mathbf{z}^{[l]})$$\n","\n","In matrix notation, the gradient computation becomes:\n","\n","$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot \\sigma'(\\mathbf{z}^{[l]})$$\n","$$\\frac{\\partial L}{\\partial \\mathbf{a}^{[l-1]}} = (\\mathbf{W}^{[l]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$\n","$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} (\\mathbf{a}^{[l-1]})^T$$\n","$$\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$\n","\n","Here, $\\odot$ denotes the Hadamard (element-wise) product, reflecting the element-wise application of the activation function derivative.\n","\n","The Jacobian matrices for each layer transformation formalize these operations:\n","\n","1. For the affine transformation: $\\mathbf{J}_{\\mathbf{W}, \\mathbf{a}} = \\mathbf{W}$\n","2. For the element-wise activation: $\\mathbf{J}_{\\sigma} = \\text{diag}(\\sigma'(\\mathbf{z}))$\n","\n","Backpropagation through a neural network sequentially applies these Jacobian operations in reverse order, propagating error gradients from the output layer back to the input layer and computing parameter gradients along the way.\n","\n","The effectiveness of backpropagation derives from its computational efficiency, requiring only one forward and one backward pass through the computation graph to compute gradients for all parameters simultaneously. This efficiency has made deep learning computationally feasible on large-scale problems."],"metadata":{"id":"8BcqMH0Y0knm"}},{"cell_type":"markdown","source":["# Deep Learning Technical Analysis: Parameters, Regularization and Optimization\n","\n","## Models with Many Parameters and Regularization\n","\n","Modern neural networks operate with millions or billions of parameters, creating systems capable of extraordinary expressivity but vulnerable to overfitting. Mathematically, a model $f_\\theta(x)$ parameterized by vector $\\theta \\in \\mathbb{R}^d$ becomes overparameterized when $d \\gg n$, where $n$ represents training samples.\n","\n","The optimization objective without regularization is:\n","\n","$$ \\min_\\theta \\frac{1}{n}\\sum_{i=1}^{n}L(f_\\theta(x_i), y_i) $$\n","\n","Regularization addresses overfitting by constraining parameter values. The regularized objective becomes:\n","\n","$$ \\min_\\theta \\frac{1}{n}\\sum_{i=1}^{n}L(f_\\theta(x_i), y_i) + \\lambda R(\\theta) $$\n","\n","Where $\\lambda$ controls regularization strength and $R(\\theta)$ is the regularization function.\n","\n","L2 regularization (weight decay) penalizes large weights using squared magnitudes:\n","\n","$$ R_{L2}(\\theta) = \\frac{1}{2}\\|\\theta\\|_2^2 = \\frac{1}{2}\\sum_{j=1}^{d}\\theta_j^2 $$\n","\n","L1 regularization induces sparsity by penalizing absolute weight values:\n","\n","$$ R_{L1}(\\theta) = \\|\\theta\\|_1 = \\sum_{j=1}^{d}|\\theta_j| $$\n","\n","The gradient update with L2 regularization becomes:\n","\n","$$ \\theta_{t+1} = \\theta_t - \\eta\\left(\\nabla_\\theta L(f_\\theta(x), y) + \\lambda\\theta_t\\right) = (1-\\eta\\lambda)\\theta_t - \\eta\\nabla_\\theta L(f_\\theta(x), y) $$\n","\n","This effectively shrinks weights by factor $(1-\\eta\\lambda)$ in each iteration.\n","\n","Mathematically, regularization modifies the loss landscape, eliminating sharp minima that generalize poorly and favoring flatter ones that generalize better under distribution shift, expressed as:\n","\n","$$ \\mathbb{E}_{x\\sim\\mathcal{D}_{test}}[L(f_\\theta(x), y)] \\leq \\mathbb{E}_{x\\sim\\mathcal{D}_{train}}[L(f_\\theta(x), y)] + \\mathcal{C}(\\theta, n) $$\n","\n","Where $\\mathcal{C}(\\theta, n)$ is the complexity term regularization minimizes.\n","\n","## Dropout\n","\n","Dropout implements stochastic regularization through temporary neuron deactivation during training. For each forward pass, neurons are retained with probability $p$ and dropped with probability $(1-p)$.\n","\n","Mathematically, given layer output $\\mathbf{y}$, dropout applies:\n","\n","$$ \\mathbf{r} \\sim \\text{Bernoulli}(p) $$\n","$$ \\tilde{\\mathbf{y}} = \\mathbf{r} \\odot \\mathbf{y} $$\n","\n","Where $\\odot$ denotes element-wise multiplication and $\\mathbf{r}$ is a binary mask. During inference, the expected output is approximated by scaling:\n","\n","$$ \\mathbb{E}[\\tilde{\\mathbf{y}}] = p\\mathbf{y} $$\n","\n","To maintain consistent expected values between training and inference, we either scale during training:\n","\n","$$ \\tilde{\\mathbf{y}}_{train} = \\frac{\\mathbf{r} \\odot \\mathbf{y}}{p} $$\n","\n","Or during inference (inverted dropout):\n","\n","$$ \\tilde{\\mathbf{y}}_{inference} = p\\mathbf{y} $$\n","\n","Dropout implements an implicit ensemble averaging of $2^N$ different \"thinned\" networks, where $N$ is the number of neurons. This provides Bayesian approximation properties, with the dropout probability governing the posterior distribution width.\n","\n","The dropout effect can be interpreted as adaptive L2 regularization:\n","\n","$$ \\mathbb{E}_{\\mathbf{r}}[L(f_{\\theta,\\mathbf{r}}(x), y)] \\approx L(f_\\theta(x), y) + \\lambda \\sum_{l} \\frac{p}{1-p}\\|\\mathbf{W}_l\\|_F^2 $$\n","\n","Where $\\mathbf{W}_l$ represents weights in layer $l$ and $\\|\\cdot\\|_F$ is the Frobenius norm.\n","\n","## Vectorization\n","\n","Vectorization transforms scalar operations into equivalent vector/matrix operations, enabling parallel computation exploitation. Given inputs $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ containing $n$ samples with $d$ features, the forward propagation in a layer is expressed as:\n","\n","$$ \\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b} $$\n","$$ \\mathbf{A} = \\sigma(\\mathbf{Z}) $$\n","\n","Where $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ contains weights, $\\mathbf{b} \\in \\mathbb{R}^m$ is the bias, and $\\sigma$ is applied element-wise.\n","\n","Computational complexity analysis shows vectorized operations achieve $O(ndm)$ complexity versus $O(n \\cdot d \\cdot m)$ for loops, with the constant factor significantly reduced through SIMD (Single Instruction Multiple Data) operations.\n","\n","Matrix calculus facilitates efficient gradient computation:\n","\n","$$ \\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{X}^T \\frac{\\partial L}{\\partial \\mathbf{Z}} $$\n","$$ \\frac{\\partial L}{\\partial \\mathbf{b}} = \\mathbf{1}^T \\frac{\\partial L}{\\partial \\mathbf{Z}} $$\n","$$ \\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{Z}} \\mathbf{W}^T $$\n","\n","The speedup factor from vectorization can be expressed as:\n","\n","$$ S = \\frac{T_{loop}}{T_{vector}} \\approx \\frac{c_{loop} \\cdot ndm}{c_{vector} \\cdot ndm} = \\frac{c_{loop}}{c_{vector}} $$\n","\n","Where constants $c_{loop} \\gg c_{vector}$ due to memory locality, cache efficiency, and hardware optimization.\n","\n","## Parameter Initialization\n","\n","Parameter initialization critically affects convergence and model performance. For a neural network with layers $l = 1,...,L$, proper initialization ensures stable signal propagation:\n","\n","$$ \\text{Var}(y^l) \\approx \\text{Var}(y^{l-1}) $$\n","\n","Xavier/Glorot initialization for tanh/sigmoid activations draws weights from:\n","\n","$$ W^l_{ij} \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right) $$\n","\n","Where $n_{in}$ and $n_{out}$ are input and output dimensions. This maintains variance across layers:\n","\n","$$ \\text{Var}(y^l) = n_{in} \\cdot \\text{Var}(W^l) \\cdot \\text{Var}(y^{l-1}) \\approx \\text{Var}(y^{l-1}) $$\n","\n","He initialization, designed for ReLU activations, accounts for variance reduction from rectification:\n","\n","$$ W^l_{ij} \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right) $$\n","\n","Orthogonal initialization ensures weight matrices satisfy:\n","\n","$$ \\mathbf{W}^T\\mathbf{W} = \\mathbf{I} $$\n","\n","Preserving gradient magnitudes during backpropagation through:\n","\n","$$ \\|\\mathbf{W}^T\\delta\\|_2 = \\|\\delta\\|_2 $$\n","\n","Mathematically, the vanishing/exploding gradient problem occurs when:\n","\n","$$ \\|\\nabla_{\\theta_l}L\\| = \\|\\nabla_{\\mathbf{y}^L}L \\cdot \\prod_{i=l+1}^{L} \\frac{\\partial \\mathbf{y}^i}{\\partial \\mathbf{y}^{i-1}} \\cdot \\frac{\\partial \\mathbf{y}^l}{\\partial \\theta_l}\\| $$\n","\n","Grows or diminishes exponentially with network depth when eigenvalues of Jacobians $\\frac{\\partial \\mathbf{y}^i}{\\partial \\mathbf{y}^{i-1}}$ deviate significantly from 1.\n","\n","## Optimizers\n","\n","Neural network training involves minimizing the objective:\n","\n","$$ \\min_\\theta \\mathcal{L}(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}L(f_\\theta(x_i), y_i) + \\lambda R(\\theta) $$\n","\n","Vanilla Gradient Descent updates parameters through:\n","\n","$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n","\n","Stochastic Gradient Descent approximates full gradient using mini-batches:\n","\n","$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L}_B(\\theta_t) $$\n","\n","Where $\\mathcal{L}_B$ represents the loss on mini-batch $B$.\n","\n","Momentum incorporates previous update directions:\n","\n","$$ v_{t+1} = \\gamma v_t + \\eta \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n","$$ \\theta_{t+1} = \\theta_t - v_{t+1} $$\n","\n","With theoretical convergence rate $O(1/t)$ for convex problems, improved to $O(1/t^2)$ with Nesterov acceleration:\n","\n","$$ v_{t+1} = \\gamma v_t + \\eta \\nabla_\\theta \\mathcal{L}(\\theta_t - \\gamma v_t) $$\n","$$ \\theta_{t+1} = \\theta_t - v_{t+1} $$\n","\n","Adaptive methods adjust learning rates per-parameter. AdaGrad accumulates squared gradients:\n","\n","$$ G_{t+1} = G_t + (\\nabla_\\theta \\mathcal{L}(\\theta_t))^2 $$\n","$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_{t+1} + \\epsilon}} \\odot \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n","\n","RMSProp uses exponential moving average for squared gradients:\n","\n","$$ G_{t+1} = \\beta G_t + (1-\\beta)(\\nabla_\\theta \\mathcal{L}(\\theta_t))^2 $$\n","$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_{t+1} + \\epsilon}} \\odot \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n","\n","Adam combines momentum and adaptive learning rates:\n","\n","$$ m_{t+1} = \\beta_1 m_t + (1-\\beta_1)\\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n","$$ v_{t+1} = \\beta_2 v_t + (1-\\beta_2)(\\nabla_\\theta \\mathcal{L}(\\theta_t))^2 $$\n","$$ \\hat{m}_{t+1} = \\frac{m_{t+1}}{1-\\beta_1^{t+1}} $$\n","$$ \\hat{v}_{t+1} = \\frac{v_{t+1}}{1-\\beta_2^{t+1}} $$\n","$$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon} $$\n","\n","Convergence analysis shows Adam achieves regret bound $O(\\sqrt{T})$ for convex problems and empirically navigates non-convex landscapes efficiently due to adaptive step sizes managing varying gradient magnitudes across parameters."],"metadata":{"id":"7rlbLx5B9oe1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"dwJQB2v50jdd"}}]}