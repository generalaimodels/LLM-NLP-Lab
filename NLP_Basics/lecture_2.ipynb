{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpqj3NrQxIfe"
   },
   "source": [
    "# Neural Computation: Mathematical Foundations and Technical Implementation\n",
    "\n",
    "## Neural Computation\n",
    "\n",
    "\n",
    "Neural computation refers to information processing systems inspired by biological neural networks. Mathematically, neural computation implements function approximation through distributed representations and parallel processing.\n",
    "![](./Image/9.webp)\n",
    "A neural computational system can be defined as:\n",
    "\n",
    "$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$$\n",
    "\n",
    "Where the function $f$ maps input space $\\mathbb{R}^n$ to output space $\\mathbb{R}^m$ through a series of transformations. The fundamental computational unit transforms input vector $\\mathbf{x} \\in \\mathbb{R}^n$ via:\n",
    "\n",
    "$$y = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\n",
    "\n",
    "Where $w_i$ represents weights, $b$ is bias, and $\\sigma$ is a non-linear activation function.\n",
    "\n",
    "## Binary Logistic Regression Unit as a Neuron\n",
    "\n",
    "A binary logistic regression unit implements a mapping $f: \\mathbb{R}^n \\rightarrow [0,1]$ that models the conditional probability:\n",
    "\n",
    "$$P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "Where $\\sigma$ is the logistic function:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This directly parallels a biological neuron where:\n",
    "- Input features $\\mathbf{x}$ correspond to dendritic inputs\n",
    "- Weights $\\mathbf{w}$ correspond to synaptic strengths\n",
    "- Bias $b$ corresponds to activation threshold\n",
    "- Sigmoid function $\\sigma$ corresponds to firing rate response\n",
    "\n",
    "The decision boundary is defined by:\n",
    "\n",
    "$$\\mathbf{w}^T\\mathbf{x} + b = 0$$\n",
    "\n",
    "Creating a hyperplane in the feature space that separates the two classes.\n",
    "\n",
    "## Neural Network as Multiple Logistic Regressions\n",
    "\n",
    "A neural network extends this concept by implementing multiple logistic regression units running simultaneously with interconnections. For a network with $L$ layers, each layer $l$ computes:\n",
    "\n",
    "$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "$$\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{a}^{[l-1]}$ is the activation from the previous layer\n",
    "- $\\mathbf{W}^{[l]}$ is the weight matrix for layer $l$\n",
    "- $\\mathbf{b}^{[l]}$ is the bias vector for layer $l$\n",
    "- $\\sigma^{[l]}$ is the activation function for layer $l$\n",
    "\n",
    "The composite function represented by the entire network is:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sigma^{[L]}(\\mathbf{W}^{[L]}\\sigma^{[L-1]}(...\\sigma^{[1]}(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]})...) + \\mathbf{b}^{[L]})$$\n",
    "\n",
    "Each unit effectively performs logistic regression, but their interconnected nature enables the modeling of complex, non-linear relationships.\n",
    "\n",
    "## Matrix Notation for a Layer\n",
    "\n",
    "For a layer with $n^{[l-1]}$ input units and $n^{[l]}$ output units, the computation can be efficiently expressed in matrix form:\n",
    "\n",
    "$$\\mathbf{Z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{A}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "$$\\mathbf{A}^{[l]} = \\sigma^{[l]}(\\mathbf{Z}^{[l]})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times n^{[l-1]}}$ is the weight matrix\n",
    "- $\\mathbf{A}^{[l-1]} \\in \\mathbb{R}^{n^{[l-1]} \\times m}$ contains activations for $m$ samples\n",
    "- $\\mathbf{b}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times 1}$ is the bias vector\n",
    "- $\\mathbf{Z}^{[l]}$ is the pre-activation output\n",
    "\n",
    "For a single sample $\\mathbf{x}^{(i)}$, the computation becomes:\n",
    "\n",
    "$$\\mathbf{z}^{[l](i)} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1](i)} + \\mathbf{b}^{[l]}$$\n",
    "\n",
    "This matrix formulation enables vectorization, critical for efficient computation on modern hardware architectures.\n",
    "\n",
    "## Non-linearities: Mathematical Necessity\n",
    "\n",
    "Non-linear activation functions are mathematically essential in neural networks. Consider a network with linear activations:\n",
    "\n",
    "$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "$$\\mathbf{a}^{[l]} = \\mathbf{z}^{[l]}$$\n",
    "\n",
    "For a two-layer network:\n",
    "$$\\mathbf{a}^{[2]} = \\mathbf{W}^{[2]}(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]}) + \\mathbf{b}^{[2]}$$\n",
    "$$= \\mathbf{W}^{[2]}\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{W}^{[2]}\\mathbf{b}^{[1]} + \\mathbf{b}^{[2]}$$\n",
    "$$= \\mathbf{W}'\\mathbf{x} + \\mathbf{b}'$$\n",
    "\n",
    "Where $\\mathbf{W}' = \\mathbf{W}^{[2]}\\mathbf{W}^{[1]}$ and $\\mathbf{b}' = \\mathbf{W}^{[2]}\\mathbf{b}^{[1]} + \\mathbf{b}^{[2]}$\n",
    "\n",
    "This demonstrates that multiple linear layers collapse mathematically into a single linear transformation, severely limiting modeling capacity. By introducing non-linearities $\\sigma$:\n",
    "\n",
    "$$\\mathbf{a}^{[1]} = \\sigma(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]})$$\n",
    "$$\\mathbf{a}^{[2]} = \\mathbf{W}^{[2]}\\mathbf{a}^{[1]} + \\mathbf{b}^{[2]} = \\mathbf{W}^{[2]}\\sigma(\\mathbf{W}^{[1]}\\mathbf{x} + \\mathbf{b}^{[1]}) + \\mathbf{b}^{[2]}$$\n",
    "\n",
    "This enables the network to model non-linear relationships and approximate arbitrary continuous functions per the Universal Approximation Theorem.\n",
    "\n",
    "Common non-linearities include:\n",
    "- Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- Hyperbolic tangent: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "- ReLU: $\\text{ReLU}(z) = \\max(0, z)$\n",
    "\n",
    "Each introduces different properties regarding gradient flow, computational efficiency, and representational capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTsy4d_pzOX1"
   },
   "source": [
    "# Gradients, Jacobian Matrices, and Backpropagation in Neural Networks\n",
    "\n",
    "## Gradients\n",
    "\n",
    "The gradient represents the multi-dimensional generalization of the derivative for scalar-valued functions of multiple variables. For a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, the gradient $\\nabla f$ is defined as the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(\\mathbf{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}(\\mathbf{x})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The gradient has fundamental mathematical properties:\n",
    "1. It points in the direction of steepest ascent of $f$\n",
    "2. The magnitude $\\|\\nabla f(\\mathbf{x})\\|$ indicates the rate of change in that direction\n",
    "3. For any unit vector $\\mathbf{u}$, the directional derivative is given by $\\nabla f(\\mathbf{x}) \\cdot \\mathbf{u}$\n",
    "\n",
    "In optimization applications, we update parameters iteratively using:\n",
    "\n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha \\nabla f(\\mathbf{x}_t)$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "## Jacobian Matrix: Generalization of the Gradient\n",
    "\n",
    "The Jacobian matrix extends the gradient concept to vector-valued functions. For a function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with component functions $f_1, f_2, \\ldots, f_m$, the Jacobian $\\mathbf{J}_\\mathbf{f}$ is defined as:\n",
    "\n",
    "$$\\mathbf{J}_\\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}(\\mathbf{x}) & \\frac{\\partial f_1}{\\partial x_2}(\\mathbf{x}) & \\cdots & \\frac{\\partial f_1}{\\partial x_n}(\\mathbf{x}) \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1}(\\mathbf{x}) & \\frac{\\partial f_2}{\\partial x_2}(\\mathbf{x}) & \\cdots & \\frac{\\partial f_2}{\\partial x_n}(\\mathbf{x}) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1}(\\mathbf{x}) & \\frac{\\partial f_m}{\\partial x_2}(\\mathbf{x}) & \\cdots & \\frac{\\partial f_m}{\\partial x_n}(\\mathbf{x})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The Jacobian $\\mathbf{J}_\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^{m \\times n}$ represents the best linear approximation of $\\mathbf{f}$ near $\\mathbf{x}$:\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x} + \\mathbf{h}) \\approx \\mathbf{f}(\\mathbf{x}) + \\mathbf{J}_\\mathbf{f}(\\mathbf{x})\\mathbf{h}$$\n",
    "\n",
    "When $m = 1$, the Jacobian reduces to the gradient (transposed):\n",
    "\n",
    "$$\\mathbf{J}_f(\\mathbf{x}) = \\nabla f(\\mathbf{x})^T$$\n",
    "\n",
    "## Chain Rule\n",
    "\n",
    "The chain rule enables the computation of derivatives for composite functions. For scalar-valued functions, if $y = g(u)$ and $u = h(x)$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "For vector-valued functions, given $\\mathbf{y} = \\mathbf{g}(\\mathbf{u})$ and $\\mathbf{u} = \\mathbf{h}(\\mathbf{x})$, the chain rule becomes:\n",
    "\n",
    "$$\\mathbf{J}_{\\mathbf{g} \\circ \\mathbf{h}}(\\mathbf{x}) = \\mathbf{J}_\\mathbf{g}(\\mathbf{h}(\\mathbf{x})) \\cdot \\mathbf{J}_\\mathbf{h}(\\mathbf{x})$$\n",
    "\n",
    "Mathematically, if $\\mathbf{g}: \\mathbb{R}^p \\rightarrow \\mathbb{R}^m$ and $\\mathbf{h}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^p$, then the Jacobian matrix of their composition has dimensions $\\mathbb{R}^{m \\times n}$ and is computed through matrix multiplication of Jacobians.\n",
    "\n",
    "## Example Jacobian: Elementwise Activation Function\n",
    "\n",
    "Consider an elementwise activation function $\\sigma: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ where each component $\\sigma_i(z) = \\sigma(z_i)$. The Jacobian matrix has a diagonal structure:\n",
    "\n",
    "$$\\mathbf{J}_\\sigma(\\mathbf{z}) = \\begin{bmatrix}\n",
    "\\sigma'(z_1) & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma'(z_2) & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sigma'(z_n)\n",
    "\\end{bmatrix} = \\text{diag}(\\sigma'(z_1), \\sigma'(z_2), \\ldots, \\sigma'(z_n))$$\n",
    "\n",
    "For specific activation functions:\n",
    "\n",
    "1. Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "   $$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "2. ReLU: $\\sigma(z) = \\max(0, z)$\n",
    "   $$\\sigma'(z) = \\begin{cases}\n",
    "   1 & \\text{if } z > 0 \\\\\n",
    "   0 & \\text{if } z \\leq 0\n",
    "   \\end{cases}$$\n",
    "\n",
    "3. Tanh: $\\sigma(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "   $$\\sigma'(z) = 1 - \\tanh^2(z)$$\n",
    "\n",
    "## Other Jacobians\n",
    "\n",
    "1. **Linear Transformation**: For $\\mathbf{f}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$ where $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$:\n",
    "   $$\\mathbf{J}_\\mathbf{f}(\\mathbf{x}) = \\mathbf{W}$$\n",
    "\n",
    "2. **Matrix-Vector Product**: For $\\mathbf{f}(\\mathbf{W}) = \\mathbf{W}\\mathbf{x}$ with fixed $\\mathbf{x}$:\n",
    "   $$\\frac{\\partial (\\mathbf{W}\\mathbf{x})_i}{\\partial W_{jk}} = \\begin{cases}\n",
    "   x_k & \\text{if } i = j \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}$$\n",
    "\n",
    "3. **Element-wise Operations**: For $\\mathbf{f}(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\odot \\mathbf{y}$ (Hadamard product):\n",
    "   $$\\frac{\\partial f_i}{\\partial x_j} = \\begin{cases}\n",
    "   y_i & \\text{if } i = j \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}$$\n",
    "\n",
    "## Back to our Neural Net!\n",
    "\n",
    "In a neural network, the forward pass for layer $l$ typically computes:\n",
    "$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "$$\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "### 1. Breaking up equations into simple pieces\n",
    "\n",
    "We decompose these operations:\n",
    "- Linear transformation: $\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$\n",
    "- Non-linear activation: $\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})$\n",
    "\n",
    "### 2. Applying the chain rule\n",
    "\n",
    "Consider a loss function $L$ dependent on the network output. To compute $\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}}$, we apply the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} \\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{W}^{[l]}}$$\n",
    "\n",
    "For multiple layers, the recursion expands:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\frac{\\partial \\mathbf{a}^{[l]}}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot \\sigma'^{[l]}(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{a}^{[l-1]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} \\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{a}^{[l-1]}} = (\\mathbf{W}^{[l]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$\n",
    "\n",
    "### 3. Writing out the Jacobians\n",
    "\n",
    "For an element-wise activation function:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{a}^{[l]}}{\\partial \\mathbf{z}^{[l]}} = \\text{diag}(\\sigma'^{[l]}(\\mathbf{z}^{[l]}_1), \\sigma'^{[l]}(\\mathbf{z}^{[l]}_2), \\ldots, \\sigma'^{[l]}(\\mathbf{z}^{[l]}_n))$$\n",
    "\n",
    "For a linear transformation:\n",
    "$$\\frac{\\partial \\mathbf{z}^{[l]}}{\\partial \\mathbf{W}^{[l]}_{ij}} = \\begin{cases}\n",
    "a^{[l-1]}_j & \\text{for element } z^{[l]}_i \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "## Re-using Computation\n",
    "\n",
    "During backpropagation, we can reuse computations from the forward pass. Define $\\boldsymbol{\\delta}^{[l]} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$. Then:\n",
    "\n",
    "$$\\boldsymbol{\\delta}^{[l]} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot \\sigma'^{[l]}(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "$$\\boldsymbol{\\delta}^{[l-1]} = (\\mathbf{W}^{[l]})^T \\boldsymbol{\\delta}^{[l]} \\odot \\sigma'^{[l-1]}(\\mathbf{z}^{[l-1]})$$\n",
    "\n",
    "The gradient with respect to weights becomes:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\boldsymbol{\\delta}^{[l]} (\\mathbf{a}^{[l-1]})^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\boldsymbol{\\delta}^{[l]}$$\n",
    "\n",
    "## Derivative with respect to Matrix: Output shape\n",
    "\n",
    "For a scalar function $L$ with respect to a matrix $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, the derivative $\\frac{\\partial L}{\\partial \\mathbf{W}}$ has the same dimensions $\\mathbb{R}^{m \\times n}$. Specifically:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_{11}} & \\frac{\\partial L}{\\partial W_{12}} & \\cdots & \\frac{\\partial L}{\\partial W_{1n}} \\\\\n",
    "\\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{22}} & \\cdots & \\frac{\\partial L}{\\partial W_{2n}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial W_{m1}} & \\frac{\\partial L}{\\partial W_{m2}} & \\cdots & \\frac{\\partial L}{\\partial W_{mn}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## Deriving local input gradient in backprop\n",
    "\n",
    "The local input gradient for layer $l$ computes how the loss changes with respect to the input of that layer. For input $\\mathbf{a}^{[l-1]}$ to layer $l$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{a}^{[l-1]}} = (\\mathbf{W}^{[l]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = (\\mathbf{W}^{[l]})^T \\boldsymbol{\\delta}^{[l]}$$\n",
    "\n",
    "This expression quantifies how changes in the activations of layer $l-1$ affect the overall loss, forming the critical recursive relationship that enables efficient backpropagation through the network.\n",
    "\n",
    "The complete backpropagation algorithm is therefore:\n",
    "\n",
    "1. Perform forward pass to compute all $\\mathbf{z}^{[l]}$ and $\\mathbf{a}^{[l]}$\n",
    "2. Compute output layer error: $\\boldsymbol{\\delta}^{[L]} = \\nabla_{\\mathbf{a}^{[L]}}L \\odot \\sigma'^{[L]}(\\mathbf{z}^{[L]})$\n",
    "3. Backpropagate error: $\\boldsymbol{\\delta}^{[l-1]} = (\\mathbf{W}^{[l]})^T \\boldsymbol{\\delta}^{[l]} \\odot \\sigma'^{[l-1]}(\\mathbf{z}^{[l-1]})$\n",
    "4. Compute gradients: $\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\boldsymbol{\\delta}^{[l]} (\\mathbf{a}^{[l-1]})^T$, $\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\boldsymbol{\\delta}^{[l]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BcqMH0Y0knm"
   },
   "source": [
    "# Backpropagation and Computation Graphs: Mathematical Foundations\n",
    "\n",
    "##  Backpropagation\n",
    "\n",
    "Backpropagation is an efficient algorithm for computing gradients in parameterized computational models through recursive application of the chain rule of differentiation. Formally, given a scalar loss function $L: \\mathbb{R}^m \\rightarrow \\mathbb{R}$ that depends on the output of a composite function $f(\\mathbf{x};\\boldsymbol{\\theta})$ with parameters $\\boldsymbol{\\theta}$, backpropagation computes $\\nabla_{\\boldsymbol{\\theta}}L$ with computational complexity proportional to the forward evaluation of $f$.\n",
    "\n",
    "The mathematical foundation of backpropagation derives from the chain rule for computing derivatives of composite functions. For scalar functions, if $y = g(h(x))$, then:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dh} \\cdot \\frac{dh}{dx}$$\n",
    "\n",
    "This generalizes to vector-valued functions through the Jacobian formulation:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$$\n",
    "\n",
    "Where $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ is the Jacobian matrix $\\mathbf{J}$ with elements $J_{ij} = \\frac{\\partial y_i}{\\partial x_j}$.\n",
    "\n",
    "## Computation Graphs and Backpropagation\n",
    "\n",
    "A computation graph $G = (V, E)$ is a directed acyclic graph (DAG) where:\n",
    "- Vertices $v \\in V$ represent variables or operations\n",
    "- Edges $(u, v) \\in E$ represent dependencies between variables\n",
    "- Input nodes have in-degree zero\n",
    "- Output nodes produce the final computation result\n",
    "- Intermediate nodes represent operations or transformations\n",
    "\n",
    "Each node $v_i$ computes a function $f_i$ of its inputs:\n",
    "\n",
    "$$v_i = f_i(\\text{Parents}(v_i))$$\n",
    "\n",
    "Mathematically, the computation graph encodes the decomposition of a complex function into primitive operations, enabling the systematic application of the chain rule.\n",
    "\n",
    "### 1. Fprop: Visit Nodes in Topological Sort Order\n",
    "\n",
    "Forward propagation traverses the graph in topological order, ensuring all inputs to a node are computed before the node itself:\n",
    "\n",
    "$$v_i = f_i(v_{j_1}, v_{j_2}, ..., v_{j_k})$$\n",
    "\n",
    "where $v_{j_1}, v_{j_2}, ..., v_{j_k}$ are the parent nodes of $v_i$.\n",
    "\n",
    "The topological ordering $\\pi$ satisfies the property that for every edge $(v_i, v_j) \\in E$, $\\pi(v_i) < \\pi(v_j)$, guaranteeing that all dependencies are resolved before computation.\n",
    "\n",
    "For a node representing a primitive operation $v_i = f_i(v_{j_1}, v_{j_2}, ..., v_{j_k})$, we compute and store:\n",
    "1. The output value $v_i$\n",
    "2. Additional information required for gradient computation (intermediate values)\n",
    "\n",
    "The forward pass has computational complexity $O(|E|)$ where $|E|$ is the number of edges in the graph.\n",
    "\n",
    "### 2. Bprop: Backward Gradient Computation\n",
    "\n",
    "Backward propagation computes gradients by applying the chain rule recursively through the graph in reverse topological order:\n",
    "\n",
    "1. Initialize output gradient $\\frac{\\partial L}{\\partial v_{\\text{output}}} = 1$ for the output node\n",
    "2. For each node $v_i$ in reverse topological order:\n",
    "   - Compute gradient with respect to each input $v_j$ using:\n",
    "   \n",
    "   $$\\frac{\\partial L}{\\partial v_j} += \\frac{\\partial L}{\\partial v_i} \\cdot \\frac{\\partial v_i}{\\partial v_j}$$\n",
    "   \n",
    "   - The += operator indicates accumulation of gradients when a node affects multiple downstream computations\n",
    "\n",
    "The mathematical justification follows from the multivariate chain rule. For a node $v_j$ that influences multiple nodes $v_{i_1}, v_{i_2}, ..., v_{i_m}$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial v_j} = \\sum_{k=1}^{m} \\frac{\\partial L}{\\partial v_{i_k}} \\cdot \\frac{\\partial v_{i_k}}{\\partial v_j}$$\n",
    "\n",
    "Each local derivative $\\frac{\\partial v_i}{\\partial v_j}$ depends on the specific operation at node $v_i$. For common operations:\n",
    "\n",
    "1. Addition $(v_i = v_j + v_k)$: $\\frac{\\partial v_i}{\\partial v_j} = 1$\n",
    "2. Multiplication $(v_i = v_j \\cdot v_k)$: $\\frac{\\partial v_i}{\\partial v_j} = v_k$\n",
    "3. Function application $(v_i = f(v_j))$: $\\frac{\\partial v_i}{\\partial v_j} = f'(v_j)$\n",
    "\n",
    "The backward pass systematically computes all required partial derivatives, eventually yielding $\\frac{\\partial L}{\\partial \\theta_i}$ for each parameter $\\theta_i$ in the model.\n",
    "\n",
    "When implemented correctly, the backpropagation algorithm has the same asymptotic complexity as forward propagation, specifically $O(|E|)$. This equivalence derives from the chain rule structure: each edge in the computation graph corresponds to exactly one multiplication and addition operation during the backward pass.\n",
    "\n",
    "For neural networks with regular layer structures, the computation graph exhibits specific patterns that enable efficient matrix-based implementations. Consider a neural network layer:\n",
    "\n",
    "$$\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$$\n",
    "$$\\mathbf{a}^{[l]} = \\sigma(\\mathbf{z}^{[l]})$$\n",
    "\n",
    "In matrix notation, the gradient computation becomes:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{a}^{[l]}} \\odot \\sigma'(\\mathbf{z}^{[l]})$$\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{a}^{[l-1]}} = (\\mathbf{W}^{[l]})^T \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}} (\\mathbf{a}^{[l-1]})^T$$\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}^{[l]}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{[l]}}$$\n",
    "\n",
    "Here, $\\odot$ denotes the Hadamard (element-wise) product, reflecting the element-wise application of the activation function derivative.\n",
    "\n",
    "The Jacobian matrices for each layer transformation formalize these operations:\n",
    "\n",
    "1. For the affine transformation: $\\mathbf{J}_{\\mathbf{W}, \\mathbf{a}} = \\mathbf{W}$\n",
    "2. For the element-wise activation: $\\mathbf{J}_{\\sigma} = \\text{diag}(\\sigma'(\\mathbf{z}))$\n",
    "\n",
    "Backpropagation through a neural network sequentially applies these Jacobian operations in reverse order, propagating error gradients from the output layer back to the input layer and computing parameter gradients along the way.\n",
    "\n",
    "The effectiveness of backpropagation derives from its computational efficiency, requiring only one forward and one backward pass through the computation graph to compute gradients for all parameters simultaneously. This efficiency has made deep learning computationally feasible on large-scale problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rlbLx5B9oe1"
   },
   "source": [
    "# Deep Learning Technical Analysis: Parameters, Regularization and Optimization\n",
    "\n",
    "## Models with Many Parameters and Regularization\n",
    "\n",
    "Modern neural networks operate with millions or billions of parameters, creating systems capable of extraordinary expressivity but vulnerable to overfitting. Mathematically, a model $f_\\theta(x)$ parameterized by vector $\\theta \\in \\mathbb{R}^d$ becomes overparameterized when $d \\gg n$, where $n$ represents training samples.\n",
    "\n",
    "The optimization objective without regularization is:\n",
    "\n",
    "$$ \\min_\\theta \\frac{1}{n}\\sum_{i=1}^{n}L(f_\\theta(x_i), y_i) $$\n",
    "\n",
    "Regularization addresses overfitting by constraining parameter values. The regularized objective becomes:\n",
    "\n",
    "$$ \\min_\\theta \\frac{1}{n}\\sum_{i=1}^{n}L(f_\\theta(x_i), y_i) + \\lambda R(\\theta) $$\n",
    "\n",
    "Where $\\lambda$ controls regularization strength and $R(\\theta)$ is the regularization function.\n",
    "\n",
    "L2 regularization (weight decay) penalizes large weights using squared magnitudes:\n",
    "\n",
    "$$ R_{L2}(\\theta) = \\frac{1}{2}\\|\\theta\\|_2^2 = \\frac{1}{2}\\sum_{j=1}^{d}\\theta_j^2 $$\n",
    "\n",
    "L1 regularization induces sparsity by penalizing absolute weight values:\n",
    "\n",
    "$$ R_{L1}(\\theta) = \\|\\theta\\|_1 = \\sum_{j=1}^{d}|\\theta_j| $$\n",
    "\n",
    "The gradient update with L2 regularization becomes:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta\\left(\\nabla_\\theta L(f_\\theta(x), y) + \\lambda\\theta_t\\right) = (1-\\eta\\lambda)\\theta_t - \\eta\\nabla_\\theta L(f_\\theta(x), y) $$\n",
    "\n",
    "This effectively shrinks weights by factor $(1-\\eta\\lambda)$ in each iteration.\n",
    "\n",
    "Mathematically, regularization modifies the loss landscape, eliminating sharp minima that generalize poorly and favoring flatter ones that generalize better under distribution shift, expressed as:\n",
    "\n",
    "$$ \\mathbb{E}_{x\\sim\\mathcal{D}_{test}}[L(f_\\theta(x), y)] \\leq \\mathbb{E}_{x\\sim\\mathcal{D}_{train}}[L(f_\\theta(x), y)] + \\mathcal{C}(\\theta, n) $$\n",
    "\n",
    "Where $\\mathcal{C}(\\theta, n)$ is the complexity term regularization minimizes.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout implements stochastic regularization through temporary neuron deactivation during training. For each forward pass, neurons are retained with probability $p$ and dropped with probability $(1-p)$.\n",
    "\n",
    "Mathematically, given layer output $\\mathbf{y}$, dropout applies:\n",
    "\n",
    "$$ \\mathbf{r} \\sim \\text{Bernoulli}(p) $$\n",
    "$$ \\tilde{\\mathbf{y}} = \\mathbf{r} \\odot \\mathbf{y} $$\n",
    "\n",
    "Where $\\odot$ denotes element-wise multiplication and $\\mathbf{r}$ is a binary mask. During inference, the expected output is approximated by scaling:\n",
    "\n",
    "$$ \\mathbb{E}[\\tilde{\\mathbf{y}}] = p\\mathbf{y} $$\n",
    "\n",
    "To maintain consistent expected values between training and inference, we either scale during training:\n",
    "\n",
    "$$ \\tilde{\\mathbf{y}}_{train} = \\frac{\\mathbf{r} \\odot \\mathbf{y}}{p} $$\n",
    "\n",
    "Or during inference (inverted dropout):\n",
    "\n",
    "$$ \\tilde{\\mathbf{y}}_{inference} = p\\mathbf{y} $$\n",
    "\n",
    "Dropout implements an implicit ensemble averaging of $2^N$ different \"thinned\" networks, where $N$ is the number of neurons. This provides Bayesian approximation properties, with the dropout probability governing the posterior distribution width.\n",
    "\n",
    "The dropout effect can be interpreted as adaptive L2 regularization:\n",
    "\n",
    "$$ \\mathbb{E}_{\\mathbf{r}}[L(f_{\\theta,\\mathbf{r}}(x), y)] \\approx L(f_\\theta(x), y) + \\lambda \\sum_{l} \\frac{p}{1-p}\\|\\mathbf{W}_l\\|_F^2 $$\n",
    "\n",
    "Where $\\mathbf{W}_l$ represents weights in layer $l$ and $\\|\\cdot\\|_F$ is the Frobenius norm.\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "Vectorization transforms scalar operations into equivalent vector/matrix operations, enabling parallel computation exploitation. Given inputs $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ containing $n$ samples with $d$ features, the forward propagation in a layer is expressed as:\n",
    "\n",
    "$$ \\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b} $$\n",
    "$$ \\mathbf{A} = \\sigma(\\mathbf{Z}) $$\n",
    "\n",
    "Where $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ contains weights, $\\mathbf{b} \\in \\mathbb{R}^m$ is the bias, and $\\sigma$ is applied element-wise.\n",
    "\n",
    "Computational complexity analysis shows vectorized operations achieve $O(ndm)$ complexity versus $O(n \\cdot d \\cdot m)$ for loops, with the constant factor significantly reduced through SIMD (Single Instruction Multiple Data) operations.\n",
    "\n",
    "Matrix calculus facilitates efficient gradient computation:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\mathbf{W}} = \\mathbf{X}^T \\frac{\\partial L}{\\partial \\mathbf{Z}} $$\n",
    "$$ \\frac{\\partial L}{\\partial \\mathbf{b}} = \\mathbf{1}^T \\frac{\\partial L}{\\partial \\mathbf{Z}} $$\n",
    "$$ \\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{Z}} \\mathbf{W}^T $$\n",
    "\n",
    "The speedup factor from vectorization can be expressed as:\n",
    "\n",
    "$$ S = \\frac{T_{loop}}{T_{vector}} \\approx \\frac{c_{loop} \\cdot ndm}{c_{vector} \\cdot ndm} = \\frac{c_{loop}}{c_{vector}} $$\n",
    "\n",
    "Where constants $c_{loop} \\gg c_{vector}$ due to memory locality, cache efficiency, and hardware optimization.\n",
    "\n",
    "## Parameter Initialization\n",
    "\n",
    "Parameter initialization critically affects convergence and model performance. For a neural network with layers $l = 1,...,L$, proper initialization ensures stable signal propagation:\n",
    "\n",
    "$$ \\text{Var}(y^l) \\approx \\text{Var}(y^{l-1}) $$\n",
    "\n",
    "Xavier/Glorot initialization for tanh/sigmoid activations draws weights from:\n",
    "\n",
    "$$ W^l_{ij} \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right) $$\n",
    "\n",
    "Where $n_{in}$ and $n_{out}$ are input and output dimensions. This maintains variance across layers:\n",
    "\n",
    "$$ \\text{Var}(y^l) = n_{in} \\cdot \\text{Var}(W^l) \\cdot \\text{Var}(y^{l-1}) \\approx \\text{Var}(y^{l-1}) $$\n",
    "\n",
    "He initialization, designed for ReLU activations, accounts for variance reduction from rectification:\n",
    "\n",
    "$$ W^l_{ij} \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right) $$\n",
    "\n",
    "Orthogonal initialization ensures weight matrices satisfy:\n",
    "\n",
    "$$ \\mathbf{W}^T\\mathbf{W} = \\mathbf{I} $$\n",
    "\n",
    "Preserving gradient magnitudes during backpropagation through:\n",
    "\n",
    "$$ \\|\\mathbf{W}^T\\delta\\|_2 = \\|\\delta\\|_2 $$\n",
    "\n",
    "Mathematically, the vanishing/exploding gradient problem occurs when:\n",
    "\n",
    "$$ \\|\\nabla_{\\theta_l}L\\| = \\|\\nabla_{\\mathbf{y}^L}L \\cdot \\prod_{i=l+1}^{L} \\frac{\\partial \\mathbf{y}^i}{\\partial \\mathbf{y}^{i-1}} \\cdot \\frac{\\partial \\mathbf{y}^l}{\\partial \\theta_l}\\| $$\n",
    "\n",
    "Grows or diminishes exponentially with network depth when eigenvalues of Jacobians $\\frac{\\partial \\mathbf{y}^i}{\\partial \\mathbf{y}^{i-1}}$ deviate significantly from 1.\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Neural network training involves minimizing the objective:\n",
    "\n",
    "$$ \\min_\\theta \\mathcal{L}(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}L(f_\\theta(x_i), y_i) + \\lambda R(\\theta) $$\n",
    "\n",
    "Vanilla Gradient Descent updates parameters through:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n",
    "\n",
    "Stochastic Gradient Descent approximates full gradient using mini-batches:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L}_B(\\theta_t) $$\n",
    "\n",
    "Where $\\mathcal{L}_B$ represents the loss on mini-batch $B$.\n",
    "\n",
    "Momentum incorporates previous update directions:\n",
    "\n",
    "$$ v_{t+1} = \\gamma v_t + \\eta \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n",
    "$$ \\theta_{t+1} = \\theta_t - v_{t+1} $$\n",
    "\n",
    "With theoretical convergence rate $O(1/t)$ for convex problems, improved to $O(1/t^2)$ with Nesterov acceleration:\n",
    "\n",
    "$$ v_{t+1} = \\gamma v_t + \\eta \\nabla_\\theta \\mathcal{L}(\\theta_t - \\gamma v_t) $$\n",
    "$$ \\theta_{t+1} = \\theta_t - v_{t+1} $$\n",
    "\n",
    "Adaptive methods adjust learning rates per-parameter. AdaGrad accumulates squared gradients:\n",
    "\n",
    "$$ G_{t+1} = G_t + (\\nabla_\\theta \\mathcal{L}(\\theta_t))^2 $$\n",
    "$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_{t+1} + \\epsilon}} \\odot \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n",
    "\n",
    "RMSProp uses exponential moving average for squared gradients:\n",
    "\n",
    "$$ G_{t+1} = \\beta G_t + (1-\\beta)(\\nabla_\\theta \\mathcal{L}(\\theta_t))^2 $$\n",
    "$$ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_{t+1} + \\epsilon}} \\odot \\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n",
    "\n",
    "Adam combines momentum and adaptive learning rates:\n",
    "\n",
    "$$ m_{t+1} = \\beta_1 m_t + (1-\\beta_1)\\nabla_\\theta \\mathcal{L}(\\theta_t) $$\n",
    "$$ v_{t+1} = \\beta_2 v_t + (1-\\beta_2)(\\nabla_\\theta \\mathcal{L}(\\theta_t))^2 $$\n",
    "$$ \\hat{m}_{t+1} = \\frac{m_{t+1}}{1-\\beta_1^{t+1}} $$\n",
    "$$ \\hat{v}_{t+1} = \\frac{v_{t+1}}{1-\\beta_2^{t+1}} $$\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon} $$\n",
    "\n",
    "Convergence analysis shows Adam achieves regret bound $O(\\sqrt{T})$ for convex problems and empirically navigates non-convex landscapes efficiently due to adaptive step sizes managing varying gradient magnitudes across parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taiww5E2vpZN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne4NpNuWtlCz"
   },
   "source": [
    "# Linear Layers in Neural Networks\n",
    "\n",
    "## Linear Layers: Fundamentals\n",
    "\n",
    "### Definition\n",
    "Linear layers are fundamental building blocks in neural networks that perform affine transformations on input data, mapping from an input space to an output space through learnable parameters.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For an input vector $\\mathbf{x} \\in \\mathbb{R}^{n}$, a linear layer transforms it to output $\\mathbf{y} \\in \\mathbb{R}^{m}$ using:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ is the weight matrix\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{m}$ is the bias vector\n",
    "\n",
    "### Computational Properties\n",
    "- Forward pass complexity: $O(m \\times n)$\n",
    "- Backward pass complexity: $O(m \\times n)$\n",
    "- Parameter count: $m \\times n + m$\n",
    "- Gradient computation:\n",
    "  $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\mathbf{x}^T$$\n",
    "  $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}$$\n",
    "  $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} = \\mathbf{W}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}$$\n",
    "\n",
    "## Identity Layer\n",
    "\n",
    "### Definition\n",
    "An Identity layer is a transformation that returns its input unchanged, serving as a pass-through function in neural networks.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For an input vector $\\mathbf{x} \\in \\mathbb{R}^{n}$, the identity transformation is:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x}$$\n",
    "\n",
    "Equivalently, it can be represented as multiplication by the identity matrix:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{I}\\mathbf{x}$$\n",
    "\n",
    "Where $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ is the identity matrix with diagonal elements set to 1 and all others to 0.\n",
    "\n",
    "### Implementation Details\n",
    "- Contains no learnable parameters\n",
    "- Gradient flow: $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}$\n",
    "- Memory footprint: $O(1)$ (constant)\n",
    "\n",
    "### Applications\n",
    "- Skip connections in residual networks\n",
    "- Placeholder modules for architecture search\n",
    "- Network pruning without changing architecture topology\n",
    "- Enabling module swapping during experimentation\n",
    "\n",
    "## Linear Layer\n",
    "\n",
    "### Definition\n",
    "A standard Linear layer performs a full affine transformation on input data with explicitly defined input and output dimensions using learnable weights and biases.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a batched input tensor $\\mathbf{X} \\in \\mathbb{R}^{b \\times n}$ with batch size $b$:\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\mathbf{W}^T + \\mathbf{b}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ is the weight matrix\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{m}$ is the bias vector\n",
    "- $\\mathbf{Y} \\in \\mathbb{R}^{b \\times m}$ is the output\n",
    "\n",
    "### Implementation Details\n",
    "- Requires explicit specification of both input and output dimensions\n",
    "- Weight initialization methods:\n",
    "  - Kaiming/He: $$\\mathcal{W} \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in}}})$$\n",
    "  - Xavier/Glorot: $$\\mathcal{W} \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}})$$\n",
    "- Bias typically initialized to zeros\n",
    "\n",
    "### Gradient Computation\n",
    "- For weights: $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}^T \\mathbf{X}$$\n",
    "- For bias: $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\sum_{i=1}^{b}\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}_i}$$\n",
    "- For input: $$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}\\mathbf{W}$$\n",
    "\n",
    "## Bilinear Layer\n",
    "\n",
    "### Definition\n",
    "A Bilinear layer models multiplicative interactions between two input vectors through a 3D tensor of weights, capturing pairwise feature interactions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input vectors $\\mathbf{x}_1 \\in \\mathbb{R}^{n_1}$ and $\\mathbf{x}_2 \\in \\mathbb{R}^{n_2}$, the bilinear transformation produces output $\\mathbf{y} \\in \\mathbb{R}^{m}$:\n",
    "\n",
    "$$y_k = \\mathbf{x}_1^T \\mathbf{W}_k \\mathbf{x}_2 + b_k = \\sum_{i=1}^{n_1}\\sum_{j=1}^{n_2} W_{ijk} x_{1i} x_{2j} + b_k$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{n_1 \\times n_2 \\times m}$ is the weight tensor\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{m}$ is the bias vector\n",
    "- $k \\in \\{1,2,\\ldots,m\\}$ indexes the output dimension\n",
    "\n",
    "### Parameter Efficiency\n",
    "- Total parameters: $n_1 \\times n_2 \\times m + m$\n",
    "- Computational complexity: $O(n_1 \\times n_2 \\times m)$\n",
    "\n",
    "### Gradient Computation\n",
    "- For first input: $$\\frac{\\partial y_k}{\\partial x_{1i}} = \\sum_{j=1}^{n_2} W_{ijk} x_{2j}$$\n",
    "- For second input: $$\\frac{\\partial y_k}{\\partial x_{2j}} = \\sum_{i=1}^{n_1} W_{ijk} x_{1i}$$\n",
    "- For weights: $$\\frac{\\partial y_k}{\\partial W_{ijk}} = x_{1i} x_{2j}$$\n",
    "\n",
    "### Applications\n",
    "- Multimodal feature fusion\n",
    "- Visual-question answering systems\n",
    "- Fine-grained classification\n",
    "- Quadratic feature interactions\n",
    "- Attention mechanisms\n",
    "\n",
    "## LazyLinear Layer\n",
    "\n",
    "### Definition\n",
    "LazyLinear is a variant of the standard linear layer that automatically infers input dimensions at runtime, deferring weight initialization until the first forward pass.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Once initialized with the first input, LazyLinear performs the standard linear transformation:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "The distinguishing feature is that $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ is dynamically created when the first input $\\mathbf{x} \\in \\mathbb{R}^{n}$ passes through the layer.\n",
    "\n",
    "### Initialization Process\n",
    "1. Layer created with only output dimension $m$ specified\n",
    "2. First forward pass receives input with shape $(*, n)$\n",
    "3. Weight matrix dynamically initialized with shape $(m, n)$\n",
    "4. Bias vector initialized with shape $(m)$\n",
    "\n",
    "### Implementation Advantages\n",
    "- Enables more flexible architecture design\n",
    "- Reduces boilerplate code when input dimensions depend on previous layers\n",
    "- Simplifies dynamic neural network creation\n",
    "- Maintains compatibility with standard optimization techniques\n",
    "\n",
    "### Applications\n",
    "- Dynamic neural architectures\n",
    "- Transfer learning scenarios with variable input dimensions\n",
    "- Network architecture search\n",
    "- Models with variable input specifications\n",
    "- Rapid prototyping of neural network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7UwyCkK0H_z"
   },
   "source": [
    "# Convolution Layers in Deep Neural Networks\n",
    "\n",
    "## Convolution Operation: Fundamentals\n",
    "\n",
    "### Definition\n",
    "Convolution is a mathematical operation that combines two functions to produce a third function expressing how the shape of one is modified by the other. In deep learning, convolution filters slide across input data to extract features.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The discrete convolution between input signal $f$ and kernel $g$ is defined as:\n",
    "\n",
    "$$f * g[n] = \\sum_{m=-\\infty}^{\\infty} f[m]g[n-m]$$\n",
    "\n",
    "In practice, deep learning implements cross-correlation:\n",
    "\n",
    "$$f \\star g[n] = \\sum_{m=-\\infty}^{\\infty} f[m]g[n+m]$$\n",
    "\n",
    "## Conv1d\n",
    "\n",
    "### Definition\n",
    "Conv1d applies a 1D convolution over input signal sequences with multiple channels.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input $x$ of shape $(N, C_{in}, L_{in})$ and filter $w$ of shape $(C_{out}, C_{in}, L_f)$, the output $y$ of shape $(N, C_{out}, L_{out})$ is:\n",
    "\n",
    "$$y[n, c_{out}, l] = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{k=0}^{L_f-1} x[n, c_{in}, l + k] \\cdot w[c_{out}, c_{in}, k] + b[c_{out}]$$\n",
    "\n",
    "Output length calculation:\n",
    "\n",
    "$$L_{out} = \\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation} \\times (L_f - 1) - 1}{\\text{stride}} + 1 \\rfloor$$\n",
    "\n",
    "### Key Parameters\n",
    "- `in_channels`: Number of input channels $(C_{in})$\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- `kernel_size`: Size of convolving kernel $(L_f)$\n",
    "- `stride`: Convolution stride (default: 1)\n",
    "- `padding`: Zero-padding added to input (default: 0)\n",
    "- `dilation`: Spacing between kernel elements (default: 1)\n",
    "- `groups`: Blocked connections between inputs and outputs (default: 1)\n",
    "- `bias`: Learnable bias addition (default: True)\n",
    "\n",
    "### Use Cases\n",
    "- Audio signal processing\n",
    "- Time series analysis\n",
    "- Sequence data modeling\n",
    "\n",
    "## Conv2d\n",
    "\n",
    "### Definition\n",
    "Conv2d applies a 2D convolution over input images with multiple channels.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input $x$ of shape $(N, C_{in}, H_{in}, W_{in})$ and filter $w$ of shape $(C_{out}, C_{in}, H_f, W_f)$, the output $y$ of shape $(N, C_{out}, H_{out}, W_{out})$ is:\n",
    "\n",
    "$$y[n, c_{out}, h, w] = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{k_h=0}^{H_f-1} \\sum_{k_w=0}^{W_f-1} x[n, c_{in}, h + k_h, w + k_w] \\cdot w[c_{out}, c_{in}, k_h, k_w] + b[c_{out}]$$\n",
    "\n",
    "Output dimensions:\n",
    "\n",
    "$$H_{out} = \\lfloor \\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (H_f - 1) - 1}{\\text{stride}[0]} + 1 \\rfloor$$\n",
    "\n",
    "$$W_{out} = \\lfloor \\frac{W_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (W_f - 1) - 1}{\\text{stride}[1]} + 1 \\rfloor$$\n",
    "\n",
    "### Key Parameters\n",
    "- Same as Conv1d but for 2D spatial dimensions\n",
    "- `kernel_size`: Size of convolving kernel $(H_f, W_f)$\n",
    "\n",
    "### Use Cases\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Semantic segmentation\n",
    "\n",
    "## Conv3d\n",
    "\n",
    "### Definition\n",
    "Conv3d applies a 3D convolution over input volumes with multiple channels.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input $x$ of shape $(N, C_{in}, D_{in}, H_{in}, W_{in})$ and filter $w$ of shape $(C_{out}, C_{in}, D_f, H_f, W_f)$, the output $y$ of shape $(N, C_{out}, D_{out}, H_{out}, W_{out})$ is:\n",
    "\n",
    "$$y[n, c_{out}, d, h, w] = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{k_d=0}^{D_f-1} \\sum_{k_h=0}^{H_f-1} \\sum_{k_w=0}^{W_f-1} x[n, c_{in}, d + k_d, h + k_h, w + k_w] \\cdot w[c_{out}, c_{in}, k_d, k_h, k_w] + b[c_{out}]$$\n",
    "\n",
    "Output dimensions:\n",
    "\n",
    "$$D_{out} = \\lfloor \\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (D_f - 1) - 1}{\\text{stride}[0]} + 1 \\rfloor$$\n",
    "\n",
    "$$H_{out} = \\lfloor \\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (H_f - 1) - 1}{\\text{stride}[1]} + 1 \\rfloor$$\n",
    "\n",
    "$$W_{out} = \\lfloor \\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times (W_f - 1) - 1}{\\text{stride}[2]} + 1 \\rfloor$$\n",
    "\n",
    "### Key Parameters\n",
    "- Same as Conv2d but for 3D spatial dimensions\n",
    "- `kernel_size`: Size of convolving kernel $(D_f, H_f, W_f)$\n",
    "\n",
    "### Use Cases\n",
    "- Video analysis\n",
    "- Medical imaging (CT, MRI)\n",
    "- Volumetric data processing\n",
    "\n",
    "## ConvTranspose1d\n",
    "\n",
    "### Definition\n",
    "ConvTranspose1d (deconvolution) performs a transposed 1D convolution operation, commonly used for upsampling.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input $x$ of shape $(N, C_{in}, L_{in})$ and filter $w$ of shape $(C_{in}, C_{out}, L_f)$, the output $y$ of shape $(N, C_{out}, L_{out})$ has length:\n",
    "\n",
    "$$L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation} \\times (L_f - 1) + \\text{output\\_padding} + 1$$\n",
    "\n",
    "### Key Parameters\n",
    "- `in_channels`: Number of input channels $(C_{in})$\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- `kernel_size`: Size of convolving kernel $(L_f)$\n",
    "- `stride`: Convolution stride (default: 1)\n",
    "- `padding`: Zero-padding added to input (default: 0)\n",
    "- `output_padding`: Additional size added to output (default: 0)\n",
    "- `groups`: Blocked connections between inputs and outputs (default: 1)\n",
    "- `bias`: Learnable bias addition (default: True)\n",
    "- `dilation`: Spacing between kernel elements (default: 1)\n",
    "\n",
    "### Use Cases\n",
    "- Signal upsampling\n",
    "- Audio generation\n",
    "- Sequence expansion\n",
    "\n",
    "## ConvTranspose2d\n",
    "\n",
    "### Definition\n",
    "ConvTranspose2d performs a transposed 2D convolution operation for upsampling feature maps.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input $x$ of shape $(N, C_{in}, H_{in}, W_{in})$ and filter $w$ of shape $(C_{in}, C_{out}, H_f, W_f)$, the output $y$ of shape $(N, C_{out}, H_{out}, W_{out})$ has dimensions:\n",
    "\n",
    "$$H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (H_f - 1) + \\text{output\\_padding}[0] + 1$$\n",
    "\n",
    "$$W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (W_f - 1) + \\text{output\\_padding}[1] + 1$$\n",
    "\n",
    "### Key Parameters\n",
    "- Same as ConvTranspose1d but for 2D spatial dimensions\n",
    "\n",
    "### Use Cases\n",
    "- Image generation (GANs, VAEs)\n",
    "- Semantic segmentation\n",
    "- Super-resolution\n",
    "\n",
    "## ConvTranspose3d\n",
    "\n",
    "### Definition\n",
    "ConvTranspose3d performs a transposed 3D convolution operation for volumetric data upsampling.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input $x$ of shape $(N, C_{in}, D_{in}, H_{in}, W_{in})$ and filter $w$ of shape $(C_{in}, C_{out}, D_f, H_f, W_f)$, the output $y$ of shape $(N, C_{out}, D_{out}, H_{out}, W_{out})$ has dimensions:\n",
    "\n",
    "$$D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (D_f - 1) + \\text{output\\_padding}[0] + 1$$\n",
    "\n",
    "$$H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (H_f - 1) + \\text{output\\_padding}[1] + 1$$\n",
    "\n",
    "$$W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2] \\times (W_f - 1) + \\text{output\\_padding}[2] + 1$$\n",
    "\n",
    "### Key Parameters\n",
    "- Same as ConvTranspose2d but for 3D spatial dimensions\n",
    "\n",
    "### Use Cases\n",
    "- Volumetric data generation\n",
    "- Medical image segmentation\n",
    "- Video frame synthesis\n",
    "\n",
    "## LazyConv1d\n",
    "\n",
    "### Definition\n",
    "LazyConv1d dynamically infers input channel dimensions during the first forward pass, eliminating the need to specify `in_channels`.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Identical to Conv1d, but weights and biases are only initialized after the first forward pass when input shape becomes known.\n",
    "\n",
    "### Key Parameters\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- Other parameters identical to Conv1d except `in_channels` is inferred\n",
    "\n",
    "### Use Cases\n",
    "- Dynamic network architectures\n",
    "- Transfer learning with varying input dimensions\n",
    "- AutoML workflows\n",
    "\n",
    "## LazyConv2d\n",
    "\n",
    "### Definition\n",
    "LazyConv2d dynamically infers input channel dimensions during the first forward pass for 2D convolutions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Identical to Conv2d with automatic inference of `in_channels`.\n",
    "\n",
    "### Key Parameters\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- Other parameters identical to Conv2d except `in_channels` is inferred\n",
    "\n",
    "### Use Cases\n",
    "- Dynamic image processing networks\n",
    "- Transfer learning across different image dimensions\n",
    "- Architecture search applications\n",
    "\n",
    "## LazyConv3d\n",
    "\n",
    "### Definition\n",
    "LazyConv3d dynamically infers input channel dimensions during the first forward pass for 3D convolutions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Identical to Conv3d with automatic inference of `in_channels`.\n",
    "\n",
    "### Key Parameters\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- Other parameters identical to Conv3d except `in_channels` is inferred\n",
    "\n",
    "### Use Cases\n",
    "- Dynamic 3D data processing networks\n",
    "- Transfer learning for volumetric data\n",
    "- Automated architecture design\n",
    "\n",
    "## LazyConvTranspose1d\n",
    "\n",
    "### Definition\n",
    "LazyConvTranspose1d dynamically infers input channel dimensions during the first forward pass for 1D transposed convolutions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Identical to ConvTranspose1d with automatic inference of `in_channels`.\n",
    "\n",
    "### Key Parameters\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- Other parameters identical to ConvTranspose1d except `in_channels` is inferred\n",
    "\n",
    "### Use Cases\n",
    "- Dynamic upsampling in signal processing\n",
    "- Adaptive sequence generation models\n",
    "\n",
    "## LazyConvTranspose2d\n",
    "\n",
    "### Definition\n",
    "LazyConvTranspose2d dynamically infers input channel dimensions during the first forward pass for 2D transposed convolutions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Identical to ConvTranspose2d with automatic inference of `in_channels`.\n",
    "\n",
    "### Key Parameters\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- Other parameters identical to ConvTranspose2d except `in_channels` is inferred\n",
    "\n",
    "### Use Cases\n",
    "- Dynamic image generation models\n",
    "- Adaptive upsampling in image processing\n",
    "\n",
    "## LazyConvTranspose3d\n",
    "\n",
    "### Definition\n",
    "LazyConvTranspose3d dynamically infers input channel dimensions during the first forward pass for 3D transposed convolutions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Identical to ConvTranspose3d with automatic inference of `in_channels`.\n",
    "\n",
    "### Key Parameters\n",
    "- `out_channels`: Number of output channels $(C_{out})$\n",
    "- Other parameters identical to ConvTranspose3d except `in_channels` is inferred\n",
    "\n",
    "### Use Cases\n",
    "- Dynamic 3D data generation\n",
    "- Adaptive volumetric upsampling\n",
    "\n",
    "## Unfold\n",
    "\n",
    "### Definition\n",
    "Unfold (im2col) extracts sliding local blocks from a batched input tensor, forming the basis for efficient convolution implementations.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input tensor $x$ of shape $(N, C, *)$ where $*$ represents spatial dimensions, Unfold extracts patches of size `kernel_size` with stride `stride` and dilation `dilation`, resulting in output tensor of shape $(N, C \\times \\prod(\\text{kernel\\_size}), L)$ where $L$ is the number of patches.\n",
    "\n",
    "For 2D input with spatial dimensions $(H, W)$:\n",
    "- Output shape: $(N, C \\times \\text{kernel\\_size}[0] \\times \\text{kernel\\_size}[1], L)$\n",
    "- Where $L = ((H - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1) / \\text{stride}[0] + 1) \\times ((W - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1) / \\text{stride}[1] + 1)$\n",
    "\n",
    "### Key Parameters\n",
    "- `kernel_size`: Size of sliding blocks\n",
    "- `stride`: Stride of sliding blocks (default: 1)\n",
    "- `padding`: Zero padding added to input (default: 0)\n",
    "- `dilation`: Spacing between kernel elements (default: 1)\n",
    "\n",
    "### Use Cases\n",
    "- Efficient convolution implementation\n",
    "- Custom kernel feature extraction\n",
    "- Patch-based representations\n",
    "\n",
    "## Fold\n",
    "\n",
    "### Definition\n",
    "Fold (col2im) combines an array of sliding local blocks into a large containing tensor, serving as the inverse of Unfold.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input tensor $x$ of shape $(N, C \\times \\prod(\\text{kernel\\_size}), L)$ and specified `output_size`, Fold combines patches to form a tensor of shape $(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], ...)$. In overlapping regions, values are summed.\n",
    "\n",
    "### Key Parameters\n",
    "- `output_size`: Spatial size of output tensor\n",
    "- `kernel_size`: Size of sliding blocks\n",
    "- `stride`: Stride of sliding blocks (default: 1)\n",
    "- `padding`: Zero padding added to input (default: 0)\n",
    "- `dilation`: Spacing between kernel elements (default: 1)\n",
    "\n",
    "### Use Cases\n",
    "- Implementing transposed convolutions\n",
    "- Reconstructing images from patches\n",
    "- Custom gradient computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atGEMsRg168L"
   },
   "source": [
    "# Pooling Layers in Neural Networks: Comprehensive Analysis\n",
    "\n",
    "## Introduction to Pooling Layers\n",
    "\n",
    "**Definition:** Pooling layers reduce the spatial dimensions (width, height, depth) of input data by performing downsampling operations. They serve to reduce computational complexity, extract dominant features, provide translational invariance, and mitigate overfitting.\n",
    "\n",
    "**General Mathematical Formulation:**\n",
    "For an input tensor $X$ with shape determined by its dimensionality, pooling applies an aggregation function $f$ over a local region $R$ to produce output $Y$:\n",
    "\n",
    "$$Y_{i} = f(\\{X_j | j \\in R_i\\})$$\n",
    "\n",
    "Where $R_i$ represents the receptive field for output position $i$.\n",
    "\n",
    "## Max Pooling Operations\n",
    "\n",
    "### MaxPool1d\n",
    "\n",
    "**Definition:** MaxPool1d performs maximum value extraction along a 1-dimensional input signal.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, L)$ where $N$ is batch size, $C$ is channels, and $L$ is sequence length:\n",
    "\n",
    "$$Y_{n,c,i} = \\max_{0 \\leq j < k} X_{n,c,stride \\cdot i + j}$$\n",
    "\n",
    "Where $k$ is kernel size and $stride$ determines step size.\n",
    "\n",
    "**Parameters:**\n",
    "- kernel_size: Size of pooling window\n",
    "- stride: Step size (default = kernel_size)\n",
    "- padding: Zero-padding added to both sides\n",
    "- dilation: Spacing between kernel elements\n",
    "- return_indices: Whether to return indices of max locations\n",
    "- ceil_mode: When True, will use ceil instead of floor for output size\n",
    "\n",
    "**Applications:** Audio signal processing, time-series analysis, 1D signal feature extraction.\n",
    "\n",
    "### MaxPool2d\n",
    "\n",
    "**Definition:** MaxPool2d extracts maximum values from 2D spatial regions of input tensors.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, H, W)$ where $H$ is height and $W$ is width:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\max_{0 \\leq h < k_h} \\max_{0 \\leq w < k_w} X_{n,c,stride_h \\cdot i + h, stride_w \\cdot j + w}$$\n",
    "\n",
    "Where $k_h, k_w$ represent kernel height and width.\n",
    "\n",
    "**Parameters:** Same as MaxPool1d but extended to 2D.\n",
    "\n",
    "**Applications:** Image processing, computer vision, CNN feature extraction.\n",
    "\n",
    "### MaxPool3d\n",
    "\n",
    "**Definition:** MaxPool3d performs max pooling over 3D spatial data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, D, H, W)$ where $D$ is depth:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\max_{0 \\leq z < k_d} \\max_{0 \\leq h < k_h} \\max_{0 \\leq w < k_w} X_{n,c,stride_d \\cdot d + z, stride_h \\cdot i + h, stride_w \\cdot j + w}$$\n",
    "\n",
    "**Parameters:** Same as above, extended to 3D.\n",
    "\n",
    "**Applications:** Video processing, medical imaging (CT/MRI), volumetric data analysis.\n",
    "\n",
    "## Max Unpooling Operations\n",
    "\n",
    "### MaxUnpool1d\n",
    "\n",
    "**Definition:** MaxUnpool1d performs partial inversion of MaxPool1d by placing values at specified indices.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, L_{out})$ and indices $I$:\n",
    "\n",
    "$$Y_{n,c,i} = \\begin{cases}\n",
    "X_{n,c,j}, & \\text{if } i = I_{n,c,j} \\text{ for some } j \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Parameters:**\n",
    "- kernel_size: Size of the max pooling window used\n",
    "- stride: Stride of the max pooling operation\n",
    "- padding: Padding added to max pooling operation\n",
    "\n",
    "**Applications:** Network visualization, reconstruction in autoencoders, feature decompression.\n",
    "\n",
    "### MaxUnpool2d\n",
    "\n",
    "**Definition:** MaxUnpool2d reverses MaxPool2d by reconstructing feature maps using saved indices.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, H_{out}, W_{out})$ and indices $I$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\begin{cases}\n",
    "X_{n,c,h,w}, & \\text{if } (i,j) = I_{n,c,h,w} \\text{ for some } (h,w) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Applications:** Segmentation networks (e.g., SegNet), feature visualization.\n",
    "\n",
    "### MaxUnpool3d\n",
    "\n",
    "**Definition:** MaxUnpool3d inverts MaxPool3d using saved indices from the pooling operation.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, D_{out}, H_{out}, W_{out})$ and indices $I$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\begin{cases}\n",
    "X_{n,c,d',h,w}, & \\text{if } (d,i,j) = I_{n,c,d',h,w} \\text{ for some } (d',h,w) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Applications:** 3D medical image segmentation, volumetric feature reconstruction.\n",
    "\n",
    "## Average Pooling Operations\n",
    "\n",
    "### AvgPool1d\n",
    "\n",
    "**Definition:** AvgPool1d applies average pooling over 1D inputs by computing mean values in sliding windows.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, L)$:\n",
    "\n",
    "$$Y_{n,c,i} = \\frac{1}{k} \\sum_{j=0}^{k-1} X_{n,c,stride \\cdot i + j}$$\n",
    "\n",
    "Where $k$ is the kernel size.\n",
    "\n",
    "**Parameters:**\n",
    "- kernel_size: Size of the averaging window\n",
    "- stride: Stride of the averaging window\n",
    "- padding: Zero-padding\n",
    "- ceil_mode: When True, uses ceil instead of floor for output size\n",
    "- count_include_pad: Include padding in averaging calculation\n",
    "\n",
    "**Applications:** Signal smoothing, feature generalization, noise reduction.\n",
    "\n",
    "### AvgPool2d\n",
    "\n",
    "**Definition:** AvgPool2d computes average values over 2D spatial regions.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, H, W)$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\frac{1}{k_h \\times k_w} \\sum_{h=0}^{k_h-1} \\sum_{w=0}^{k_w-1} X_{n,c,stride_h \\cdot i + h, stride_w \\cdot j + w}$$\n",
    "\n",
    "**Applications:** Image blurring, feature smoothing, texture analysis.\n",
    "\n",
    "### AvgPool3d\n",
    "\n",
    "**Definition:** AvgPool3d performs average pooling over 3D volumetric data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, D, H, W)$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\frac{1}{k_d \\times k_h \\times k_w} \\sum_{z=0}^{k_d-1} \\sum_{h=0}^{k_h-1} \\sum_{w=0}^{k_w-1} X_{n,c,stride_d \\cdot d + z, stride_h \\cdot i + h, stride_w \\cdot j + w}$$\n",
    "\n",
    "**Applications:** Video processing, 3D medical image analysis, volumetric data smoothing.\n",
    "\n",
    "## Fractional Max Pooling\n",
    "\n",
    "### FractionalMaxPool2d\n",
    "\n",
    "**Definition:** FractionalMaxPool2d implements max pooling with non-integer stride values, allowing for fractional output sizes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "The output size follows:\n",
    "\n",
    "$$H_{out} = \\lfloor \\frac{H_{in}}{output\\_ratio} \\rfloor \\quad \\text{or} \\quad \\lceil \\frac{H_{in}}{output\\_ratio} \\rceil$$\n",
    "$$W_{out} = \\lfloor \\frac{W_{in}}{output\\_ratio} \\rfloor \\quad \\text{or} \\quad \\lceil \\frac{W_{in}}{output\\_ratio} \\rceil$$\n",
    "\n",
    "Where pooling windows are generated either deterministically or randomly.\n",
    "\n",
    "**Parameters:**\n",
    "- kernel_size: Maximum kernel size\n",
    "- output_size: Target output size\n",
    "- output_ratio: Ratio of input to output size\n",
    "- return_indices: Whether to return indices\n",
    "- random_samples: Use random sampling to determine window locations\n",
    "\n",
    "**Applications:** Data augmentation, regularization, multi-scale feature extraction.\n",
    "\n",
    "### FractionalMaxPool3d\n",
    "\n",
    "**Definition:** FractionalMaxPool3d extends fractional max pooling to 3D volumes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Similar to FractionalMaxPool2d but with an additional dimension:\n",
    "\n",
    "$$D_{out} = \\lfloor \\frac{D_{in}}{output\\_ratio} \\rfloor \\quad \\text{or} \\quad \\lceil \\frac{D_{in}}{output\\_ratio} \\rceil$$\n",
    "\n",
    "**Applications:** Video processing with variable frame rates, multi-scale 3D feature extraction.\n",
    "\n",
    "## LP Pooling Operations\n",
    "\n",
    "### LPPool1d\n",
    "\n",
    "**Definition:** LPPool1d implements Lp norm pooling over 1D inputs.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, L)$:\n",
    "\n",
    "$$Y_{n,c,i} = \\left( \\sum_{j=0}^{k-1} |X_{n,c,stride \\cdot i + j}|^p \\right)^{1/p}$$\n",
    "\n",
    "Where $p$ is the norm parameter.\n",
    "\n",
    "**Parameters:**\n",
    "- norm_type: Lp norm value (p)\n",
    "- kernel_size: Size of pooling window\n",
    "- stride: Stride between pooling windows\n",
    "- ceil_mode: Use ceil or floor for output size\n",
    "\n",
    "**Applications:** Feature extraction with different norms, signal processing.\n",
    "\n",
    "### LPPool2d\n",
    "\n",
    "**Definition:** LPPool2d applies Lp norm pooling to 2D spatial data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, H, W)$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\left( \\sum_{h=0}^{k_h-1} \\sum_{w=0}^{k_w-1} |X_{n,c,stride_h \\cdot i + h, stride_w \\cdot j + w}|^p \\right)^{1/p}$$\n",
    "\n",
    "**Applications:** Image feature extraction with specialized norms, texture analysis.\n",
    "\n",
    "### LPPool3d\n",
    "\n",
    "**Definition:** LPPool3d extends Lp norm pooling to 3D volumes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, D, H, W)$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\left( \\sum_{z=0}^{k_d-1} \\sum_{h=0}^{k_h-1} \\sum_{w=0}^{k_w-1} |X_{n,c,stride_d \\cdot d + z, stride_h \\cdot i + h, stride_w \\cdot j + w}|^p \\right)^{1/p}$$\n",
    "\n",
    "**Applications:** Volumetric feature extraction with different norm constraints.\n",
    "\n",
    "## Adaptive Max Pooling Operations\n",
    "\n",
    "### AdaptiveMaxPool1d\n",
    "\n",
    "**Definition:** AdaptiveMaxPool1d performs max pooling where the output size is fixed and kernel size is adjusted automatically.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, L_{in})$ and target output length $L_{out}$:\n",
    "\n",
    "$$Y_{n,c,i} = \\max_{j \\in \\mathcal{R}(i)} X_{n,c,j}$$\n",
    "\n",
    "Where $\\mathcal{R}(i)$ is the region corresponding to output index $i$:\n",
    "\n",
    "$$\\mathcal{R}(i) = \\left\\{ j \\mid \\lfloor \\frac{j \\times L_{out}}{L_{in}} \\rfloor \\leq i < \\lfloor \\frac{(j+1) \\times L_{out}}{L_{in}} \\rfloor \\right\\}$$\n",
    "\n",
    "**Parameters:**\n",
    "- output_size: Desired output size\n",
    "- return_indices: Whether to return indices of maxima\n",
    "\n",
    "**Applications:** Feature extraction with consistent output dimensions regardless of input size.\n",
    "\n",
    "### AdaptiveMaxPool2d\n",
    "\n",
    "**Definition:** AdaptiveMaxPool2d adapts kernel size to achieve fixed output spatial dimensions.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, H_{in}, W_{in})$ and target output $(H_{out}, W_{out})$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\max_{h \\in \\mathcal{R}_h(i)} \\max_{w \\in \\mathcal{R}_w(j)} X_{n,c,h,w}$$\n",
    "\n",
    "Where $\\mathcal{R}_h(i)$ and $\\mathcal{R}_w(j)$ define the adaptive regions.\n",
    "\n",
    "**Applications:** Feature extraction for multi-scale inputs, transfer learning across architectures.\n",
    "\n",
    "### AdaptiveMaxPool3d\n",
    "\n",
    "**Definition:** AdaptiveMaxPool3d extends adaptive max pooling to 3D volumes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, D_{in}, H_{in}, W_{in})$ and target output $(D_{out}, H_{out}, W_{out})$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\max_{z \\in \\mathcal{R}_d(d)} \\max_{h \\in \\mathcal{R}_h(i)} \\max_{w \\in \\mathcal{R}_w(j)} X_{n,c,z,h,w}$$\n",
    "\n",
    "**Applications:** 3D medical image analysis, video processing with consistent output dimensions.\n",
    "\n",
    "## Adaptive Average Pooling Operations\n",
    "\n",
    "### AdaptiveAvgPool1d\n",
    "\n",
    "**Definition:** AdaptiveAvgPool1d performs average pooling with automatically adjusted kernel size to produce fixed output dimensions.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, L_{in})$ and target output length $L_{out}$:\n",
    "\n",
    "$$Y_{n,c,i} = \\frac{1}{|\\mathcal{R}(i)|} \\sum_{j \\in \\mathcal{R}(i)} X_{n,c,j}$$\n",
    "\n",
    "Where $|\\mathcal{R}(i)|$ is the cardinality of region $\\mathcal{R}(i)$.\n",
    "\n",
    "**Applications:** Audio feature extraction with fixed-length outputs, signal processing.\n",
    "\n",
    "### AdaptiveAvgPool2d\n",
    "\n",
    "**Definition:** AdaptiveAvgPool2d adapts window size to achieve fixed 2D output dimensions.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, H_{in}, W_{in})$ and target output $(H_{out}, W_{out})$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\frac{1}{|\\mathcal{R}_h(i)| \\times |\\mathcal{R}_w(j)|} \\sum_{h \\in \\mathcal{R}_h(i)} \\sum_{w \\in \\mathcal{R}_w(j)} X_{n,c,h,w}$$\n",
    "\n",
    "**Applications:** Global feature extraction, spatial dimension normalization, network architecture flexibility.\n",
    "\n",
    "### AdaptiveAvgPool3d\n",
    "\n",
    "**Definition:** AdaptiveAvgPool3d performs average pooling on 3D data with adaptive window sizes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input $X$ of shape $(N, C, D_{in}, H_{in}, W_{in})$ and target output $(D_{out}, H_{out}, W_{out})$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\frac{1}{|\\mathcal{R}_d(d)| \\times |\\mathcal{R}_h(i)| \\times |\\mathcal{R}_w(j)|} \\sum_{z \\in \\mathcal{R}_d(d)} \\sum_{h \\in \\mathcal{R}_h(i)} \\sum_{w \\in \\mathcal{R}_w(j)} X_{n,c,z,h,w}$$\n",
    "\n",
    "**Applications:** Video feature extraction, 3D medical image analysis with consistent output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35OQBw732wIK"
   },
   "source": [
    "# Padding Layers in Neural Networks: Comprehensive Analysis\n",
    "\n",
    "## Introduction to Padding Layers\n",
    "\n",
    "**Definition:** Padding layers extend input tensors by adding values around the borders, preserving spatial dimensions during convolution operations, reducing edge artifacts, and controlling boundary conditions for different data types.\n",
    "\n",
    "## Reflection Padding\n",
    "\n",
    "### ReflectionPad1d\n",
    "\n",
    "**Definition:** ReflectionPad1d extends 1D signals by mirroring input values at boundaries.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, L)$ with padding $(p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i} = \\begin{cases}\n",
    "X_{n,c,2p_l-i}, & \\text{if } i < p_l \\\\\n",
    "X_{n,c,i-p_l}, & \\text{if } p_l \\leq i < L+p_l \\\\\n",
    "X_{n,c,2(L-1)-(i-p_l)}, & \\text{if } i \\geq L+p_l\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Requires input width $L > p_l$ and $L > p_r$ to avoid mirroring padding region\n",
    "- Output shape: $(N, C, L+p_l+p_r)$\n",
    "- Creates continuous signal transitions at boundaries\n",
    "\n",
    "**Applications:**\n",
    "- Audio signal processing\n",
    "- Time-series analysis requiring continuous boundaries\n",
    "- Signal filtering while preserving edge characteristics\n",
    "\n",
    "### ReflectionPad2d\n",
    "\n",
    "**Definition:** ReflectionPad2d mirrors 2D data at boundaries, reflecting across edge pixels.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, H, W)$ with padding $(p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i,j} = X_{n,c,\\text{reflect}_H(i,p_t),\\text{reflect}_W(j,p_l)}$$\n",
    "\n",
    "Where reflection functions are:\n",
    "\n",
    "$$\\text{reflect}_H(i,p) = \\begin{cases}\n",
    "2p-i, & \\text{if } i < p \\\\\n",
    "i-p, & \\text{if } p \\leq i < H+p \\\\\n",
    "2(H-1)-(i-p), & \\text{if } i \\geq H+p\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Requires input dimensions $H > p_t$, $H > p_b$, $W > p_l$, $W > p_r$\n",
    "- Output shape: $(N, C, H+p_t+p_b, W+p_l+p_r)$\n",
    "- Preserves spatial continuity at image boundaries\n",
    "\n",
    "**Applications:**\n",
    "- Image generation and style transfer\n",
    "- Image inpainting and restoration\n",
    "- Reducing boundary artifacts in CNNs\n",
    "\n",
    "### ReflectionPad3d\n",
    "\n",
    "**Definition:** ReflectionPad3d extends reflection padding to volumetric 3D data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, D, H, W)$ with padding $(p_f, p_b, p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = X_{n,c,\\text{reflect}_D(d,p_f),\\text{reflect}_H(i,p_t),\\text{reflect}_W(j,p_l)}$$\n",
    "\n",
    "Using similar reflection functions for each dimension.\n",
    "\n",
    "**Applications:**\n",
    "- 3D medical image processing (MRI, CT)\n",
    "- Video data augmentation\n",
    "- Volumetric data analysis requiring boundary continuity\n",
    "\n",
    "## Replication Padding\n",
    "\n",
    "### ReplicationPad1d\n",
    "\n",
    "**Definition:** ReplicationPad1d extends input by repeating edge values.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, L)$ with padding $(p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i} = \\begin{cases}\n",
    "X_{n,c,0}, & \\text{if } i < p_l \\\\\n",
    "X_{n,c,i-p_l}, & \\text{if } p_l \\leq i < L+p_l \\\\\n",
    "X_{n,c,L-1}, & \\text{if } i \\geq L+p_l\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, L+p_l+p_r)$\n",
    "- Repeats first/last input values for all padding regions\n",
    "\n",
    "**Applications:**\n",
    "- Signal processing with meaningful boundary values\n",
    "- Time-series analysis where edge values represent state boundaries\n",
    "- Audio processing requiring constant edge extension\n",
    "\n",
    "### ReplicationPad2d\n",
    "\n",
    "**Definition:** ReplicationPad2d extends images by repeating edge pixels.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, H, W)$ with padding $(p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i,j} = X_{n,c,\\text{clamp}(i-p_t,0,H-1),\\text{clamp}(j-p_l,0,W-1)}$$\n",
    "\n",
    "Where $\\text{clamp}(x,\\text{min},\\text{max})$ restricts $x$ to the range $[\\text{min},\\text{max}]$.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, H+p_t+p_b, W+p_l+p_r)$\n",
    "- Corner regions replicate the corresponding corner pixel\n",
    "\n",
    "**Applications:**\n",
    "- Image segmentation\n",
    "- Object detection\n",
    "- Medical image analysis where boundary values have significance\n",
    "\n",
    "### ReplicationPad3d\n",
    "\n",
    "**Definition:** ReplicationPad3d extends the replication padding concept to 3D volumes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, D, H, W)$ with padding $(p_f, p_b, p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = X_{n,c,\\text{clamp}(d-p_f,0,D-1),\\text{clamp}(i-p_t,0,H-1),\\text{clamp}(j-p_l,0,W-1)}$$\n",
    "\n",
    "**Applications:**\n",
    "- 3D medical imaging\n",
    "- Video processing requiring edge frame preservation\n",
    "- Volumetric data analysis where boundary values are meaningful\n",
    "\n",
    "## Zero Padding\n",
    "\n",
    "### ZeroPad1d\n",
    "\n",
    "**Definition:** ZeroPad1d extends input signals by filling boundary regions with zeros.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, L)$ with padding $(p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i} = \\begin{cases}\n",
    "0, & \\text{if } i < p_l \\text{ or } i \\geq L+p_l \\\\\n",
    "X_{n,c,i-p_l}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, L+p_l+p_r)$\n",
    "- Simplest padding method computationally\n",
    "\n",
    "**Applications:**\n",
    "- General signal processing\n",
    "- Neural network feature extraction\n",
    "- Default padding in many CNN implementations\n",
    "\n",
    "### ZeroPad2d\n",
    "\n",
    "**Definition:** ZeroPad2d adds zero-valued pixels around 2D input data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, H, W)$ with padding $(p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\begin{cases}\n",
    "0, & \\text{if } i < p_t \\text{ or } i \\geq H+p_t \\text{ or } j < p_l \\text{ or } j \\geq W+p_l \\\\\n",
    "X_{n,c,i-p_t,j-p_l}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, H+p_t+p_b, W+p_l+p_r)$\n",
    "- Most common padding method in CNN architectures\n",
    "\n",
    "**Applications:**\n",
    "- Image classification\n",
    "- Feature extraction\n",
    "- Standard padding for convolutional layers\n",
    "\n",
    "### ZeroPad3d\n",
    "\n",
    "**Definition:** ZeroPad3d extends zero padding to volumetric 3D data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, D, H, W)$ with padding $(p_f, p_b, p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\begin{cases}\n",
    "0, & \\text{if } d < p_f \\text{ or } d \\geq D+p_f \\text{ or } i < p_t \\text{ or } i \\geq H+p_t \\text{ or } j < p_l \\text{ or } j \\geq W+p_l \\\\\n",
    "X_{n,c,d-p_f,i-p_t,j-p_l}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Applications:**\n",
    "- 3D convolutions in medical imaging\n",
    "- Video analysis\n",
    "- Volumetric data processing in deep learning\n",
    "\n",
    "## Constant Padding\n",
    "\n",
    "### ConstantPad1d\n",
    "\n",
    "**Definition:** ConstantPad1d fills padding regions with a specified constant value.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, L)$ with padding $(p_l, p_r)$ and constant value $k$:\n",
    "\n",
    "$$Y_{n,c,i} = \\begin{cases}\n",
    "k, & \\text{if } i < p_l \\text{ or } i \\geq L+p_l \\\\\n",
    "X_{n,c,i-p_l}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Generalizes ZeroPad1d by allowing arbitrary fill values\n",
    "- Output shape: $(N, C, L+p_l+p_r)$\n",
    "\n",
    "**Applications:**\n",
    "- Signal processing with specific background values\n",
    "- Creating signals with defined boundary conditions\n",
    "- Audio processing with controlled padding values\n",
    "\n",
    "### ConstantPad2d\n",
    "\n",
    "**Definition:** ConstantPad2d extends 2D data with a uniform constant value.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, H, W)$ with padding $(p_t, p_b, p_l, p_r)$ and constant value $k$:\n",
    "\n",
    "$$Y_{n,c,i,j} = \\begin{cases}\n",
    "k, & \\text{if } i < p_t \\text{ or } i \\geq H+p_t \\text{ or } j < p_l \\text{ or } j \\geq W+p_l \\\\\n",
    "X_{n,c,i-p_t,j-p_l}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, H+p_t+p_b, W+p_l+p_r)$\n",
    "- Provides control over padding values for specific applications\n",
    "\n",
    "**Applications:**\n",
    "- Image processing with defined background\n",
    "- Feature map preparation with semantic padding values\n",
    "- Data augmentation with controlled boundaries\n",
    "\n",
    "### ConstantPad3d\n",
    "\n",
    "**Definition:** ConstantPad3d extends constant padding to 3D volumes.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, D, H, W)$ with padding $(p_f, p_b, p_t, p_b, p_l, p_r)$ and constant value $k$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = \\begin{cases}\n",
    "k, & \\text{if } d < p_f \\text{ or } d \\geq D+p_f \\text{ or } i < p_t \\text{ or } i \\geq H+p_t \\text{ or } j < p_l \\text{ or } j \\geq W+p_l \\\\\n",
    "X_{n,c,d-p_f,i-p_t,j-p_l}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Applications:**\n",
    "- 3D medical imaging with defined background values\n",
    "- Volumetric data analysis with specific padding semantics\n",
    "- Video processing with controlled frame padding\n",
    "\n",
    "## Circular Padding\n",
    "\n",
    "### CircularPad1d\n",
    "\n",
    "**Definition:** CircularPad1d implements periodic boundary conditions by wrapping signal values.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, L)$ with padding $(p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i} = \\begin{cases}\n",
    "X_{n,c,(i+L) \\bmod L}, & \\text{if } i < p_l \\\\\n",
    "X_{n,c,i-p_l}, & \\text{if } p_l \\leq i < L+p_l \\\\\n",
    "X_{n,c,(i-p_l) \\bmod L}, & \\text{if } i \\geq L+p_l\n",
    "\\end{cases}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, L+p_l+p_r)$\n",
    "- Creates perfect circular continuity at boundaries\n",
    "\n",
    "**Applications:**\n",
    "- Fourier analysis and spectral methods\n",
    "- Periodic signal processing\n",
    "- Time-series with cyclical patterns\n",
    "\n",
    "### CircularPad2d\n",
    "\n",
    "**Definition:** CircularPad2d applies periodic boundary conditions to 2D data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, H, W)$ with padding $(p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,i,j} = X_{n,c,(i-p_t) \\bmod H,(j-p_l) \\bmod W}$$\n",
    "\n",
    "Where negative indices wrap around to the opposite edge.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Output shape: $(N, C, H+p_t+p_b, W+p_l+p_r)$\n",
    "- Establishes toroidal topology for 2D data\n",
    "\n",
    "**Applications:**\n",
    "- Texture synthesis and analysis\n",
    "- Image processing requiring continuous boundaries\n",
    "- CNNs for data with periodic structure (e.g., panoramic images)\n",
    "\n",
    "### CircularPad3d\n",
    "\n",
    "**Definition:** CircularPad3d extends circular padding to 3D volumetric data.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $X$ of shape $(N, C, D, H, W)$ with padding $(p_f, p_b, p_t, p_b, p_l, p_r)$:\n",
    "\n",
    "$$Y_{n,c,d,i,j} = X_{n,c,(d-p_f) \\bmod D,(i-p_t) \\bmod H,(j-p_l) \\bmod W}$$\n",
    "\n",
    "**Implementation Details:**\n",
    "- Creates periodicity in all three spatial dimensions\n",
    "- Useful for simulations with periodic boundary conditions\n",
    "\n",
    "**Applications:**\n",
    "- 3D physical simulations (fluid dynamics, electromagnetic fields)\n",
    "- Periodic volumetric data processing\n",
    "- Medical imaging with cyclic boundary requirements\n",
    "\n",
    "## Comparative Analysis of Padding Types\n",
    "\n",
    "| Padding Type | Boundary Continuity | Preserves Spatial Information | Computational Efficiency | Primary Application Domains |\n",
    "|--------------|---------------------|------------------------------|-------------------------|----------------------------|\n",
    "| Reflection   | High (C continuous) | High at boundaries          | Medium                  | Image generation, signal processing |\n",
    "| Replication  | Medium (C continuous) | High at boundaries        | High                    | Image segmentation, object detection |\n",
    "| Zero         | None                | Low at boundaries           | Very high               | General CNN architectures |\n",
    "| Constant     | None                | Low at boundaries           | High                    | Custom boundary requirements |\n",
    "| Circular     | High (perfect wrap) | High (periodic)             | Medium                  | Fourier analysis, periodic data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vhp6H8y9ss6o"
   },
   "source": [
    "<!-- ## 1. Recurrent Layers Overview\n",
    "\n",
    "A **Recurrent Layer** is a neural network component designed for sequential data processing. Unlike feedforward networks, recurrent layers maintain a hidden state that captures information about previous inputs. This intrinsic recurrence makes them suitable for tasks where the temporal or sequential order is critical. In mathematical terms, a recurrent layer updates its hidden state by applying a function over the current input and the previous hidden state.\n",
    "\n",
    "- **Definition:**  \n",
    "  A recurrent layer processes a sequence $$\\{x_1,x_2,\\ldots,x_T\\}$$ by updating a hidden state $$h_t$$ at each time step $$t$$:\n",
    "  $$\n",
    "  h_t = f(x_t, h_{t-1})\n",
    "  $$\n",
    "  \n",
    "- **Key Characteristics:**  \n",
    "  - **Memory:** Captures historical context via recursion.\n",
    "  - **Parameter Sharing:** Reuses the same weights at every time step.\n",
    "  - **Challenges:** Training can be difficult due to issues like vanishing and exploding gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Recurrent Neural Networks (RNN)\n",
    "\n",
    "### 2.1. RNN Base and Vanilla RNN\n",
    "\n",
    "The **Vanilla RNN** (or base RNN) is the simplest form of recurrent neural network. It updates its hidden state using a fixed transition function, typically a non-linear activation function such as $$\\tanh$$ or ReLU.\n",
    "\n",
    "- **Mathematical Formulation:**\n",
    "\n",
    "  Given an input sequence $$\\{x_t\\}$$, the hidden state $$h_t$$ is updated as:\n",
    "  $$\n",
    "  h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "  $$\n",
    "  where:\n",
    "  - $$W_{xh}$$ is the weight matrix connecting the input $$x_t$$ to the hidden state.\n",
    "  - $$W_{hh}$$ is the recurrent weight matrix connecting the previous hidden state $$h_{t-1}$$ to the current one.\n",
    "  - $$b_h$$ is the bias vector.\n",
    "  - $$\\phi(\\cdot)$$ is a nonlinear activation function, for example, $$\\tanh$$ or ReLU.\n",
    "\n",
    "- **Output Computation:**\n",
    "\n",
    "  The output $$y_t$$ at each time step can be computed as:\n",
    "  $$\n",
    "  y_t = \\psi(W_{hy} h_t + b_y)\n",
    "  $$\n",
    "  where:\n",
    "  - $$W_{hy}$$ is the weight matrix mapping the hidden state to the output.\n",
    "  - $$b_y$$ is the bias term for the output.\n",
    "  - $$\\psi(\\cdot)$$ is a suitable activation function depending on the application.\n",
    "\n",
    "- **Challenges:**\n",
    "  - **Vanishing/Exploding Gradients:** Gradients can diminish or explode during backpropagation through time (BPTT), making training unstable for long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Long Short-Term Memory (LSTM) Networks\n",
    "\n",
    "### 3.1. Definition\n",
    "\n",
    "**LSTM Networks** are a type of recurrent network designed to overcome the vanishing gradient problem by introducing a memory cell and gating mechanisms. These gates control the flow of information, allowing the network to retain or discard information over longer sequences.\n",
    "\n",
    "### 3.2. LSTM Equations and Components\n",
    "\n",
    "At each time step $$t$$, the LSTM unit computes the following:\n",
    "\n",
    "- **Input Gate ($$i_t$$):** Regulates the degree to which a new input contributes to the cell state.\n",
    "  $$\n",
    "  i_t = \\sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\n",
    "  $$\n",
    "\n",
    "- **Forget Gate ($$f_t$$):** Decides what information from the previous cell state should be retained.\n",
    "  $$\n",
    "  f_t = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\n",
    "  $$\n",
    "\n",
    "- **Cell Candidate ($$\\tilde{c}_t$$):** Represents new candidate values for the cell state.\n",
    "  $$\n",
    "  \\tilde{c}_t = \\tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\n",
    "  $$\n",
    "\n",
    "- **Cell State Update ($$c_t$$):** Combines the previous cell state and the candidate state according to the gates.\n",
    "  $$\n",
    "  c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "  $$\n",
    "  where $$\\odot$$ denotes element-wise multiplication.\n",
    "\n",
    "- **Output Gate ($$o_t$$):** Controls which parts of the cell state form the output.\n",
    "  $$\n",
    "  o_t = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\n",
    "  $$\n",
    "\n",
    "- **Hidden State ($$h_t$$):** Final output of the LSTM cell at time $$t$$.\n",
    "  $$\n",
    "  h_t = o_t \\odot \\tanh(c_t)\n",
    "  $$\n",
    "\n",
    "### 3.3. Key Points\n",
    "\n",
    "- **Memory Cell ($$c_t$$):** Stores long-term information.\n",
    "- **Gates:** Use the sigmoid activation $$\\sigma$$ to generate values between 0 and 1, effectively deciding how much information flows through each gate.\n",
    "- **Advantage:** LSTMs can capture long-term dependencies in sequential data, mitigating the exponential decay of gradients.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Gated Recurrent Unit (GRU) Networks\n",
    "\n",
    "### 4.1. Definition\n",
    "\n",
    "The **GRU Network** is a simplified version of the LSTM designed to achieve similar performance with fewer parameters. GRUs merge the input and forget gates into a single **update gate** and combine the cell state with the hidden state.\n",
    "\n",
    "### 4.2. GRU Equations and Components\n",
    "\n",
    "At each time step $$t$$, the GRU unit performs the following computations:\n",
    "\n",
    "- **Update Gate ($$z_t$$):** Determines the extent to which the previous hidden state is retained.\n",
    "  $$\n",
    "  z_t = \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)\n",
    "  $$\n",
    "\n",
    "- **Reset Gate ($$r_t$$):** Decides how to combine the new input with the previous hidden state.\n",
    "  $$\n",
    "  r_t = \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)\n",
    "  $$\n",
    "\n",
    "- **Candidate Hidden State ($$\\tilde{h}_t$$):** Proposed new hidden state based on the reset gate.\n",
    "  $$\n",
    "  \\tilde{h}_t = \\tanh(W_{xh} x_t + W_{hh}(r_t \\odot h_{t-1}) + b_h)\n",
    "  $$\n",
    "\n",
    "- **Final Hidden State Update ($$h_t$$):** Blend the old hidden state with the candidate state via the update gate.\n",
    "  $$\n",
    "  h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "  $$\n",
    "\n",
    "### 4.3. Key Points\n",
    "\n",
    "- **Simplification:** GRU combines LSTM gates, leading to a more compact model with fewer parameters.\n",
    "- **Efficiency:** Often achieves comparable performance to LSTM in many tasks with reduced computational complexity.\n",
    "- **Flexibility:** Can be particularly effective when training data is limited or computational resources are constrained.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. RNNCell, LSTMCell, and GRUCell\n",
    "\n",
    "### 5.1. Definition and Role\n",
    "\n",
    "- **Cell vs. Layer:**  \n",
    "  - A **cell** encapsulates the computation performed at a single time step.  \n",
    "  - A **layer** often stacks multiple cells together and manages the iteration over time steps automatically.\n",
    "\n",
    "- **Cells Provide:**\n",
    "  - **Granularity:** Fine control over each time step's operations.\n",
    "  - **Flexibility:** Ability to customize operations for each cell manually in research and experimental setups.\n",
    "\n",
    "### 5.2. Detailed Descriptions\n",
    "\n",
    "- **RNNCell:**\n",
    "  - Implements the computation of a vanilla RNN for one time step.\n",
    "  - **Equation:**\n",
    "    $$\n",
    "    h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "    $$\n",
    "  - **Usage:** Frequently used for custom implementations where manual looping over time or sophisticated management of hidden states is necessary.\n",
    "\n",
    "- **LSTMCell:**\n",
    "  - Implements the computation of an LSTM unit for one time step.\n",
    "  - **Equations:**\n",
    "    - Input Gate:  \n",
    "      $$\n",
    "      i_t = \\sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)\n",
    "      $$\n",
    "    - Forget Gate:  \n",
    "      $$\n",
    "      f_t = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)\n",
    "      $$\n",
    "    - Candidate Cell State:  \n",
    "      $$\n",
    "      \\tilde{c}_t = \\tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)\n",
    "      $$\n",
    "    - Cell State Update:  \n",
    "      $$\n",
    "      c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "      $$\n",
    "    - Output Gate:  \n",
    "      $$\n",
    "      o_t = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)\n",
    "      $$\n",
    "    - Hidden State Update:  \n",
    "      $$\n",
    "      h_t = o_t \\odot \\tanh(c_t)\n",
    "      $$\n",
    "  - **Usage:** Provides direct control over the internal gating mechanisms when fine-tuning or modifying the LSTM behavior.\n",
    "\n",
    "- **GRUCell:**\n",
    "  - Implements the computation of a GRU unit for one time step.\n",
    "  - **Equations:**\n",
    "    - Update Gate:  \n",
    "      $$\n",
    "      z_t = \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)\n",
    "      $$\n",
    "    - Reset Gate:  \n",
    "      $$\n",
    "      r_t = \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)\n",
    "      $$\n",
    "    - Candidate Hidden State:  \n",
    "      $$\n",
    "      \\tilde{h}_t = \\tanh(W_{xh} x_t + W_{hh} (r_t \\odot h_{t-1}) + b_h)\n",
    "      $$\n",
    "    - Hidden State Update:  \n",
    "      $$\n",
    "      h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "      $$\n",
    "  - **Usage:** Employed when a compact recurrent cell architecture is required and when the benefits of an LSTMs separate cell state are either unnecessary or detrimental to performance.\n",
    "\n",
    "### 5.3. Practical Considerations\n",
    "\n",
    "- **Choice of Cell vs. Layer:**  \n",
    "  - **Cells** offer granular control but require manual handling of sequences.\n",
    "  - **Layers** abstract away time-step iteration and are used for standard applications.\n",
    "\n",
    "- **Custom Architectures:**  \n",
    "  Researchers may create custom recurrent architectures by modifying the behavior of the basic cells (e.g., incorporating additional gates or using alternative activation functions) to better suit specific tasks such as language modeling, time-series forecasting, or speech recognition.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Recurrent Layers:** Provide sequential processing capabilities by integrating past information into current computations.\n",
    "- **Vanilla RNN:** Simplest recurrent model; suffers from gradient issues over long sequences.\n",
    "- **LSTM Networks:** Introduce memory cells and multiple gates to handle long-term dependencies.\n",
    "- **GRU Networks:** Offer a simplified alternative to LSTM with competitive performance.\n",
    "- **Cells (RNNCell, LSTMCell, GRUCell):** Represent the atomic operations at one time step, allowing for precise control and customization in recurrent network architectures.\n",
    "\n",
    "This comprehensive understanding of recurrent layers and their variants, detailed with mathematical equations and technical insights, equips researchers and AI scientists with the theoretical and practical knowledge necessary to design, implement, and analyze advanced sequence models. -->\n",
    "\n",
    "\n",
    "# Recurrent Layers\n",
    "\n",
    "## Recurrent Layers: Definition and Foundation\n",
    "\n",
    "Recurrent layers are neural network components designed to process sequential data by maintaining a hidden state that captures information from previous timesteps. The fundamental characteristic of recurrent layers is their ability to handle variable-length input sequences by sharing parameters across different positions in the sequence.\n",
    "\n",
    "The general form of a recurrent layer is:\n",
    "\n",
    "$$h_t = f(h_{t-1}, x_t; \\theta)$$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the input at time $t$\n",
    "- $\\theta$ represents the parameters of the function\n",
    "- $f$ is a non-linear activation function\n",
    "\n",
    "## RNN Base\n",
    "\n",
    "### Definition\n",
    "RNN Base refers to the foundational architecture upon which all recurrent neural networks are built. It defines the core recurrent computation pattern without specifying the exact internal transformation.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The base recurrent computation can be expressed as:\n",
    "\n",
    "$$h_t = \\phi(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n",
    "\n",
    "Where:\n",
    "- $W_{hh}$ is the recurrent weight matrix\n",
    "- $W_{xh}$ is the input-to-hidden weight matrix\n",
    "- $b_h$ is the bias vector\n",
    "- $\\phi$ is a non-linear activation function (typically tanh or ReLU)\n",
    "\n",
    "## RNN\n",
    "\n",
    "### Definition\n",
    "A Recurrent Neural Network (RNN) is the standard implementation of the recurrent layer concept, featuring a simple structure that applies the same transformation at each timestep.\n",
    "\n",
    "### Mathematical Formulation\n",
    "The standard RNN updates its hidden state as follows:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n",
    "$$y_t = W_{hy}h_t + b_y$$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the input at time $t$\n",
    "- $y_t$ is the output at time $t$\n",
    "- $W_{hh}$, $W_{xh}$, $W_{hy}$ are weight matrices\n",
    "- $b_h$, $b_y$ are bias vectors\n",
    "\n",
    "### Training Dynamics\n",
    "RNNs suffer from vanishing and exploding gradient problems during backpropagation through time (BPTT):\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\prod_{k=t}^{1} \\frac{\\partial h_k}{\\partial h_{k-1}} \\frac{\\partial h_0}{\\partial W}$$\n",
    "\n",
    "This involves repeated multiplication by the Jacobian matrix $\\frac{\\partial h_k}{\\partial h_{k-1}}$, leading to vanishing or exploding gradients.\n",
    "\n",
    "## LSTM\n",
    "\n",
    "### Definition\n",
    "Long Short-Term Memory (LSTM) networks address the vanishing gradient problem by introducing gating mechanisms that control information flow through the network.\n",
    "\n",
    "### Mathematical Formulation\n",
    "LSTM maintains both a cell state $c_t$ and a hidden state $h_t$:\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "Where:\n",
    "- $f_t$ is the forget gate\n",
    "- $i_t$ is the input gate\n",
    "- $\\tilde{c}_t$ is the candidate cell state\n",
    "- $c_t$ is the cell state\n",
    "- $o_t$ is the output gate\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "## GRU\n",
    "\n",
    "### Definition\n",
    "Gated Recurrent Unit (GRU) simplifies the LSTM architecture while maintaining its ability to capture long-term dependencies, using only two gates instead of three.\n",
    "\n",
    "### Mathematical Formulation\n",
    "GRU updates its hidden state as follows:\n",
    "\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
    "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "Where:\n",
    "- $z_t$ is the update gate\n",
    "- $r_t$ is the reset gate\n",
    "- $\\tilde{h}_t$ is the candidate hidden state\n",
    "- $h_t$ is the hidden state\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "\n",
    "## RNNCell\n",
    "\n",
    "### Definition\n",
    "RNNCell represents the atomic computational unit of a standard RNN, processing a single timestep rather than a sequence.\n",
    "\n",
    "### Mathematical Formulation\n",
    "RNNCell computes:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n",
    "\n",
    "### Implementation Considerations\n",
    "RNNCell is typically used in scenarios requiring explicit control over timestep processing, such as:\n",
    "\n",
    "$$\\text{state} = \\text{initial\\_state}$$\n",
    "$$\\text{for } x_t \\text{ in } x_{1:T}:$$\n",
    "$$\\quad \\text{state} = \\text{RNNCell}(x_t, \\text{state})$$\n",
    "$$\\quad \\text{outputs.append(state)}$$\n",
    "\n",
    "## LSTMCell\n",
    "\n",
    "### Definition\n",
    "LSTMCell is the atomic unit of LSTM computation, processing a single timestep and returning both updated cell state and hidden state.\n",
    "\n",
    "### Mathematical Formulation\n",
    "LSTMCell computes the same equations as full LSTM but for a single timestep:\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "### Implementation Dynamics\n",
    "LSTMCell returns a tuple $(h_t, c_t)$ requiring explicit management of both states:\n",
    "\n",
    "$$\\text{h, c} = \\text{initial\\_state}$$\n",
    "$$\\text{for } x_t \\text{ in } x_{1:T}:$$\n",
    "$$\\quad \\text{h, c} = \\text{LSTMCell}(x_t, (h, c))$$\n",
    "$$\\quad \\text{outputs.append(h)}$$\n",
    "\n",
    "## GRUCell\n",
    "\n",
    "### Definition\n",
    "GRUCell represents the atomic computational unit of a GRU, processing a single timestep and returning the updated hidden state.\n",
    "\n",
    "### Mathematical Formulation\n",
    "GRUCell computes:\n",
    "\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
    "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "### Computational Complexity\n",
    "GRUCell requires fewer parameters than LSTMCell:\n",
    "- GRU: $3 \\times (n_h \\times (n_h + n_x) + n_h)$ parameters\n",
    "- LSTM: $4 \\times (n_h \\times (n_h + n_x) + n_h)$ parameters\n",
    "\n",
    "Where $n_h$ is the hidden size and $n_x$ is the input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BJ7j__Z2y4r"
   },
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "## Transformer Layers\n",
    "\n",
    "Transformer layers are fundamental building blocks of the Transformer architecture that process sequential data through self-attention mechanisms and feed-forward neural networks. They are stacked together to form the encoder and decoder components.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "A transformer layer $L$ applies a series of transformations to an input sequence $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the sequence length and $d$ is the embedding dimension:\n",
    "\n",
    "$$L(X) = LayerNorm(SubLayer(X) + X)$$\n",
    "\n",
    "Where $SubLayer$ represents either attention mechanisms or feed-forward networks with their own parameters.\n",
    "\n",
    "## TransformerEncoder\n",
    "\n",
    "The TransformerEncoder consists of $N$ identical encoder layers stacked sequentially, processing input sequences to create contextual representations.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Given an input sequence $X = [x_1, x_2, ..., x_n]$, where each $x_i \\in \\mathbb{R}^d$:\n",
    "\n",
    "$$E_0(X) = PositionalEncoding(X)$$\n",
    "$$E_i(X) = EncoderLayer_i(E_{i-1}(X)) \\quad \\text{for} \\quad i \\in \\{1,...,N\\}$$\n",
    "$$TransformerEncoder(X) = E_N(X)$$\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "- **Input Embeddings**: Converts tokens to vectors of dimension $d_{model}$\n",
    "- **Positional Encoding**: Adds position information using sinusoidal functions\n",
    "  $$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n",
    "  $$PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n",
    "- **Encoder Stack**: $N$ identical encoder layers that preserve input dimensionality\n",
    "\n",
    "## TransformerDecoder\n",
    "\n",
    "The TransformerDecoder consists of $N$ identical decoder layers stacked sequentially, generating output sequences based on encoder outputs and previously generated tokens.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Given encoder output $E(X) \\in \\mathbb{R}^{n \\times d}$ and target sequence $Y_{<t} = [y_1, y_2, ..., y_{t-1}]$:\n",
    "\n",
    "$$D_0(Y_{<t}) = PositionalEncoding(Y_{<t})$$\n",
    "$$D_i(Y_{<t}, E(X)) = DecoderLayer_i(D_{i-1}(Y_{<t}, E(X)), E(X)) \\quad \\text{for} \\quad i \\in \\{1,...,N\\}$$\n",
    "$$TransformerDecoder(Y_{<t}, E(X)) = D_N(Y_{<t}, E(X))$$\n",
    "\n",
    "### Output Generation\n",
    "\n",
    "The decoder output is processed through a linear projection and softmax to generate probabilities:\n",
    "\n",
    "$$P(y_t|Y_{<t}, X) = Softmax(W \\cdot TransformerDecoder(Y_{<t}, E(X))_{t-1} + b)$$\n",
    "\n",
    "Where $W \\in \\mathbb{R}^{d \\times V}$ and $V$ is vocabulary size.\n",
    "\n",
    "## TransformerEncoderLayer\n",
    "\n",
    "A single layer within the encoder stack, consisting of multi-head self-attention and a position-wise feed-forward network.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For input $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "1. **Multi-Head Self-Attention**:\n",
    "   $$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$\n",
    "   $$head_i = Attention(XW_i^Q, XW_i^K, XW_i^V)$$\n",
    "   $$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "2. **Attention Output Processing**:\n",
    "   $$X' = LayerNorm(X + MultiHead(X, X, X))$$\n",
    "\n",
    "3. **Feed-Forward Network**:\n",
    "   $$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "   $$EncoderLayer(X) = LayerNorm(X' + FFN(X'))$$\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "- Layer normalization uses:\n",
    "  $$LayerNorm(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "  where $\\mu$ and $\\sigma$ are mean and standard deviation of input features\n",
    "\n",
    "- Dimensionality: $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$, $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ where $d_{ff}$ is typically $4d$\n",
    "\n",
    "## TransformerDecoderLayer\n",
    "\n",
    "A single layer within the decoder stack, consisting of masked multi-head self-attention, multi-head cross-attention over encoder outputs, and a feed-forward network.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For decoder input $Y \\in \\mathbb{R}^{m \\times d}$ and encoder output $E \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**:\n",
    "   $$A_1(Y) = MultiHead(Y, Y, Y, mask)$$\n",
    "   \n",
    "   Where $mask$ ensures each position only attends to prior positions:\n",
    "   $$mask_{ij} = \\begin{cases}\n",
    "   0 & \\text{if } i \\geq j \\\\\n",
    "   -\\infty & \\text{if } i < j\n",
    "   \\end{cases}$$\n",
    "   \n",
    "   $$Y' = LayerNorm(Y + A_1(Y))$$\n",
    "\n",
    "2. **Cross-Attention to Encoder**:\n",
    "   $$A_2(Y', E) = MultiHead(Y', E, E)$$\n",
    "   $$Y'' = LayerNorm(Y' + A_2(Y', E))$$\n",
    "\n",
    "3. **Feed-Forward Network**:\n",
    "   $$DecoderLayer(Y, E) = LayerNorm(Y'' + FFN(Y''))$$\n",
    "\n",
    "### Key Implementation Features\n",
    "\n",
    "- The masking in self-attention enforces autoregressive property by preventing attention to future positions\n",
    "- Cross-attention allows decoder to focus on relevant parts of the input sequence\n",
    "- Each sub-layer (self-attention, cross-attention, FFN) maintains the input dimension $d$ through the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlGsZKu8xFqH"
   },
   "source": [
    "# Sparse Layers\n",
    "\n",
    "## Definition\n",
    "Sparse layers are neural network components that process sparse inputs or produce sparse activations, where most elements are zero. These layers exploit sparsity for computational efficiency and reduced memory usage.\n",
    "\n",
    "## Mathematical Formulation\n",
    "For a sparse input vector $x \\in \\mathbb{R}^n$ with only $k$ non-zero elements where $k \\ll n$, a sparse layer operation can be expressed as:\n",
    "\n",
    "$$y = f(Wx + b)$$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{m \\times n}$ is the weight matrix\n",
    "- $b \\in \\mathbb{R}^m$ is the bias vector\n",
    "- $f$ is an activation function\n",
    "\n",
    "The computational complexity is reduced from $O(mn)$ to $O(mk)$ by only computing operations for non-zero elements.\n",
    "\n",
    "## Sparse Matrix Representation\n",
    "Sparse matrices can be represented in multiple formats:\n",
    "- Coordinate (COO): $(row_i, col_i, value_i)$ tuples\n",
    "- Compressed Sparse Row (CSR):\n",
    "  $$CSR = (values, column\\_indices, row\\_pointers)$$\n",
    "- Compressed Sparse Column (CSC)\n",
    "\n",
    "## Sparsity in Neural Networks\n",
    "Sparsity can be introduced through:\n",
    "1. **Structural sparsity**: Predetermined sparse connection patterns\n",
    "   $$M_{ij} = \\begin{cases}\n",
    "   1 & \\text{if connection exists} \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}$$\n",
    "   \n",
    "2. **Weight pruning**: Removing weights based on magnitude\n",
    "   $$W_{pruned} = W \\odot M \\text{ where } M_{ij} = \\begin{cases}\n",
    "   1 & \\text{if } |W_{ij}| > \\tau \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}$$\n",
    "\n",
    "3. **Activation sparsity**: Using ReLU or similar activations\n",
    "   $$ReLU(x) = \\max(0, x)$$\n",
    "\n",
    "## Block-Sparse Operations\n",
    "For structured sparsity with block patterns:\n",
    "$$Y_{block} = W_{block} \\cdot X_{block}$$\n",
    "\n",
    "Where operations only occur on non-zero blocks, typically accelerated by specialized hardware.\n",
    "\n",
    "# Embedding\n",
    "\n",
    "## Definition\n",
    "Embedding is a technique that maps discrete categorical variables (like words, tokens, or IDs) to continuous vector spaces of lower dimensionality, capturing semantic relationships.\n",
    "\n",
    "## Mathematical Formulation\n",
    "For vocabulary size $V$ and embedding dimension $d$:\n",
    "$$E: \\{1,2,...,V\\} \\rightarrow \\mathbb{R}^d$$\n",
    "\n",
    "Implementation as a lookup table:\n",
    "$$E = [e_1, e_2, ..., e_V]^T \\in \\mathbb{R}^{V \\times d}$$\n",
    "\n",
    "For input token $i$, the embedding is retrieved as:\n",
    "$$e_i = E[i]$$\n",
    "\n",
    "## Training Methods\n",
    "1. **Supervised learning**: Embeddings trained as part of neural network\n",
    "   $$\\mathcal{L}_{supervised} = \\mathcal{L}(f(E[i]), y_i)$$\n",
    "\n",
    "2. **Self-supervised learning**: Word2Vec approaches\n",
    "   - Skip-gram:\n",
    "  $$\\mathcal{L}_{skip} = -\\sum_{i=1}^{T}\\sum_{j \\in context(i)}\\log P(w_j|w_i)$$\n",
    "   - CBOW:\n",
    "$$\\mathcal{L}_{CBOW} = -\\sum_{i=1}^{T}\\log P(w_i|context(i))$$\n",
    "\n",
    "3. **Matrix factorization**: Factorizing co-occurrence matrices\n",
    "   $$\\min_{E,C} \\sum_{i,j} f(X_{ij})(e_i^T c_j - \\log X_{ij})^2$$\n",
    "\n",
    "## Properties\n",
    "- **Dimensionality**: Typically $d \\ll V$\n",
    "- **Semantic similarity**: $sim(e_i, e_j) = \\frac{e_i \\cdot e_j}{||e_i|| \\cdot ||e_j||}$\n",
    "- **Linear relationships**: $e_{king} - e_{man} + e_{woman} \\approx e_{queen}$\n",
    "\n",
    "# EmbeddingBag\n",
    "\n",
    "## Definition\n",
    "EmbeddingBag extends standard embeddings by efficiently handling variable-length sequences of embeddings through pooling operations over multiple indices.\n",
    "\n",
    "## Mathematical Formulation\n",
    "For a bag of indices $\\{i_1, i_2, ..., i_n\\}$ from vocabulary of size $V$:\n",
    "$$EmbeddingBag(\\{i_1, i_2, ..., i_n\\}) = \\text{pool}(E[i_1], E[i_2], ..., E[i_n])$$\n",
    "\n",
    "Where $\\text{pool}$ is typically a sum, mean, or max operation:\n",
    "- Sum pooling: $$\\sum_{j=1}^{n} E[i_j]$$\n",
    "- Mean pooling: $$\\frac{1}{n}\\sum_{j=1}^{n} E[i_j]$$\n",
    "- Max pooling: $$\\max_{j} E[i_j]$$\n",
    "\n",
    "## Efficient Implementation\n",
    "EmbeddingBag optimizes computation by:\n",
    "1. Avoiding intermediate per-token embeddings storage\n",
    "2. Fusing lookup and pooling operations\n",
    "3. Supporting sparse gradients during backpropagation\n",
    "\n",
    "## Mathematical Advantages\n",
    "For sparse inputs with embedding dimension $d$ and sequence length $s$:\n",
    "- Memory complexity: $O(d)$ vs $O(s \\times d)$ for regular embeddings\n",
    "- Computational complexity: $O(k \\times d)$ where $k$ is unique embeddings\n",
    "\n",
    "## Applications\n",
    "1. **Bag-of-words representations**:\n",
    "   $$\\text{doc}_i = EmbeddingBag(\\text{tokens in doc}_i)$$\n",
    "\n",
    "2. **Feature hashing**:\n",
    "   $$\\text{feature}_j = EmbeddingBag(\\text{hash}(\\text{feature}_j))$$\n",
    "\n",
    "3. **Multi-hot encodings**:\n",
    "   $$\\text{categorical features} = EmbeddingBag(\\text{active categories})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hnvm4mIwada"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Tb5yRyepLm6"
   },
   "source": [
    "# Activation Functions\n",
    "\n",
    "## ReLU Family\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\begin{cases}\n",
    "1, & \\text{if}\\ x > 0 \\\\\n",
    "0, & \\text{if}\\ x < 0 \\\\\n",
    "\\text{undefined}, & \\text{if}\\ x = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Properties:\n",
    "- Sparse activation\n",
    "- Unbounded positive range\n",
    "- Suffers from \"dying ReLU\" problem\n",
    "\n",
    "### LeakyReLU\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha x, & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\alpha$ is small constant (e.g., 0.01).\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\begin{cases}\n",
    "1, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha, & \\text{if}\\ x < 0 \\\\\n",
    "\\text{undefined}, & \\text{if}\\ x = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "### PReLU (Parametric ReLU)\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha_i x, & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\alpha_i$ is learned parameter per channel $i$.\n",
    "\n",
    "### RReLU (Randomized Leaky ReLU)\n",
    "During training:\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha_i x, & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "$\\alpha_i$ sampled from uniform distribution $\\mathcal{U}(l, u)$.\n",
    "\n",
    "During inference:\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\frac{l+u}{2} x, & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "### ReLU6\n",
    "$$f(x) = \\min(\\max(0, x), 6)$$\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\begin{cases}\n",
    "1, & \\text{if}\\ 0 < x < 6 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### ELU (Exponential Linear Unit)\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha(e^x - 1), & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\begin{cases}\n",
    "1, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha e^x, & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "### SELU (Scaled ELU)\n",
    "$$f(x) = \\lambda \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha(e^x - 1), & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\lambda \\approx 1.0507$ and $\\alpha \\approx 1.6733$.\n",
    "\n",
    "### CELU (Continuously Differentiable ELU)\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > 0 \\\\\n",
    "\\alpha(e^{x/\\alpha} - 1), & \\text{if}\\ x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "### GELU (Gaussian Error Linear Unit)\n",
    "$$f(x) = x \\cdot \\Phi(x)$$\n",
    "\n",
    "Where $\\Phi(x)$ is cumulative distribution function of standard normal distribution.\n",
    "\n",
    "Approximation:\n",
    "$$f(x) \\approx 0.5x(1 + \\tanh[\\sqrt{2/\\pi}(x + 0.044715x^3)])$$\n",
    "\n",
    "### SiLU (Sigmoid Linear Unit) / Swish\n",
    "$$f(x) = x \\cdot \\sigma(x)$$\n",
    "\n",
    "Where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "### Mish\n",
    "$$f(x) = x \\cdot \\tanh(\\ln(1 + e^x))$$\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\frac{e^x \\cdot \\omega}{(1 + e^x)^2 \\cdot (1 + \\omega)^2} + \\tanh(\\ln(1 + e^x))$$\n",
    "\n",
    "Where $\\omega = 4(x+1) + 4e^{2x} + e^{3x} + e^x(4x+6)$\n",
    "\n",
    "## Sigmoid Family\n",
    "\n",
    "### Sigmoid\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Derivative:\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "### Hardsigmoid\n",
    "$$f(x) = \\begin{cases}\n",
    "0, & \\text{if}\\ x \\leq -3 \\\\\n",
    "1, & \\text{if}\\ x \\geq 3 \\\\\n",
    "\\frac{x+3}{6}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### LogSigmoid\n",
    "$$f(x) = \\log(\\sigma(x)) = \\log\\left(\\frac{1}{1 + e^{-x}}\\right) = -\\log(1 + e^{-x})$$\n",
    "\n",
    "## Tanh Family\n",
    "\n",
    "### Tanh (Hyperbolic Tangent)\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "Derivative:\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "\n",
    "### Hardtanh\n",
    "$$f(x) = \\begin{cases}\n",
    "-1, & \\text{if}\\ x < -1 \\\\\n",
    "1, & \\text{if}\\ x > 1 \\\\\n",
    "x, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Tanhshrink\n",
    "$$f(x) = x - \\tanh(x)$$\n",
    "\n",
    "## Specialized Activation Functions\n",
    "\n",
    "### Hardswish\n",
    "$$f(x) = x \\cdot \\text{Hardsigmoid}(x) = \\begin{cases}\n",
    "0, & \\text{if}\\ x \\leq -3 \\\\\n",
    "x, & \\text{if}\\ x \\geq 3 \\\\\n",
    "x \\cdot \\frac{x+3}{6}, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Hardshrink\n",
    "$$f(x) = \\begin{cases}\n",
    "x, & \\text{if}\\ x > \\lambda\\ \\text{or}\\ x < -\\lambda \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Softshrink\n",
    "$$f(x) = \\begin{cases}\n",
    "x - \\lambda, & \\text{if}\\ x > \\lambda \\\\\n",
    "x + \\lambda, & \\text{if}\\ x < -\\lambda \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Softplus\n",
    "$$f(x) = \\frac{1}{\\beta} \\log(1 + e^{\\beta x})$$\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\frac{1}{1 + e^{-\\beta x}} = \\sigma(\\beta x)$$\n",
    "\n",
    "### Softsign\n",
    "$$f(x) = \\frac{x}{1 + |x|}$$\n",
    "\n",
    "Derivative:\n",
    "$$f'(x) = \\frac{1}{(1 + |x|)^2}$$\n",
    "\n",
    "### Threshold\n",
    "$$f(x) = \\begin{cases}\n",
    "\\text{value}, & \\text{if}\\ x > \\text{threshold} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### GLU (Gated Linear Unit)\n",
    "$$\\text{GLU}(a, b) = a \\otimes \\sigma(b)$$\n",
    "\n",
    "Where $a, b$ are inputs, typically split from same tensor and $\\otimes$ is element-wise product.\n",
    "\n",
    "## MultiheadAttention\n",
    "\n",
    "Mathematical formulation:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "For multihead with $h$ heads:\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "$W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$, and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$ are parameter matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_nqLChkqBh7"
   },
   "source": [
    "# Neural Network Activation Functions: Softmax and Variants\n",
    "\n",
    "## 1. Softmin\n",
    "\n",
    "### Definition\n",
    "Softmin applies the softmax operation to the negation of input values, giving higher probabilities to smaller values in the input tensor.\n",
    "\n",
    "### Mathematical Formulation\n",
    "$$\\text{Softmin}(x_i) = \\frac{e^{-x_i}}{\\sum_j e^{-x_j}}$$\n",
    "\n",
    "Alternatively expressed as:\n",
    "$$\\text{Softmin}(x) = \\text{Softmax}(-x)$$\n",
    "\n",
    "### Technical Details\n",
    "- **Input Transformation**: Converts each input element $x_i$ to $e^{-x_i}$\n",
    "- **Normalization**: Divides by sum of all transformed values to ensure outputs sum to 1\n",
    "- **Properties**:\n",
    "  - Output range: $(0,1)$ for each element\n",
    "  - Sum of outputs equals 1\n",
    "  - Emphasizes smaller values (inverse behavior of Softmax)\n",
    "- **Applications**: Distance-based attention, inverse priority weighting, minimization problems\n",
    "\n",
    "## 2. Softmax\n",
    "\n",
    "### Definition\n",
    "Softmax transforms a vector of real numbers into a probability distribution by exponentiating inputs and normalizing them to sum to 1.\n",
    "\n",
    "### Mathematical Formulation\n",
    "$$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "### Technical Details\n",
    "- **Input Transformation**: Applies exponential function $e^{x_i}$ to each input\n",
    "- **Normalization**: Divides by sum of all exponentiated values\n",
    "- **Dimension Handling**: Applied along specified dimension (default: last dimension)\n",
    "- **Mathematical Properties**:\n",
    "  - Outputs in range $(0,1)$\n",
    "  - Sum of outputs equals 1\n",
    "  - Preserves ordering: if $x_i > x_j$ then $\\text{Softmax}(x_i) > \\text{Softmax}(x_j)$\n",
    "  - Not invariant to constant addition: $\\text{Softmax}(x + c) \\neq \\text{Softmax}(x)$\n",
    "  - Sensitive to scaling: $\\text{Softmax}(\\lambda x) \\neq \\text{Softmax}(x)$ for $\\lambda \\neq 1$\n",
    "- **Numerical Stability**: Improved by subtracting $\\max(x)$ from all inputs:\n",
    "  $$\\text{Softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{n} e^{x_j - \\max(x)}}$$\n",
    "\n",
    "## 3. Softmax2d\n",
    "\n",
    "### Definition\n",
    "Specialized Softmax implementation for 2D feature maps in convolutional networks, applying softmax across channels at each spatial location.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a 4D tensor with shape $(N, C, H, W)$:\n",
    "$$\\text{Softmax2d}(x)_{n,c,h,w} = \\frac{e^{x_{n,c,h,w}}}{\\sum_{c'=1}^{C} e^{x_{n,c',h,w}}}$$\n",
    "\n",
    "Where:\n",
    "- $N$ = batch size\n",
    "- $C$ = number of channels\n",
    "- $H$ = height\n",
    "- $W$ = width\n",
    "\n",
    "### Technical Details\n",
    "- **Input Format**: 4D tensor with shape $(N, C, H, W)$\n",
    "- **Operation**: Applies softmax independently at each spatial position $(h,w)$ across channel dimension\n",
    "- **Output**: Same shape as input, with values normalized across channels\n",
    "- **Applications**:\n",
    "  - Semantic segmentation\n",
    "  - Pixel-wise classification\n",
    "  - Attention maps in vision models\n",
    "- **Implementation Note**: Equivalent to reshaping tensor to $(N \\times H \\times W, C)$, applying standard softmax, then reshaping back\n",
    "\n",
    "## 4. LogSoftmax\n",
    "\n",
    "### Definition\n",
    "LogSoftmax computes logarithm of softmax values directly, providing numerical stability for classification tasks.\n",
    "\n",
    "### Mathematical Formulation\n",
    "$$\\text{LogSoftmax}(x_i) = \\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\\right) = x_i - \\log\\left(\\sum_{j=1}^{n} e^{x_j}\\right)$$\n",
    "\n",
    "### Technical Details\n",
    "- **Numerical Stability**: More stable than separate softmax and logarithm operations\n",
    "- **Computational Efficiency**: Optimized implementation avoids redundant calculations\n",
    "- **Output Properties**:\n",
    "  - All values are $\\leq 0$ (logarithm of values in range $(0,1)$)\n",
    "  - Maximum possible value is 0 (when one input dominates completely)\n",
    "  - Sum of exponentiated outputs equals 1: $\\sum_i e^{\\text{LogSoftmax}(x_i)} = 1$\n",
    "- **Gradient Calculation**: Simpler and more stable than computing through separate operations\n",
    "- **Common Usage**: Paired with NLLLoss for classification tasks, equivalent to CrossEntropyLoss\n",
    "\n",
    "## 5. AdaptiveLogSoftmaxWithLoss\n",
    "\n",
    "### Definition\n",
    "Efficient approximation of softmax with negative log-likelihood loss for large vocabulary tasks, using hierarchical structure to reduce computational complexity.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Vocabulary Partitioning\n",
    "Given vocabulary $V$ partitioned into clusters $\\{C_0, C_1, ..., C_k\\}$ based on frequency:\n",
    "- $C_0$: head cluster (frequent words)\n",
    "- $C_1, ..., C_k$: tail clusters (rare words)\n",
    "\n",
    "#### Probability Computation\n",
    "For word $w$ in cluster $C_j$:\n",
    "$$P(w|x) = P(C_j|x) \\times P(w|C_j,x)$$\n",
    "\n",
    "Head cluster probability ($w \\in C_0$):\n",
    "$$P(w|x) = \\frac{e^{x_w^T\\theta_w}}{\\sum_{i \\in C_0} e^{x_i^T\\theta_i} + \\sum_{j=1}^k e^{x_{C_j}^T\\theta_{C_j}}}$$\n",
    "\n",
    "Tail cluster probability ($w \\in C_j, j > 0$):\n",
    "$$P(w|x) = \\frac{e^{x_{C_j}^T\\theta_{C_j}}}{\\sum_{i \\in C_0} e^{x_i^T\\theta_i} + \\sum_{l=1}^k e^{x_{C_l}^T\\theta_{C_l}}} \\times \\frac{e^{x_w^T\\theta_w}}{\\sum_{i \\in C_j} e^{x_i^T\\theta_i}}$$\n",
    "\n",
    "Where:\n",
    "- $x$: input embedding\n",
    "- $\\theta_w$: word projection parameters\n",
    "- $\\theta_{C_j}$: cluster projection parameters\n",
    "\n",
    "#### Projection Dimension Reduction\n",
    "For word $w$ in cluster $C_j$:\n",
    "$$D_j = \\frac{D}{\\text{div\\_value}^{j}}$$\n",
    "\n",
    "Where:\n",
    "- $D$: original projection dimension\n",
    "- $D_j$: reduced projection dimension for cluster $C_j$\n",
    "- $\\text{div\\_value}$: hyperparameter controlling dimension reduction\n",
    "\n",
    "### Technical Details\n",
    "- **Efficiency**: Reduces complexity from $O(N \\times V)$ to approximately $O(N \\times \\log(V))$\n",
    "- **Cluster Organization**:\n",
    "  - Based on word frequency (Zipfian distribution)\n",
    "  - Specified through cutoff thresholds\n",
    "- **Projection Dimensions**:\n",
    "  - Full dimension for frequent words\n",
    "  - Reduced dimensions for rare words\n",
    "  - Controlled by div_value parameter\n",
    "- **Training Process**:\n",
    "  - Jointly optimizes cluster and word probabilities\n",
    "  - Computes loss efficiently using hierarchical structure\n",
    "- **Memory Optimization**: Uses smaller matrices for rare words, significantly reducing parameter count\n",
    "- **Applications**: Language modeling, machine translation, any task with large output vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm9DxoHnrfyu"
   },
   "source": [
    "# Normalization Layers in Deep Neural Networks\n",
    "\n",
    "## Introduction to Normalization\n",
    "\n",
    "Normalization stabilizes and accelerates training by transforming feature distributions. Normalization techniques control feature statistics across various dimensions of the neural network's activation tensors.\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "### BatchNorm1d\n",
    "\n",
    "**Definition:** Applies normalization over a mini-batch of 1D inputs.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $x \\in \\mathbb{R}^{B \\times C \\times L}$ (batch, channels, length):\n",
    "\n",
    "$$\\hat{x}_{b,c,l} = \\frac{x_{b,c,l} - \\mathrm{E}[x_{:,c,l}]}{\\sqrt{\\mathrm{Var}[x_{:,c,l}] + \\epsilon}}$$\n",
    "\n",
    "$$y_{b,c,l} = \\gamma_c \\cdot \\hat{x}_{b,c,l} + \\beta_c$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{E}[x_{:,c,l}] = \\frac{1}{B} \\sum_{b=1}^{B} x_{b,c,l}$\n",
    "- $\\mathrm{Var}[x_{:,c,l}] = \\frac{1}{B} \\sum_{b=1}^{B} (x_{b,c,l} - \\mathrm{E}[x_{:,c,l}])^2$\n",
    "- $\\gamma_c, \\beta_c$ are learnable parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "\n",
    "### BatchNorm2d\n",
    "\n",
    "**Definition:** Normalizes 2D feature maps in CNNs.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $x \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ (batch, channels, height, width):\n",
    "\n",
    "$$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mathrm{E}[x_{:,c,:,:}]}{\\sqrt{\\mathrm{Var}[x_{:,c,:,:}] + \\epsilon}}$$\n",
    "\n",
    "$$y_{b,c,h,w} = \\gamma_c \\cdot \\hat{x}_{b,c,h,w} + \\beta_c$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{E}[x_{:,c,:,:}] = \\frac{1}{B \\cdot H \\cdot W} \\sum_{b=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{b,c,h,w}$\n",
    "- $\\mathrm{Var}[x_{:,c,:,:}]$ calculated similarly over $(b,h,w)$ dimensions\n",
    "\n",
    "### BatchNorm3d\n",
    "\n",
    "**Definition:** Normalizes 3D feature volumes (e.g., video data).\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $x \\in \\mathbb{R}^{B \\times C \\times D \\times H \\times W}$ (batch, channels, depth, height, width):\n",
    "\n",
    "$$\\hat{x}_{b,c,d,h,w} = \\frac{x_{b,c,d,h,w} - \\mathrm{E}[x_{:,c,:,:,:}]}{\\sqrt{\\mathrm{Var}[x_{:,c,:,:,:}] + \\epsilon}}$$\n",
    "\n",
    "$$y_{b,c,d,h,w} = \\gamma_c \\cdot \\hat{x}_{b,c,d,h,w} + \\beta_c$$\n",
    "\n",
    "**Training vs. Inference:**\n",
    "- During training: Uses mini-batch statistics\n",
    "- During inference: Uses running estimates of population statistics\n",
    "  $$\\mu_c = (1-\\alpha) \\cdot \\mu_c + \\alpha \\cdot \\mathrm{E}[x_{:,c,:,:}]$$\n",
    "  $$\\sigma^2_c = (1-\\alpha) \\cdot \\sigma^2_c + \\alpha \\cdot \\mathrm{Var}[x_{:,c,:,:}]$$\n",
    "  where $\\alpha$ is momentum parameter (typically 0.1)\n",
    "\n",
    "## Lazy Batch Normalization\n",
    "\n",
    "### LazyBatchNorm1d, LazyBatchNorm2d, LazyBatchNorm3d\n",
    "\n",
    "**Definition:** Variant of BatchNorm that infers feature dimensions on first input.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Identical mathematical formulation to corresponding BatchNorm\n",
    "- Automatically initializes $\\gamma$ and $\\beta$ parameters based on first input tensor\n",
    "- Infers number of channels ($C$) from first forward pass\n",
    "- Parameters are initialized only when dimensions are known\n",
    "\n",
    "**Mathematical Initialization:**\n",
    "When first input passes through, for $c$ channels:\n",
    "$$\\gamma = \\text{ones}(c)$$\n",
    "$$\\beta = \\text{zeros}(c)$$\n",
    "\n",
    "## Group Normalization\n",
    "\n",
    "**Definition:** Normalizes by dividing channels into groups and normalizing each group.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $x \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ divided into $G$ groups:\n",
    "\n",
    "$$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mathrm{E}[x_{b,g(c),:,:}]}{\\sqrt{\\mathrm{Var}[x_{b,g(c),:,:}] + \\epsilon}}$$\n",
    "\n",
    "$$y_{b,c,h,w} = \\gamma_c \\cdot \\hat{x}_{b,c,h,w} + \\beta_c$$\n",
    "\n",
    "Where:\n",
    "- $g(c)$ represents the group containing channel $c$\n",
    "- $\\mathrm{E}[x_{b,g(c),:,:}]$ is mean over channels in group $g(c)$ and spatial dimensions\n",
    "- Each group contains $C/G$ channels\n",
    "\n",
    "**Key Advantage:** Stable training regardless of batch size, beneficial for small batches.\n",
    "\n",
    "## SyncBatchNorm\n",
    "\n",
    "**Definition:** Synchronized BatchNorm across multiple GPUs/devices.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Computes statistics across all GPUs in distributed training\n",
    "- Requires communication between processes during forward/backward pass\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Same as BatchNorm, but statistics are aggregated across devices:\n",
    "\n",
    "$$\\mathrm{E}_{global}[x_{:,c,:,:}] = \\frac{1}{N_{global}} \\sum_{i=1}^{D} N_i \\cdot \\mathrm{E}_i[x_{:,c,:,:}]$$\n",
    "\n",
    "$$\\mathrm{Var}_{global}[x_{:,c,:,:}] = \\frac{1}{N_{global}} \\sum_{i=1}^{D} N_i \\cdot (\\mathrm{Var}_i[x_{:,c,:,:}] + (\\mathrm{E}_i[x_{:,c,:,:}] - \\mathrm{E}_{global}[x_{:,c,:,:}])^2)$$\n",
    "\n",
    "Where:\n",
    "- $D$ is number of devices\n",
    "- $N_i$ is samples on device $i$\n",
    "- $N_{global}$ is total samples across devices\n",
    "\n",
    "## Instance Normalization\n",
    "\n",
    "### InstanceNorm1d, InstanceNorm2d, InstanceNorm3d\n",
    "\n",
    "**Definition:** Normalizes each instance in batch independently.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For InstanceNorm2d with input $x \\in \\mathbb{R}^{B \\times C \\times H \\times W}$:\n",
    "\n",
    "$$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mathrm{E}[x_{b,c,:,:}]}{\\sqrt{\\mathrm{Var}[x_{b,c,:,:}] + \\epsilon}}$$\n",
    "\n",
    "$$y_{b,c,h,w} = \\gamma_c \\cdot \\hat{x}_{b,c,h,w} + \\beta_c$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{E}[x_{b,c,:,:}] = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{b,c,h,w}$\n",
    "- $\\mathrm{Var}[x_{b,c,:,:}]$ calculated over spatial dimensions only\n",
    "\n",
    "**InstanceNorm1d/3d:** Analogous formulations for respective dimensions.\n",
    "\n",
    "**Primary Application:** Style transfer, where normalizing per instance removes style information.\n",
    "\n",
    "## Lazy Instance Normalization\n",
    "\n",
    "### LazyInstanceNorm1d, LazyInstanceNorm2d, LazyInstanceNorm3d\n",
    "\n",
    "**Definition:** Instance normalization with lazy parameter initialization.\n",
    "\n",
    "**Implementation Details:**\n",
    "- Same mathematical operation as regular InstanceNorm\n",
    "- Infers feature dimensions on first forward pass\n",
    "- Initializes parameters dynamically based on channel count\n",
    "\n",
    "## Layer Normalization\n",
    "\n",
    "**Definition:** Normalizes across feature dimensions, not batch dimension.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $x \\in \\mathbb{R}^{B \\times C \\times H \\times W}$:\n",
    "\n",
    "$$\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mathrm{E}[x_{b,:,:,:}]}{\\sqrt{\\mathrm{Var}[x_{b,:,:,:}] + \\epsilon}}$$\n",
    "\n",
    "$$y_{b,c,h,w} = \\gamma_{c,h,w} \\cdot \\hat{x}_{b,c,h,w} + \\beta_{c,h,w}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{E}[x_{b,:,:,:}]$ is mean over all feature dimensions (C,H,W) for sample $b$\n",
    "- For 1D data (NLP): $\\hat{x}_{b,l,d} = \\frac{x_{b,l,d} - \\mathrm{E}[x_{b,l,:}]}{\\sqrt{\\mathrm{Var}[x_{b,l,:}] + \\epsilon}}$\n",
    "\n",
    "**Primary Application:** Transformer architectures, RNNs, where batch statistics are unstable.\n",
    "\n",
    "## Local Response Normalization\n",
    "\n",
    "**Definition:** Normalizes across adjacent feature maps/channels.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input tensor $x$ and channel index $i$:\n",
    "\n",
    "$$y_{i} = \\frac{x_{i}}{\\left(k + \\alpha \\sum_{j=\\max(0,i-n/2)}^{\\min(N-1,i+n/2)} x_{j}^{2}\\right)^{\\beta}}$$\n",
    "\n",
    "Where:\n",
    "- $n$ is normalization window size\n",
    "- $k, \\alpha, \\beta$ are hyperparameters\n",
    "- Normalization across adjacent channels instead of spatial locations\n",
    "\n",
    "**Historical Context:** Used in AlexNet, less common in modern architectures.\n",
    "\n",
    "## RMSNorm\n",
    "\n",
    "**Definition:** Root Mean Square Layer Normalization, simplified version of LayerNorm.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "For input vector $x \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "$$\\hat{x} = \\frac{x}{\\mathrm{RMS}(x) + \\epsilon}$$\n",
    "\n",
    "$$y = \\gamma \\odot \\hat{x}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathrm{RMS}(x) = \\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_i^2}$\n",
    "- $\\gamma$ are learnable parameters\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "\n",
    "**Key Advantage:** Computational efficiency by omitting mean centering, while maintaining most normalization benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nxP8t7bwDpV"
   },
   "source": [
    "# Dropout Layers in Neural Networks\n",
    "\n",
    "## Dropout\n",
    "\n",
    "### Definition\n",
    "Dropout is a regularization technique that prevents overfitting by randomly deactivating neurons during training with probability $p$. This forces the network to learn redundant representations and prevents co-adaptation of neurons.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For an input vector $\\mathbf{x}$, dropout applies a binary mask $\\mathbf{m}$ where each element is drawn from a Bernoulli distribution:\n",
    "\n",
    "$$\\mathbf{m}_i \\sim \\text{Bernoulli}(1-p)$$\n",
    "\n",
    "The forward pass becomes:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{m} \\odot \\mathbf{x}$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication. During inference, no neurons are dropped, but outputs are scaled:\n",
    "\n",
    "$$\\mathbf{y}_{\\text{inference}} = (1-p) \\cdot \\mathbf{x}$$\n",
    "\n",
    "Alternatively, during training the activations can be scaled by $\\frac{1}{1-p}$ (inverted dropout):\n",
    "\n",
    "$$\\mathbf{y}_{\\text{train}} = \\frac{\\mathbf{m} \\odot \\mathbf{x}}{1-p}$$\n",
    "\n",
    "This allows direct use during inference without scaling.\n",
    "\n",
    "## Dropout1d\n",
    "\n",
    "### Definition\n",
    "Dropout1d applies channel-wise dropout to inputs of shape $(N, C, L)$ where $N$ is batch size, $C$ is channels, and $L$ is sequence length.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input tensor $\\mathbf{X} \\in \\mathbb{R}^{N \\times C \\times L}$, generates mask $\\mathbf{M} \\in \\mathbb{R}^{N \\times C \\times 1}$:\n",
    "\n",
    "$$\\mathbf{M}_{i,j,1} \\sim \\text{Bernoulli}(1-p)$$\n",
    "\n",
    "The output becomes:\n",
    "\n",
    "$$\\mathbf{Y}_{i,j,k} = \\mathbf{X}_{i,j,k} \\cdot \\mathbf{M}_{i,j,1}$$\n",
    "\n",
    "This drops entire channels across the spatial dimension, enforcing feature-level regularization rather than individual neuron dropout.\n",
    "\n",
    "## Dropout2d\n",
    "\n",
    "### Definition\n",
    "Dropout2d applies channel-wise dropout to inputs of shape $(N, C, H, W)$ where $H$ and $W$ are height and width dimensions.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input tensor $\\mathbf{X} \\in \\mathbb{R}^{N \\times C \\times H \\times W}$, generates mask $\\mathbf{M} \\in \\mathbb{R}^{N \\times C \\times 1 \\times 1}$:\n",
    "\n",
    "$$\\mathbf{M}_{i,j,1,1} \\sim \\text{Bernoulli}(1-p)$$\n",
    "\n",
    "The output becomes:\n",
    "\n",
    "$$\\mathbf{Y}_{i,j,k,l} = \\mathbf{X}_{i,j,k,l} \\cdot \\mathbf{M}_{i,j,1,1}$$\n",
    "\n",
    "This technique is especially effective for convolutional neural networks as it drops entire feature maps, promoting independence between feature detectors.\n",
    "\n",
    "## Dropout3d\n",
    "\n",
    "### Definition\n",
    "Dropout3d extends the channel-wise dropout concept to 5D tensors with shape $(N, C, D, H, W)$, where $D$ represents depth.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For input tensor $\\mathbf{X} \\in \\mathbb{R}^{N \\times C \\times D \\times H \\times W}$, generates mask $\\mathbf{M} \\in \\mathbb{R}^{N \\times C \\times 1 \\times 1 \\times 1}$:\n",
    "\n",
    "$$\\mathbf{M}_{i,j,1,1,1} \\sim \\text{Bernoulli}(1-p)$$\n",
    "\n",
    "The output becomes:\n",
    "\n",
    "$$\\mathbf{Y}_{i,j,k,l,m} = \\mathbf{X}_{i,j,k,l,m} \\cdot \\mathbf{M}_{i,j,1,1,1}$$\n",
    "\n",
    "This implementation is particularly useful for 3D convolutions in medical imaging, video processing, and volumetric data analysis.\n",
    "\n",
    "## AlphaDropout\n",
    "\n",
    "### Definition\n",
    "AlphaDropout is designed specifically for Self-Normalizing Neural Networks (SNNs) using SELU activation. It maintains the mean and variance of activations before and after dropout.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For an input $\\mathbf{x}$ with SELU activation:\n",
    "\n",
    "$$\\alpha = 1.6732632423543772848170429916717$$\n",
    "$$\\lambda = 1.0507009873554804934193349852946$$\n",
    "\n",
    "AlphaDropout generates mask $\\mathbf{m}$ and transforms the input:\n",
    "\n",
    "$$\\mathbf{m}_i \\sim \\text{Bernoulli}(1-p)$$\n",
    "$$a = \\lambda\\alpha$$\n",
    "$$b = -\\lambda\\alpha$$\n",
    "\n",
    "The transformed output becomes:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{m} \\odot \\mathbf{x} + (1-\\mathbf{m}) \\odot \\alpha' \\cdot b$$\n",
    "\n",
    "Where $\\alpha'$ is calculated to preserve the self-normalizing property:\n",
    "\n",
    "$$\\alpha' = \\sqrt{\\frac{1-p+p\\alpha^2(1-p)}{1-p}}$$\n",
    "\n",
    "This ensures outputs maintain approximately zero mean and unit variance, preserving the self-normalizing property of SELU networks.\n",
    "\n",
    "## FeatureAlphaDropout\n",
    "\n",
    "### Definition\n",
    "FeatureAlphaDropout applies AlphaDropout at the feature level rather than individual neuron level, similar to how Dropout2d relates to Dropout.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For an input tensor $\\mathbf{X}$ with SELU activation, FeatureAlphaDropout applies AlphaDropout's transformation to entire feature channels:\n",
    "\n",
    "$$\\mathbf{M}_{i,j} \\sim \\text{Bernoulli}(1-p)$$\n",
    "\n",
    "Using the same $\\alpha$, $\\lambda$, $a$, $b$, and $\\alpha'$ values as AlphaDropout, the output becomes:\n",
    "\n",
    "$$\\mathbf{Y}_{i,j} = \\mathbf{M}_{i,j} \\odot \\mathbf{X}_{i,j} + (1-\\mathbf{M}_{i,j}) \\odot \\alpha' \\cdot b$$\n",
    "\n",
    "This implementation maintains the mean and variance across features while providing stronger regularization by dropping entire feature channels, particularly useful in deep self-normalizing neural networks with convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VysMLWQ5yieW"
   },
   "source": [
    "# Distance Functions\n",
    "\n",
    "## Definition\n",
    "\n",
    "Distance functions, also called metrics, are mathematical functions that define a notion of similarity or dissimilarity between elements in a vector space. Formally, a distance function $d: X \\times X \\rightarrow \\mathbb{R}$ must satisfy these axioms:\n",
    "\n",
    "1. Non-negativity: $d(x, y) \\geq 0$ for all $x, y \\in X$\n",
    "2. Identity of indiscernibles: $d(x, y) = 0$ if and only if $x = y$\n",
    "3. Symmetry: $d(x, y) = d(y, x)$ for all $x, y \\in X$\n",
    "4. Triangle inequality: $d(x, z) \\leq d(x, y) + d(y, z)$ for all $x, y, z \\in X$\n",
    "\n",
    "Distance functions serve as foundational components in numerous AI and machine learning algorithms, including clustering, classification, retrieval systems, and dimensionality reduction.\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "### Definition\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two non-zero vectors in an inner product space. This metric evaluates orientation similarity rather than magnitude, making it particularly effective for high-dimensional spaces.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For two vectors $\\mathbf{A}$ and $\\mathbf{B}$ in an $n$-dimensional space, cosine similarity is defined as:\n",
    "\n",
    "$$\\text{CosineSimilarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| \\cdot ||\\mathbf{B}||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{A} \\cdot \\mathbf{B}$ represents the dot product of vectors $\\mathbf{A}$ and $\\mathbf{B}$\n",
    "- $||\\mathbf{A}||$ and $||\\mathbf{B}||$ are the Euclidean norms (L2 norms) of vectors $\\mathbf{A}$ and $\\mathbf{B}$\n",
    "\n",
    "### Properties\n",
    "\n",
    "1. Range: Cosine similarity ranges from -1 to 1\n",
    "   - 1: Vectors point in the same direction (perfectly similar)\n",
    "   - 0: Vectors are orthogonal (unrelated)\n",
    "   - -1: Vectors point in opposite directions (perfectly dissimilar)\n",
    "\n",
    "2. Not a true metric as it doesn't satisfy the triangle inequality\n",
    "\n",
    "3. Invariant to scaling: $\\text{CosineSimilarity}(c\\mathbf{A}, d\\mathbf{B}) = \\text{CosineSimilarity}(\\mathbf{A}, \\mathbf{B})$ for any non-zero scalars $c$ and $d$\n",
    "\n",
    "4. To convert to a distance measure: $\\text{CosineDistance}(\\mathbf{A}, \\mathbf{B}) = 1 - \\text{CosineSimilarity}(\\mathbf{A}, \\mathbf{B})$\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Natural Language Processing: Document similarity, semantic search, word embeddings comparison\n",
    "- Recommendation Systems: User-item similarity calculation\n",
    "- Computer Vision: Image retrieval and comparison\n",
    "- Information Retrieval: Query-document matching\n",
    "\n",
    "## Pairwise Distance\n",
    "\n",
    "### Definition\n",
    "\n",
    "Pairwise distance refers to the computation of distances between pairs of points in a dataset, forming a distance matrix where each element $(i,j)$ represents the distance between points $i$ and $j$.\n",
    "\n",
    "### Common Pairwise Distance Metrics\n",
    "\n",
    "#### Euclidean Distance (L2 Norm)\n",
    "\n",
    "The straight-line distance between two points in Euclidean space.\n",
    "\n",
    "$$d_{euclidean}(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x} - \\mathbf{y}||_2 = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n",
    "\n",
    "#### Manhattan Distance (L1 Norm)\n",
    "\n",
    "The sum of absolute differences between corresponding coordinates.\n",
    "\n",
    "$$d_{manhattan}(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x} - \\mathbf{y}||_1 = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "#### Minkowski Distance (Lp Norm)\n",
    "\n",
    "A generalization of both Euclidean and Manhattan distances.\n",
    "\n",
    "$$d_{minkowski}(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x} - \\mathbf{y}||_p = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}$$\n",
    "\n",
    "Where:\n",
    "- $p = 1$: Manhattan distance\n",
    "- $p = 2$: Euclidean distance\n",
    "- $p = \\infty$: Chebyshev distance, $\\max_i |x_i - y_i|$\n",
    "\n",
    "#### Mahalanobis Distance\n",
    "\n",
    "Accounts for correlations between variables by incorporating the covariance matrix.\n",
    "\n",
    "$$d_{mahalanobis}(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{y})}$$\n",
    "\n",
    "Where $\\mathbf{\\Sigma}$ is the covariance matrix of the dataset.\n",
    "\n",
    "### Pairwise Distance Matrix\n",
    "\n",
    "For a dataset with $m$ points, the pairwise distance matrix $D$ is an $m \\times m$ matrix where:\n",
    "\n",
    "$$D_{ij} = d(\\mathbf{x}_i, \\mathbf{x}_j)$$\n",
    "\n",
    "For a symmetric distance function, $D$ is symmetric with zeros on the diagonal.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Clustering algorithms (k-means, hierarchical clustering, DBSCAN)\n",
    "- Dimensionality reduction (MDS, t-SNE, UMAP)\n",
    "- Nearest neighbor calculations\n",
    "- Anomaly detection\n",
    "- Phylogenetic tree construction\n",
    "- Similarity-based learning algorithms\n",
    "\n",
    "## Computational Considerations\n",
    "\n",
    "### Efficient Implementations\n",
    "\n",
    "1. Cosine Similarity matrix computation:\n",
    "   $$\\text{CosineSimilarity}(X) = \\frac{X X^T}{||X||_2 ||X||_2^T}$$\n",
    "\n",
    "2. Euclidean pairwise distance matrix using vector operations:\n",
    "   $$D_{ij}^2 = ||x_i||^2 + ||x_j||^2 - 2x_i \\cdot x_j$$\n",
    "\n",
    "3. Sparse vector optimizations:\n",
    "   - For sparse vectors, compute only over non-zero dimensions\n",
    "   - Utilize specialized sparse matrix libraries\n",
    "\n",
    "### Complexity Analysis\n",
    "\n",
    "- Cosine similarity between two $n$-dimensional vectors: $O(n)$\n",
    "- Pairwise distance matrix for $m$ points in $n$ dimensions: $O(m^2n)$\n",
    "- Approximate methods using locality-sensitive hashing: $O(m \\log m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-evqpIGzKOK"
   },
   "source": [
    "# Loss Functions\n",
    "\n",
    "## Definition\n",
    "\n",
    "Loss functions, also known as cost functions or objective functions, are mathematical functions that quantify the error between predicted values and actual values during model training. They map the difference between model outputs and ground truth targets to a scalar value that represents the \"cost\" of making incorrect predictions. Minimizing this cost through optimization algorithms enables the model to learn the underlying patterns in the data.\n",
    "\n",
    "## Regression Loss Functions\n",
    "\n",
    "### L1Loss (Mean Absolute Error)\n",
    "\n",
    "#### Definition\n",
    "L1Loss measures the average absolute difference between predictions and targets.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with predicted values $\\hat{y}$ and target values $y$:\n",
    "\n",
    "$$L_{L1}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "If reduction is set to 'sum', then:\n",
    "\n",
    "$$L_{L1}(y, \\hat{y}) = \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "#### Properties\n",
    "- Less sensitive to outliers compared to MSE\n",
    "- Produces a constant gradient magnitude regardless of error size\n",
    "- Non-differentiable at zero, requiring subgradient methods for optimization\n",
    "- Encourages sparsity in the solution\n",
    "- Penalizes all errors linearly\n",
    "\n",
    "#### Applications\n",
    "- Robust regression tasks\n",
    "- Computer vision for tasks like depth estimation\n",
    "- When outliers in the data should not dominate the loss\n",
    "\n",
    "### MSELoss (Mean Squared Error)\n",
    "\n",
    "#### Definition\n",
    "MSELoss calculates the average squared difference between predictions and targets.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with predicted values $\\hat{y}$ and target values $y$:\n",
    "\n",
    "$$L_{MSE}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "#### Properties\n",
    "- Differentiable everywhere, making it amenable to gradient-based optimization\n",
    "- Heavily penalizes large errors due to the squaring operation\n",
    "- Sensitive to outliers\n",
    "- Corresponds to maximum likelihood estimation under Gaussian noise assumptions\n",
    "- Gradient magnitude is proportional to the error\n",
    "\n",
    "#### Applications\n",
    "- General regression tasks\n",
    "- Linear regression models\n",
    "- Neural networks for continuous value prediction\n",
    "- Signal processing\n",
    "\n",
    "### HuberLoss\n",
    "\n",
    "#### Definition\n",
    "HuberLoss combines the best properties of MSE and MAE by being quadratic for small errors and linear for large errors.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with predicted values $\\hat{y}$ and target values $y$, and a threshold parameter $\\delta$:\n",
    "\n",
    "$$L_{Huber}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} L_\\delta(y_i - \\hat{y}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$L_\\delta(a) = \\begin{cases}\n",
    "\\frac{1}{2}a^2, & \\text{if } |a| \\leq \\delta \\\\\n",
    "\\delta(|a| - \\frac{1}{2}\\delta), & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Properties\n",
    "- Combines MSE and MAE characteristics\n",
    "- Less sensitive to outliers than MSE\n",
    "- Differentiable everywhere unlike L1Loss\n",
    "- Parameter $\\delta$ controls the transition point between quadratic and linear regions\n",
    "\n",
    "#### Applications\n",
    "- Robust regression tasks\n",
    "- Regression with noisy data containing outliers\n",
    "- Used in many reinforcement learning algorithms\n",
    "\n",
    "### SmoothL1Loss\n",
    "\n",
    "#### Definition\n",
    "SmoothL1Loss is a modification of HuberLoss with $\\delta=1$, providing a smoother transition between the linear and quadratic parts.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with predicted values $\\hat{y}$ and target values $y$:\n",
    "\n",
    "$$L_{SmoothL1}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} L_{smooth}(y_i - \\hat{y}_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$L_{smooth}(a) = \\begin{cases}\n",
    "0.5a^2, & \\text{if } |a| < 1 \\\\\n",
    "|a| - 0.5, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Properties\n",
    "- Specific case of Huber loss with $\\delta=1$\n",
    "- Less sensitive to outliers than MSE\n",
    "- Smoother gradient near zero than L1Loss\n",
    "- Computationally efficient\n",
    "\n",
    "#### Applications\n",
    "- Object detection networks (e.g., Fast R-CNN)\n",
    "- Regression tasks requiring robustness to outliers\n",
    "- Computer vision applications\n",
    "\n",
    "## Classification Loss Functions\n",
    "\n",
    "### CrossEntropyLoss\n",
    "\n",
    "#### Definition\n",
    "CrossEntropyLoss combines log-softmax and negative log-likelihood loss for multi-class classification problems.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with $C$ classes, predicted probability distributions $p$ and target class indices $y$:\n",
    "\n",
    "$$L_{CE}(y, p) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(p_{i,y_i})$$\n",
    "\n",
    "Where $p_{i,y_i}$ is the predicted probability for the true class $y_i$ of the $i$-th sample.\n",
    "\n",
    "With the softmax function applied to model outputs $z$:\n",
    "\n",
    "$$p_{i,c} = \\frac{\\exp(z_{i,c})}{\\sum_{j=1}^{C} \\exp(z_{i,j})}$$\n",
    "\n",
    "Combining these:\n",
    "\n",
    "$$L_{CE}(y, z) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log\\left(\\frac{\\exp(z_{i,y_i})}{\\sum_{j=1}^{C} \\exp(z_{i,j})}\\right)$$\n",
    "\n",
    "#### Properties\n",
    "- Penalizes confident wrong predictions more heavily\n",
    "- Numerical stability issues can occur; implementations typically use log-sum-exp tricks\n",
    "- Asymmetric: misclassifying A as B isn't treated the same as misclassifying B as A\n",
    "- Corresponds to minimizing the KL divergence between predicted and true distributions\n",
    "- Appropriate for mutually exclusive classes\n",
    "\n",
    "#### Applications\n",
    "- Standard loss for multi-class classification problems\n",
    "- Image classification networks\n",
    "- Natural language processing classification tasks\n",
    "\n",
    "### NLLLoss (Negative Log Likelihood Loss)\n",
    "\n",
    "#### Definition\n",
    "NLLLoss applies negative log-likelihood loss to inputs that have already gone through a log-softmax operation.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with $C$ classes, log-probabilities $\\log(p)$ and target class indices $y$:\n",
    "\n",
    "$$L_{NLL}(y, \\log(p)) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(p_{i,y_i})$$\n",
    "\n",
    "#### Properties\n",
    "- Usually paired with a prior LogSoftmax layer\n",
    "- CrossEntropyLoss combines LogSoftmax and NLLLoss into a single more efficient operation\n",
    "- Numerically more stable than computing raw probabilities\n",
    "\n",
    "#### Applications\n",
    "- Multi-class classification when logits have already been transformed with log-softmax\n",
    "- Often used in modular network designs where activation functions are separate from loss computation\n",
    "\n",
    "### BCELoss (Binary Cross Entropy Loss)\n",
    "\n",
    "#### Definition\n",
    "BCELoss measures the binary cross entropy between predicted probabilities and binary targets.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with predicted probabilities $\\hat{y} \\in [0, 1]$ and target values $y \\in \\{0, 1\\}$:\n",
    "\n",
    "$$L_{BCE}(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
    "\n",
    "#### Properties\n",
    "- Specific for binary classification problems\n",
    "- Requires input to be pre-processed with sigmoid function\n",
    "- Numerically unstable when predictions approach 0 or 1\n",
    "- Each sample can contribute independently to the loss\n",
    "\n",
    "#### Applications\n",
    "- Binary classification problems\n",
    "- Multi-label classification where each output is treated as an independent binary problem\n",
    "- Generative models like VAEs and GANs\n",
    "\n",
    "### BCEWithLogitsLoss\n",
    "\n",
    "#### Definition\n",
    "BCEWithLogitsLoss combines a sigmoid layer and BCELoss in one single class for improved numerical stability.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For a batch of size $N$ with raw model outputs $z$ and target values $y \\in \\{0, 1\\}$:\n",
    "\n",
    "$$L_{BCEL}(y, z) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i))]$$\n",
    "\n",
    "where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ is the sigmoid function.\n",
    "\n",
    "#### Properties\n",
    "- More numerically stable than using a separate sigmoid followed by BCELoss\n",
    "- Can use log-sum-exp tricks for stable computation\n",
    "- Allows for a weight parameter to deal with class imbalance\n",
    "- Automatically prevents problematic output values (0 or 1)\n",
    "\n",
    "#### Applications\n",
    "- Binary classification problems\n",
    "- Multi-label classification\n",
    "- Imbalanced datasets where positive and negative samples occur with different frequencies\n",
    "\n",
    "### KLDivLoss (Kullback-Leibler Divergence Loss)\n",
    "\n",
    "#### Definition\n",
    "KLDivLoss measures the relative entropy between two probability distributions, representing how one distribution diverges from another.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predicted log-probabilities $\\log(p)$ and target probabilities $q$:\n",
    "\n",
    "$$L_{KL}(q, \\log(p)) = \\sum_{i=1}^{N} q_i \\cdot (\\log(q_i) - \\log(p_i))$$\n",
    "\n",
    "#### Properties\n",
    "- Not symmetric: $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$\n",
    "- Always non-negative\n",
    "- Zero if and only if the distributions are identical\n",
    "- Input is expected to be log-probabilities for numerical stability\n",
    "- Target values should be probabilities, not classes\n",
    "\n",
    "#### Applications\n",
    "- Training generative models\n",
    "- Knowledge distillation\n",
    "- Distribution matching\n",
    "- Regularization in neural networks\n",
    "\n",
    "### PoissonNLLLoss\n",
    "\n",
    "#### Definition\n",
    "PoissonNLLLoss applies the negative log-likelihood loss for Poisson distribution.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predicted values $\\hat{y}$ and target values $y$:\n",
    "\n",
    "$$L_{Poisson}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} [\\hat{y}_i - y_i \\log(\\hat{y}_i) + \\log(y_i!)]$$\n",
    "\n",
    "If the full reduction is used, the constant term $\\log(y_i!)$ can be omitted:\n",
    "\n",
    "$$L_{Poisson}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} [\\hat{y}_i - y_i \\log(\\hat{y}_i)]$$\n",
    "\n",
    "#### Properties\n",
    "- Applicable when target values follow a Poisson distribution\n",
    "- Appropriate for count data (non-negative integers)\n",
    "- Can be used with non-integer target values as an approximation\n",
    "- Model output is expected to be the predicted expectation of the Poisson distribution\n",
    "\n",
    "#### Applications\n",
    "- Event count prediction\n",
    "- Time series forecasting for discrete events\n",
    "- Predicting rare event occurrences\n",
    "- Neuron firing rate prediction\n",
    "\n",
    "### GaussianNLLLoss\n",
    "\n",
    "#### Definition\n",
    "GaussianNLLLoss implements the negative log-likelihood loss for Gaussian distributions.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predicted means $\\mu$, predicted variances $\\sigma^2$, and target values $y$:\n",
    "\n",
    "$$L_{Gaussian}(y, \\mu, \\sigma^2) = \\frac{1}{2N} \\sum_{i=1}^{N} \\left[\\frac{(y_i - \\mu_i)^2}{\\sigma_i^2} + \\log(\\sigma_i^2) + \\log(2\\pi)\\right]$$\n",
    "\n",
    "#### Properties\n",
    "- Model outputs both the mean and variance of predictions\n",
    "- Allows the model to express uncertainty about its predictions\n",
    "- Balances fit quality and variance estimation\n",
    "- Penalizes overconfident wrong predictions\n",
    "\n",
    "#### Applications\n",
    "- Regression with uncertainty estimation\n",
    "- Heteroscedastic regression (where output variance depends on input)\n",
    "- Probabilistic forecasting\n",
    "- Bayesian neural networks\n",
    "\n",
    "### CTCLoss (Connectionist Temporal Classification Loss)\n",
    "\n",
    "#### Definition\n",
    "CTCLoss is designed for sequence-to-sequence learning problems without requiring aligned input-output pairs.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For an input sequence of length $T$, with $C$ classes and a target sequence $y$:\n",
    "\n",
    "$$L_{CTC}(y, \\hat{y}) = -\\log(p(y|\\hat{y}))$$\n",
    "\n",
    "Where $p(y|\\hat{y})$ is computed by summing over all possible alignments that can generate the target sequence:\n",
    "\n",
    "$$p(y|\\hat{y}) = \\sum_{\\pi \\in \\mathcal{B}^{-1}(y)} \\prod_{t=1}^{T} \\hat{y}_{\\pi_t}^t$$\n",
    "\n",
    "$\\mathcal{B}$ is a many-to-one mapping that removes repeated labels and blank symbols.\n",
    "\n",
    "#### Properties\n",
    "- Allows for variable-length input and output sequences\n",
    "- Doesn't require pre-aligned target sequences\n",
    "- Uses dynamic programming for efficient computation\n",
    "- Introduces a blank symbol to handle alignments\n",
    "- Directly optimizes sequence-level objectives\n",
    "\n",
    "#### Applications\n",
    "- Speech recognition\n",
    "- Handwriting recognition\n",
    "- Optical character recognition (OCR)\n",
    "- Any sequence recognition task without explicit alignment\n",
    "\n",
    "## Ranking and Metric Learning Losses\n",
    "\n",
    "### MarginRankingLoss\n",
    "\n",
    "#### Definition\n",
    "MarginRankingLoss creates a criterion that measures the loss given inputs $x_1$, $x_2$, and a label $y$ where $y = 1$ indicates $x_1$ should be ranked higher than $x_2$ and $y = -1$ indicates the opposite.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For inputs $x_1$, $x_2$, label $y \\in \\{-1, 1\\}$, and margin $m$:\n",
    "\n",
    "$$L_{margin}(x_1, x_2, y) = \\max(0, -y \\cdot (x_1 - x_2) + m)$$\n",
    "\n",
    "#### Properties\n",
    "- Enforces a margin between ranked items\n",
    "- Only penalizes violations of the desired ranking\n",
    "- Encourages correct orderings, not absolute values\n",
    "- Parameter $m$ controls the margin size required between items\n",
    "\n",
    "#### Applications\n",
    "- Learning to rank\n",
    "- Information retrieval\n",
    "- Recommendation systems\n",
    "- Preference learning\n",
    "\n",
    "### TripletMarginLoss\n",
    "\n",
    "#### Definition\n",
    "TripletMarginLoss measures the relative similarity between an anchor, a positive example, and a negative example.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For an anchor $a$, positive example $p$, negative example $n$, distance function $d$, and margin $m$:\n",
    "\n",
    "$$L_{triplet}(a, p, n) = \\max(0, d(a, p) - d(a, n) + m)$$\n",
    "\n",
    "#### Properties\n",
    "- Creates embeddings where similar examples are closer than dissimilar ones\n",
    "- Utilizes the concept of relative distances rather than absolute values\n",
    "- Margin parameter controls the minimum difference between positive and negative distances\n",
    "- Typically uses Euclidean distance, but can use other metrics\n",
    "\n",
    "#### Applications\n",
    "- Face recognition\n",
    "- Person re-identification\n",
    "- Image retrieval\n",
    "- Sentence embeddings in NLP\n",
    "\n",
    "### TripletMarginWithDistanceLoss\n",
    "\n",
    "#### Definition\n",
    "An extension of TripletMarginLoss that allows flexible distance functions beyond the default Euclidean distance.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For an anchor $a$, positive example $p$, negative example $n$, custom distance function $d$, and margin $m$:\n",
    "\n",
    "$$L_{triplet\\_dist}(a, p, n) = \\max(0, d(a, p) - d(a, n) + m)$$\n",
    "\n",
    "#### Properties\n",
    "- Generalizes TripletMarginLoss to support custom distance metrics\n",
    "- Can leverage domain-specific distance functions\n",
    "- Maintains the same margin-based learning approach\n",
    "- More flexible for specialized embedding spaces\n",
    "\n",
    "#### Applications\n",
    "- Specialized metric learning tasks\n",
    "- Learning embeddings with non-Euclidean geometry\n",
    "- Applications requiring specific similarity notions (e.g., cosine similarity)\n",
    "\n",
    "### CosineEmbeddingLoss\n",
    "\n",
    "#### Definition\n",
    "CosineEmbeddingLoss measures the cosine similarity between paired inputs and encourages them to be similar or dissimilar based on a target label.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For inputs $x_1$, $x_2$, target $y \\in \\{-1, 1\\}$, and margin $m$:\n",
    "\n",
    "$$L_{cosine}(x_1, x_2, y) = \\begin{cases}\n",
    "1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\\n",
    "\\max(0, \\cos(x_1, x_2) - m), & \\text{if } y = -1\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\cos(x_1, x_2) = \\frac{x_1 \\cdot x_2}{||x_1|| \\cdot ||x_2||}$\n",
    "\n",
    "#### Properties\n",
    "- Uses cosine similarity, focusing on directional similarity rather than magnitude\n",
    "- Useful when dealing with high-dimensional spaces\n",
    "- Invariant to scaling of the input vectors\n",
    "- Different loss calculations for similar and dissimilar pairs\n",
    "\n",
    "#### Applications\n",
    "- Learning semantic similarity in NLP\n",
    "- Cross-modal embedding learning\n",
    "- Learning document similarities\n",
    "- Feature matching across domains\n",
    "\n",
    "### MultiMarginLoss\n",
    "\n",
    "#### Definition\n",
    "MultiMarginLoss applies a multi-class hinge loss, generalizing the binary SVM loss to multiple classes.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predicted scores $x$ of dimension $C$ (classes), target class $y$, and margin $m$:\n",
    "\n",
    "$$L_{multi}(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{C-1} \\sum_{j \\neq y_i} \\max(0, m - (x_{i,y_i} - x_{i,j}))$$\n",
    "\n",
    "#### Properties\n",
    "- Enforces a margin between the score of the correct class and all other classes\n",
    "- Penalizes only when margin is violated\n",
    "- Parameter $p$ controls the norm used (1 or 2)\n",
    "- Weight parameter can handle class imbalance\n",
    "\n",
    "#### Applications\n",
    "- Multi-class classification with support vector machines\n",
    "- Structured prediction problems\n",
    "- Maximum-margin learning\n",
    "- Alternative to cross-entropy for classification tasks\n",
    "\n",
    "### HingeEmbeddingLoss\n",
    "\n",
    "#### Definition\n",
    "HingeEmbeddingLoss measures whether two inputs are similar or dissimilar and is typically used for nonlinear embeddings.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For an input $x$ and target $y \\in \\{-1, 1\\}$ with margin $m$:\n",
    "\n",
    "$$L_{hinge}(x, y) = \\begin{cases}\n",
    "x, & \\text{if } y = 1 \\\\\n",
    "\\max(0, m - x), & \\text{if } y = -1\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Properties\n",
    "- Often used after a distance measure between two embeddings\n",
    "- For $y=1$, minimizes the distance; for $y=-1$, pushes the distance beyond the margin\n",
    "- Linear penalty for similar samples\n",
    "- Ignores dissimilar samples once they are beyond the margin\n",
    "\n",
    "#### Applications\n",
    "- Siamese networks\n",
    "- Learning embeddings for retrieval\n",
    "- One-shot learning\n",
    "- Similarity learning\n",
    "\n",
    "### MultiLabelMarginLoss\n",
    "\n",
    "#### Definition\n",
    "MultiLabelMarginLoss optimizes a multi-class, multi-classification hinge loss where each sample can have multiple correct labels.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predicted scores $x$ and target labels $y$ (where positive values indicate true labels):\n",
    "\n",
    "$$L_{mlm}(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{C} \\sum_{j=1}^{C} \\sum_{k: y_{i,k} > 0} \\max(0, 1 - (x_{i,k} - x_{i,j}))$$\n",
    "\n",
    "Where the inner sum is over all positive targets $k$ and all targets $j$ where $y_{i,j} \\leq 0$.\n",
    "\n",
    "#### Properties\n",
    "- Supports multi-label classification\n",
    "- Enforces a margin between scores of correct and incorrect classes\n",
    "- Penalizes only when margin is violated\n",
    "- Can handle multiple correct classes for each sample\n",
    "\n",
    "#### Applications\n",
    "- Multi-label classification\n",
    "- Scene classification with multiple objects\n",
    "- Document tagging\n",
    "- Attribute recognition\n",
    "\n",
    "### SoftMarginLoss\n",
    "\n",
    "#### Definition\n",
    "SoftMarginLoss creates a criterion that optimizes a two-class classification logistic loss.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predictions $x$ and targets $y \\in \\{-1, 1\\}$:\n",
    "\n",
    "$$L_{soft}(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\log(1 + \\exp(-y_i x_i))$$\n",
    "\n",
    "#### Properties\n",
    "- Smooth approximation of the hinge loss\n",
    "- Continuously differentiable\n",
    "- Provides a probability interpretation\n",
    "- Similar to logistic regression loss\n",
    "\n",
    "#### Applications\n",
    "- Binary classification\n",
    "- Support vector machines with probabilistic output\n",
    "- Alternative to hinge loss when smooth gradients are preferred\n",
    "\n",
    "### MultiLabelSoftMarginLoss\n",
    "\n",
    "#### Definition\n",
    "MultiLabelSoftMarginLoss creates a criterion that optimizes a multi-label, multi-class classification sigmoid loss.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "For predicted scores $x$ and binary target vectors $y$:\n",
    "\n",
    "$$L_{mlsm}(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{C} \\sum_{j=1}^{C} \\left[ y_{i,j} \\log(1 + \\exp(-x_{i,j})) + (1 - y_{i,j}) \\log(1 + \\exp(x_{i,j})) \\right]$$\n",
    "\n",
    "#### Properties\n",
    "- Extension of binary sigmoid cross-entropy to multiple labels\n",
    "- Each output dimension is treated as an independent binary classification problem\n",
    "- Smooth and continuous loss function\n",
    "- Weights can be applied to handle class imbalance\n",
    "\n",
    "#### Applications\n",
    "- Multi-label classification problems\n",
    "- Tag prediction\n",
    "- Scene classification\n",
    "- Any task requiring non-exclusive class assignments\n",
    "\n",
    "## Comparison and Selection Guidelines\n",
    "\n",
    "### Loss Function Selection Criteria\n",
    "\n",
    "1. **Task Type**:\n",
    "   - Regression: MSELoss, L1Loss, HuberLoss, SmoothL1Loss\n",
    "   - Binary Classification: BCELoss, BCEWithLogitsLoss, SoftMarginLoss\n",
    "   - Multi-class Classification: CrossEntropyLoss, NLLLoss, MultiMarginLoss\n",
    "   - Multi-label Classification: BCELoss (per label), MultiLabelSoftMarginLoss\n",
    "   - Ranking/Similarity: TripletMarginLoss, CosineEmbeddingLoss, MarginRankingLoss\n",
    "\n",
    "2. **Data Distribution**:\n",
    "   - Gaussian noise: MSELoss\n",
    "   - Poisson-distributed data: PoissonNLLLoss\n",
    "   - Heavy-tailed distributions: L1Loss, HuberLoss\n",
    "   - Probability distributions: KLDivLoss, CrossEntropyLoss\n",
    "\n",
    "3. **Robustness Requirements**:\n",
    "   - Robustness to outliers: L1Loss, HuberLoss, SmoothL1Loss\n",
    "   - Emphasis on hard examples: Margin-based losses\n",
    "\n",
    "4. **Optimization Considerations**:\n",
    "   - Gradient stability: LogSoftmax + NLLLoss vs raw Softmax + NLLLoss\n",
    "   - Continuous gradients: SoftMarginLoss vs HingeLoss\n",
    "   - Numerical stability: BCEWithLogitsLoss vs separate Sigmoid + BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9C_rKudrgWu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOEUmme6tWEJuJyFfrr3Wff",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
