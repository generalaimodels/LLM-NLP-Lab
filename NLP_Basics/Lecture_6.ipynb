{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOHxu/hoa7qJmRs33GwPzkM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Language Modeling: A Comprehensive Overview\n","\n","Language modeling is a foundational task in natural language processing (NLP) that underpins many applications, including machine translation, text generation, and speech recognition. Below, we provide an in-depth explanation of language modeling, covering its definition, mathematical formulations, core principles, pros and cons, and recent advancements in the field.\n","\n","---\n","\n","## 1. Definition of Language Modeling\n","\n","Language modeling is the task of predicting the next word (or token) in a sequence of words, given the preceding context. It is essentially a probabilistic framework that assigns a probability distribution over a vocabulary of possible words (or tokens) for the next word in a sequence.\n","\n","### Example:\n","Consider the sentence:  \n","*\"The students opened their ______\"*  \n","\n","The goal of a language model is to predict the most likely word to fill in the blank, such as \"books,\" \"laptops,\" or \"minds,\" by computing a probability distribution over all possible words in the vocabulary.\n","\n","More formally, given a sequence of words $ x_1, x_2, \\ldots, x_t $, the language model computes the probability distribution of the next word $ x_{t+1} $:\n","\n","$$ P(x_{t+1} | x_1, x_2, \\ldots, x_t) $$\n","\n","where $ x_{t+1} $ can be any word in the vocabulary $ V $.\n","\n","### Broader Perspective:\n","A language model can also be thought of as a system that assigns a probability to an entire piece of text. For a sequence of words $ x_1, x_2, \\ldots, x_n $, the joint probability of the text is computed as:\n","\n","$$ P(x_1, x_2, \\ldots, x_n) = \\prod_{t=1}^n P(x_t | x_1, x_2, \\ldots, x_{t-1}) $$\n","\n","This decomposition leverages the chain rule of probability, making language modeling a sequential prediction problem.\n","\n","---\n","\n","## 2. Mathematical Equations and Formulations\n","\n","Language modeling is inherently a probabilistic task, and its mathematical foundation is based on probability theory and statistical modeling. Below are the key equations and formulations:\n","\n","### 2.1 Probability of a Sequence\n","The joint probability of a sequence $ x_1, x_2, \\ldots, x_n $ is expressed using the chain rule of probability:\n","\n","$$ P(x_1, x_2, \\ldots, x_n) = P(x_1) \\cdot P(x_2 | x_1) \\cdot P(x_3 | x_1, x_2) \\cdots P(x_n | x_1, x_2, \\ldots, x_{n-1}) $$\n","\n","This formulation implies that a language model must learn to estimate conditional probabilities of the form $ P(x_t | x_1, x_2, \\ldots, x_{t-1}) $.\n","\n","### 2.2 Objective Function: Maximum Likelihood Estimation (MLE)\n","The goal of training a language model is to maximize the likelihood of the observed data (i.e., a corpus of text). For a dataset $ D $ containing sequences of words, the likelihood is:\n","\n","$$ L(\\theta) = \\prod_{s \\in D} P(x_1, x_2, \\ldots, x_n; \\theta) = \\prod_{s \\in D} \\prod_{t=1}^n P(x_t | x_1, x_2, \\ldots, x_{t-1}; \\theta) $$\n","\n","where $ \\theta $ represents the parameters of the model (e.g., weights of a neural network). In practice, we maximize the log-likelihood for numerical stability:\n","\n","$$ \\log L(\\theta) = \\sum_{s \\in D} \\sum_{t=1}^n \\log P(x_t | x_1, x_2, \\ldots, x_{t-1}; \\theta) $$\n","\n","The corresponding loss function, known as the negative log-likelihood (NLL), is minimized:\n","\n","$$ \\mathcal{L}(\\theta) = - \\frac{1}{|D|} \\sum_{s \\in D} \\sum_{t=1}^n \\log P(x_t | x_1, x_2, \\ldots, x_{t-1}; \\theta) $$\n","\n","### 2.3 Perplexity\n","Perplexity is a commonly used metric to evaluate language models. It measures how well a model predicts a sample of text, with lower perplexity indicating better performance. For a test sequence $ x_1, x_2, \\ldots, x_n $, perplexity is defined as:\n","\n","$$ \\text{Perplexity} = 2^{-\\frac{1}{n} \\sum_{t=1}^n \\log_2 P(x_t | x_1, x_2, \\ldots, x_{t-1})} $$\n","\n","Intuitively, perplexity represents the effective number of choices the model considers for the next word, with lower values implying the model is more confident in its predictions.\n","\n","---\n","\n","## 3. Core Principles of Language Modeling\n","\n","Language modeling relies on several core principles, which guide the design and training of models:\n","\n","### 3.1 Sequential Dependency\n","Language models assume that words in a sequence are not independent; the probability of a word depends on the preceding context (i.e., Markovian or non-Markovian dependencies). Early models, such as n-gram models, used a fixed context window (e.g., bigrams, trigrams), while modern neural models can capture long-range dependencies using architectures like transformers.\n","\n","### 3.2 Representation Learning\n","Modern language models learn dense, continuous representations (embeddings) of words or tokens. These embeddings capture semantic and syntactic relationships, enabling the model to generalize to unseen data. For example, words like \"king\" and \"queen\" are represented in a way that reflects their similarity.\n","\n","### 3.3 Generalization\n","Language models must generalize to unseen sequences, which requires learning patterns from large, diverse datasets. Overfitting is a challenge, addressed through techniques like regularization, dropout, and large-scale pretraining.\n","\n","### 3.4 Autoregressive Modeling\n","Most language models are autoregressive, meaning they predict the next word based on the previous words in a left-to-right manner. This is in contrast to masked language models (e.g., BERT), which predict masked words in a bidirectional context (though such models are not true generative language models).\n","\n","### 3.5 Scalability\n","Modern language models, especially large language models (LLMs), rely on massive computational resources and datasets. Scaling laws suggest that increasing model size, data, and compute leads to better performance, though with diminishing returns.\n","\n","---\n","\n","## 4. Pros and Cons of Language Modeling\n","\n","### 4.1 Pros\n","- **Versatility**: Language models are foundational to many NLP tasks, including machine translation, text generation, sentiment analysis, and dialogue systems.\n","- **Transfer Learning**: Pretrained language models (e.g., BERT, GPT) can be fine-tuned for specific tasks, reducing the need for task-specific data.\n","- **Contextual Understanding**: Modern models, especially transformers, capture long-range dependencies and contextual nuances, enabling more coherent and accurate predictions.\n","- **Scalability**: Large-scale models can generalize to diverse domains and languages, making them widely applicable.\n","- **Human-like Generation**: Advanced models can generate fluent, human-like text, enabling applications like chatbots and automated content creation.\n","\n","### 4.2 Cons\n","- **Computational Cost**: Training and deploying large language models require significant computational resources, making them inaccessible to smaller organizations.\n","- **Data Dependency**: High performance relies on massive, high-quality datasets, which may contain biases or sensitive information.\n","- **Bias and Ethics**: Language models can perpetuate biases present in the training data, leading to ethical concerns (e.g., generating biased or harmful content).\n","- **Lack of True Understanding**: Despite their fluency, language models lack true semantic understanding or reasoning capabilities, relying instead on statistical patterns.\n","- **Overfitting to Training Data**: Models may memorize training data, leading to poor generalization, especially on out-of-distribution data.\n","- **Interpretability**: Neural language models, especially transformers, are often considered \"black boxes,\" making it difficult to understand their decision-making process.\n","\n","---\n","\n","## 5. Recent Advancements in Language Modeling\n","\n","Language modeling has seen significant advancements in recent years, driven by innovations in model architectures, training paradigms, and hardware. Below are some key developments:\n","\n","### 5.1 Transformer-Based Models\n","The introduction of the transformer architecture (Vaswani et al., 2017) revolutionized language modeling. Transformers use self-attention mechanisms to capture long-range dependencies, replacing earlier recurrent neural networks (RNNs) and long short-term memory (LSTM) models. Key transformer-based models include:\n","\n","- **GPT (Generative Pretrained Transformer)**: Autoregressive models trained to predict the next word in a sequence. Variants include GPT-2, GPT-3, and GPT-4, with increasing scale and performance.\n","- **BERT (Bidirectional Encoder Representations from Transformers)**: While not a traditional generative language model, BERT uses masked language modeling to predict masked words, enabling bidirectional context understanding.\n","- **T5 (Text-to-Text Transfer Transformer)**: Frames all NLP tasks as text-to-text problems, unifying language modeling with tasks like translation and summarization.\n","\n","### 5.2 Large Language Models (LLMs)\n","The scaling of language models to billions or even trillions of parameters has led to remarkable performance gains. Examples include:\n","\n","- **GPT-3**: A 175-billion-parameter model capable of few-shot learning, where it performs tasks with minimal task-specific training data.\n","- **PaLM (Pathways Language Model)**: A 540-billion-parameter model with improved reasoning and multilingual capabilities.\n","- **LLaMA (Large Language and Multimodal Assistant)**: Open-source models that achieve competitive performance with smaller parameter counts through efficient training.\n","\n","### 5.3 Efficient Training Techniques\n","To address the computational cost of training large models, several techniques have emerged:\n","\n","- **Mixed-Precision Training**: Uses lower-precision arithmetic (e.g., FP16) to reduce memory usage and speed up training.\n","- **Parameter-Efficient Fine-Tuning (PEFT)**: Methods like LoRA (Low-Rank Adaptation) and adapters enable fine-tuning large models with minimal additional parameters.\n","- **Sparse Models**: Techniques like mixture-of-experts (MoE) reduce computational cost by activating only a subset of parameters during inference.\n","\n","### 5.4 Multilingual and Cross-Lingual Models\n","Recent models aim to support multiple languages, reducing the need for language-specific models. Examples include:\n","\n","- **mBERT**: A multilingual version of BERT trained on text from over 100 languages.\n","- **XLM-R (Cross-Lingual Language Model - RoBERTa)**: A robust multilingual model for cross-lingual transfer learning.\n","\n","### 5.5 In-Context Learning and Prompting\n","Modern LLMs like GPT-3 and PaLM support in-context learning, where the model performs tasks based on examples provided in the input prompt, without fine-tuning. This paradigm has enabled few-shot and zero-shot learning, significantly increasing flexibility.\n","\n","### 5.6 Ethical and Responsible AI\n","Recent advancements also focus on mitigating biases and improving the ethical use of language models:\n","\n","- **Bias Detection and Mitigation**: Techniques to identify and reduce biases in model predictions, such as fairness-aware training and debiasing algorithms.\n","- **Energy Efficiency**: Efforts to reduce the carbon footprint of training large models, including energy-efficient hardware and algorithms.\n","\n","### 5.7 Integration with Other Modalities\n","Language models are increasingly integrated with other modalities, such as vision and audio, to create multimodal systems. Examples include:\n","\n","- **CLIP (Contrastive Language-Image Pretraining)**: Combines language and vision to enable tasks like image captioning.\n","- **DALL-E**: Uses language models to generate images from text prompts.\n","\n","### 5.8 Compression and Deployment\n","To deploy large models in resource-constrained environments, techniques like model quantization, pruning, and knowledge distillation are used to compress models without significant performance loss.\n","\n","---\n","\n","## 6. Conclusion\n","\n","Language modeling is a cornerstone of modern NLP, with applications ranging from text generation to dialogue systems. Its mathematical foundation lies in probability theory, and its practical success is driven by advances in neural architectures, particularly transformers. While language models offer remarkable capabilities, they also face challenges related to computational cost, bias, and interpretability. Recent advancements, such as large-scale models, efficient training techniques, and multimodal integration, continue to push the boundaries of what language models can achieve, making them a vibrant area of research and application in artificial intelligence."],"metadata":{"id":"1unQQHgKK4RI"}},{"cell_type":"markdown","source":["# n-gram Language Models\n","\n","## Definition and Foundations\n","\n","n-gram language models are statistical language models based on the Markov assumption that the probability of a word depends only on the previous $n-1$ words rather than the entire history. These models approximate the joint probability of a sequence by conditioning each word on its preceding context of fixed length.\n","\n","### Core Definition\n","An n-gram is a contiguous sequence of $n$ items (characters, words, or tokens) from a given text. In language modeling:\n","- Unigram ($n=1$): single word model\n","- Bigram ($n=2$): two-word model\n","- Trigram ($n=3$): three-word model\n","- and so on for higher values of $n$\n","\n","## Mathematical Formulation\n","\n","### Chain Rule Decomposition\n","For a sequence of words $w_1, w_2, ..., w_m$, the joint probability is:\n","\n","$$P(w_1, w_2, ..., w_m) = \\prod_{i=1}^{m} P(w_i|w_1, w_2, ..., w_{i-1})$$\n","\n","### Markov Assumption\n","The n-gram model makes the approximation that a word's probability depends only on the $n-1$ previous words:\n","\n","$$P(w_i|w_1, w_2, ..., w_{i-1}) \\approx P(w_i|w_{i-(n-1)}, ..., w_{i-1})$$\n","\n","### Probability Computation\n","The probability of an n-gram divided by the probability of its history (the first $n-1$ words):\n","\n","$$P(w_i|w_{i-(n-1)}, ..., w_{i-1}) = \\frac{P(w_{i-(n-1)}, ..., w_{i-1}, w_i)}{P(w_{i-(n-1)}, ..., w_{i-1})}$$\n","\n","### Maximum Likelihood Estimation\n","Using count statistics from a corpus:\n","\n","$$P(w_i|w_{i-(n-1)}, ..., w_{i-1}) = \\frac{C(w_{i-(n-1)}, ..., w_{i-1}, w_i)}{C(w_{i-(n-1)}, ..., w_{i-1})}$$\n","\n","Where $C()$ represents the count function.\n","\n","## Estimation Methods\n","\n","### Count-Based Estimation\n","1. **Direct counting**:\n","   $$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$\n","\n","2. **Handling sentence boundaries**: Special tokens like $\\langle \\text{s} \\rangle$ (start) and $\\langle \\text{/s} \\rangle$ (end)\n","\n","3. **Unknown words**: Using $\\langle \\text{UNK} \\rangle$ token:\n","   $$P(\\langle \\text{UNK} \\rangle|w_{i-1}) = \\frac{C(w_{i-1}, \\langle \\text{UNK} \\rangle)}{C(w_{i-1})}$$\n","\n","## Smoothing Techniques\n","\n","### Zero Probability Problem\n","Count-based methods assign zero probability to unseen n-grams, which is problematic when multiplying probabilities.\n","\n","### Laplace (Add-1) Smoothing\n","Add 1 to all counts:\n","\n","$$P_{Laplace}(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |V|}$$\n","\n","Where $|V|$ is vocabulary size.\n","\n","### Add-k Smoothing\n","Generalization of add-1, with fractional counts:\n","\n","$$P_{Add-k}(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + k|V|}$$\n","\n","### Backoff and Interpolation\n","\n","#### Backoff\n","Use lower-order n-grams when higher-order ones have insufficient data:\n","\n","$$P_{backoff}(w_i|w_{i-(n-1)}, ..., w_{i-1}) =\n","\\begin{cases}\n","\\alpha(w_{i-(n-1)}, ..., w_{i-1}) \\cdot P(w_i|w_{i-(n-2)}, ..., w_{i-1}) & \\text{if } C(w_{i-(n-1)}, ..., w_{i-1}, w_i) = 0 \\\\\n","P_{MLE}(w_i|w_{i-(n-1)}, ..., w_{i-1}) & \\text{otherwise}\n","\\end{cases}$$\n","\n","#### Interpolation\n","Linear combination of different order n-gram models:\n","\n","$$P_{interp}(w_i|w_{i-2}, w_{i-1}) = \\lambda_3 P(w_i|w_{i-2}, w_{i-1}) + \\lambda_2 P(w_i|w_{i-1}) + \\lambda_1 P(w_i)$$\n","\n","Where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$ and $\\lambda_i \\geq 0$\n","\n","### Kneser-Ney Smoothing\n","Advanced technique accounting for word distribution:\n","\n","$$P_{KN}(w_i|w_{i-1}) = \\frac{\\max(C(w_{i-1}, w_i) - d, 0)}{C(w_{i-1})} + \\lambda(w_{i-1})P_{continuation}(w_i)$$\n","\n","Where:\n","- $d$ is the discount parameter\n","- $\\lambda(w_{i-1})$ is a normalization constant\n","- $P_{continuation}(w_i)$ is the continuation probability\n","\n","$$P_{continuation}(w_i) = \\frac{|\\{w_{j-1}: C(w_{j-1}, w_i) > 0\\}|}{\\sum_{w'} |\\{w_{j-1}: C(w_{j-1}, w') > 0\\}|}$$\n","\n","## Variants of n-gram Models\n","\n","### Character-level n-grams\n","- Operate on character sequences instead of words\n","- Smaller vocabulary, longer sequences\n","- Mathematical formulation remains the same but with characters\n","\n","### Skip-grams\n","- Allow gaps in the sequence\n","- For example, a 2-skip bigram considers words separated by up to 2 other words\n","\n","$$P_{skip}(w_i|w_{i-k}) \\text{ for } 1 \\leq k \\leq n$$\n","\n","### Variable-length n-grams\n","- Adapt context length based on available data\n","- Use longer contexts when data supports it\n","\n","## Core Principles\n","\n","### Training Procedure\n","1. Collect large text corpus\n","2. Tokenize text into words/tokens\n","3. Extract n-grams from text\n","4. Calculate n-gram statistics\n","5. Apply smoothing techniques\n","6. Store model parameters (n-gram probabilities)\n","\n","### Computational Complexity\n","- Space complexity: $O(|V|^n)$ where $|V|$ is vocabulary size\n","- Training time complexity: $O(T)$ where $T$ is corpus size\n","- Inference time complexity: $O(1)$ for a single prediction\n","\n","### Context Limitation\n","- Fixed context window of $n-1$ words\n","- Limited ability to capture long-range dependencies\n","- Exponential sparsity with increasing $n$\n","\n","## Evaluation Metrics\n","\n","### Perplexity\n","Primary evaluation metric for language models:\n","\n","$$PP(W) = \\sqrt[N]{\\frac{1}{P(w_1, w_2, ..., w_N)}} = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1, ..., w_{i-1})}}$$\n","\n","Lower perplexity indicates better model performance.\n","\n","### Cross-Entropy\n","Related to perplexity:\n","\n","$$H(W) = -\\frac{1}{N}\\sum_{i=1}^{N}\\log_2 P(w_i|w_1, ..., w_{i-1})$$\n","\n","Perplexity is $2^{H(W)}$\n","\n","## Practical Implementations\n","\n","### Data Structures\n","- Hash tables for sparse n-gram storage\n","- Tries/prefix trees for efficient lookup\n","- Compressed data structures for large models\n","\n","### Pruning Techniques\n","- Count cutoffs: exclude n-grams below frequency threshold\n","- Entropy-based pruning: remove n-grams that contribute least to model performance\n","\n","## Pros and Cons\n","\n","### Advantages\n","- Conceptually simple and interpretable\n","- Efficient training and inference\n","- Works well for limited domains with sufficient data\n","- Minimal computational resources required\n","- Theoretically well-understood\n","\n","### Disadvantages\n","- Cannot capture long-range dependencies\n","- Suffers from data sparsity (exponential with n)\n","- Storage requirements grow exponentially with n\n","- Limited semantic understanding\n","- Poor generalization to unseen contexts\n","- Fixed context window\n","\n","## Transition to Neural Language Models\n","\n","### Limitations Addressed by Neural Models\n","- Curse of dimensionality\n","- Data sparsity\n","- Fixed-length context\n","- Lack of semantic generalization\n","\n","### Feed-Forward Neural Language Model\n","- Inputs: one-hot vectors or embeddings for $n-1$ previous words\n","- Hidden layers: typically dense with non-linear activations\n","- Output: probability distribution over vocabulary\n","\n","$$P(w_t|w_{t-(n-1)}, ..., w_{t-1}) = \\text{softmax}(W_2 \\cdot \\text{tanh}(W_1 \\cdot [e(w_{t-(n-1)}); ...; e(w_{t-1})]) + b_2) + b_1)$$\n","\n","Where:\n","- $e(w_i)$ is the embedding of word $w_i$\n","- $W_1, W_2, b_1, b_2$ are learnable parameters\n","- $[;]$ denotes concatenation\n","\n","### Recurrent Neural Models\n","Overcome fixed context limitation:\n","\n","$$h_t = f(h_{t-1}, e(w_t))$$\n","$$P(w_{t+1}|w_1, ..., w_t) = \\text{softmax}(W \\cdot h_t + b)$$\n","\n","### Transformer-Based Models\n","Now dominate language modeling with self-attention:\n","\n","$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n","\n","## Recent Advancements in n-gram Models\n","\n","### Neural-Symbolic Integration\n","- Combining n-gram statistics with neural representations\n","- Improved smoothing with neural techniques\n","- n-gram feature extraction for neural models\n","\n","### Subword n-grams\n","- BPE (Byte Pair Encoding) and WordPiece tokenization\n","- Mitigates vocabulary size issues\n","- Handles out-of-vocabulary words\n","\n","### Efficient n-gram Deployment\n","- Compressed n-gram models for mobile devices\n","- Pruned models maintaining performance\n","- Quantized representations\n","\n","### Hybrid Models\n","- n-gram caching for neural LMs\n","- Ensemble methods combining statistical and neural approaches\n","- Continuous-space n-gram models\n","\n","### Applications in Modern Systems\n","- Spelling correction and input prediction\n","- Low-resource language modeling\n","- Domain-specific applications (legal, medical)\n","- Fallback mechanisms in production systems"],"metadata":{"id":"6Ae254GML45U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lq3WS40yKy2T"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Tuple\n","\n","\n","class LSTMCell(nn.Module):\n","    \"\"\"Custom LSTM Cell implementation from scratch\"\"\"\n","\n","    def __init__(self, input_size: int, hidden_size: int):\n","        super(LSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # Weight matrices for input transformations\n","        self.W_i = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n","        self.W_f = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n","        self.W_c = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n","        self.W_o = nn.Parameter(torch.Tensor(input_size + hidden_size, hidden_size))\n","\n","        # Bias terms\n","        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n","        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n","        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n","        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n","\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self) -> None:\n","        \"\"\"Initialize weights using Xavier/Glorot initialization\"\"\"\n","        nn.init.xavier_uniform_(self.W_i)\n","        nn.init.xavier_uniform_(self.W_f)\n","        nn.init.xavier_uniform_(self.W_c)\n","        nn.init.xavier_uniform_(self.W_o)\n","        nn.init.zeros_(self.b_i)\n","        nn.init.zeros_(self.b_f)\n","        nn.init.zeros_(self.b_c)\n","        nn.init.zeros_(self.b_o)\n","\n","    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"Forward pass of LSTM cell\n","\n","        Args:\n","            x: Input tensor of shape (batch_size, input_size)\n","            state: Tuple of (hidden_state, cell_state) from previous time step\n","\n","        Returns:\n","            Tuple of (new_hidden_state, new_cell_state)\n","        \"\"\"\n","        h_prev, c_prev = state\n","\n","        # Concatenate previous hidden state and input\n","        combined = torch.cat((h_prev, x), dim=1)  # Shape: (batch_size, input_size + hidden_size)\n","\n","        # Input gate\n","        i_t = torch.sigmoid(combined @ self.W_i + self.b_i)\n","\n","        # Forget gate\n","        f_t = torch.sigmoid(combined @ self.W_f + self.b_f)\n","\n","        # Cell update\n","        c_tilde = torch.tanh(combined @ self.W_c + self.b_c)\n","\n","        # Update cell state\n","        c_t = f_t * c_prev + i_t * c_tilde\n","\n","        # Output gate\n","        o_t = torch.sigmoid(combined @ self.W_o + self.b_o)\n","\n","        # Update hidden state\n","        h_t = o_t * torch.tanh(c_t)\n","\n","        return h_t, c_t\n","\n","\n","class LSTMEncoder(nn.Module):\n","    \"\"\"Encoder using custom LSTM implementation\"\"\"\n","\n","    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 1):\n","        super(LSTMEncoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # Create LSTM layers\n","        self.lstm_cells = nn.ModuleList([\n","            LSTMCell(input_size if i == 0 else hidden_size, hidden_size)\n","            for i in range(num_layers)\n","        ])\n","\n","    def _init_hidden(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"Initialize hidden and cell states\"\"\"\n","        hidden = torch.zeros(batch_size, self.hidden_size, device=device)\n","        cell = torch.zeros(batch_size, self.hidden_size, device=device)\n","        return hidden, cell\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"Forward pass of encoder\n","\n","        Args:\n","            x: Input tensor of shape (batch_size, seq_len, input_size)\n","\n","        Returns:\n","            Tuple of (final_hidden_state, final_cell_state)\n","        \"\"\"\n","        batch_size, seq_len, _ = x.size()\n","        device = x.device\n","\n","        # Initialize states for each layer\n","        states = [\n","            self._init_hidden(batch_size, device)\n","            for _ in range(self.num_layers)\n","        ]\n","\n","        # Process sequence\n","        for t in range(seq_len):\n","            input_t = x[:, t, :]\n","\n","            # Process through each layer\n","            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n","                if layer_idx == 0:\n","                    states[layer_idx] = lstm_cell(input_t, states[layer_idx])\n","                else:\n","                    states[layer_idx] = lstm_cell(states[layer_idx-1][0], states[layer_idx])\n","\n","        # Return final states from the last layer\n","        return states[-1]\n","\n","\n","class LSTMDecoder(nn.Module):\n","    \"\"\"Decoder using custom LSTM implementation\"\"\"\n","\n","    def __init__(self, hidden_size: int, output_size: int, num_layers: int = 1):\n","        super(LSTMDecoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.num_layers = num_layers\n","\n","        # Create LSTM layers\n","        self.lstm_cells = nn.ModuleList([\n","            LSTMCell(hidden_size, hidden_size)\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Output projection\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","        # Projection layer to match decoder input dimensions\n","        self.input_proj = nn.Linear(output_size, hidden_size)\n","\n","    def forward(self, x: torch.Tensor, states: Tuple[torch.Tensor, torch.Tensor], seq_len: int) -> torch.Tensor:\n","        \"\"\"Forward pass of decoder\n","\n","        Args:\n","            x: Initial input tensor of shape (batch_size, hidden_size)\n","            states: Initial states from encoder\n","            seq_len: Length of output sequence\n","\n","        Returns:\n","            Output tensor of shape (batch_size, seq_len, output_size)\n","        \"\"\"\n","        batch_size = x.size(0)\n","        outputs = []\n","\n","        # Initial states for each layer\n","        layer_states = [states] * self.num_layers\n","        current_input = x\n","\n","        for _ in range(seq_len):\n","            # Process through each layer\n","            for layer_idx, lstm_cell in enumerate(self.lstm_cells):\n","                layer_states[layer_idx] = lstm_cell(\n","                    current_input,\n","                    layer_states[layer_idx]\n","                )\n","                current_input = layer_states[layer_idx][0]\n","\n","            # Project to output space\n","            output = self.fc(current_input)\n","            outputs.append(output)\n","\n","            # Project output back to hidden_size for next step\n","            current_input = self.input_proj(output)\n","\n","        # Stack outputs\n","        outputs = torch.stack(outputs, dim=1)  # (batch_size, seq_len, output_size)\n","        return outputs\n","\n","\n","class Seq2SeqTransfer(nn.Module):\n","    \"\"\"Sequence-to-sequence model for transfer learning\"\"\"\n","\n","    def __init__(self, input_size: int, hidden_size: int, output_size: int, num_layers: int = 1):\n","        super(Seq2SeqTransfer, self).__init__()\n","        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers)\n","        self.decoder = LSTMDecoder(hidden_size, output_size, num_layers)\n","\n","        # Input projection for initial decoder input\n","        self.decoder_input_proj = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, source_seq: torch.Tensor, target_seq_len: int) -> torch.Tensor:\n","        \"\"\"Forward pass of seq2seq model\n","\n","        Args:\n","            source_seq: Input sequence of shape (batch_size, source_seq_len, input_size)\n","            target_seq_len: Length of target sequence\n","\n","        Returns:\n","            Output tensor of shape (batch_size, target_seq_len, output_size)\n","        \"\"\"\n","        # Encode source sequence\n","        encoder_states = self.encoder(source_seq)\n","\n","        # Prepare initial decoder input\n","        batch_size = source_seq.size(0)\n","        device = source_seq.device\n","        decoder_input = torch.zeros(batch_size, self.decoder.hidden_size, device=device)\n","        decoder_input = self.decoder_input_proj(decoder_input)\n","\n","        # Decode to target sequence\n","        decoder_output = self.decoder(decoder_input, encoder_states, target_seq_len)\n","\n","        return decoder_output\n","\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Hyperparameters\n","    input_size = 10\n","    hidden_size = 32\n","    output_size = 10\n","    num_layers = 2\n","    batch_size = 4\n","    source_seq_len = 5\n","    target_seq_len = 3\n","\n","    # Create model\n","    model = Seq2SeqTransfer(input_size, hidden_size, output_size, num_layers)\n","\n","    # Create dummy input\n","    source_seq = torch.randn(batch_size, source_seq_len, input_size)\n","\n","    # Forward pass\n","    output = model(source_seq, target_seq_len)\n","    print(f\"Output shape: {output.shape}\")  # Should be (batch_size, target_seq_len, output_size)"]},{"cell_type":"markdown","source":["# Multi-layer Deep Encoder-Decoder Machine Translation Networks\n","\n","## Definition\n","\n","Multi-layer deep encoder-decoder networks are neural architectures that transform sequences from one domain (e.g., sentences in English) to sequences in another domain (e.g., sentences in French). These models consist of two primary components: an encoder that processes the input sequence into a continuous representation, and a decoder that generates the output sequence based on this representation.\n","\n","## Core Principles of Sequence-to-Sequence Models\n","\n","- **Encoder**: Processes input sequence $X = (x_1, x_2, ..., x_n)$ into hidden representations\n","- **Decoder**: Generates output sequence $Y = (y_1, y_2, ..., y_m)$ using encoded information\n","- **Transfer learning**: Information flows from source to target language through latent representations\n","- **End-to-end training**: Entire model is optimized jointly to maximize translation probability\n","\n","## Why Attention? Sequence-to-Sequence: The Bottleneck Problem\n","\n","### The Bottleneck Problem\n","\n","In traditional sequence-to-sequence models, the encoder compresses the entire input sequence into a fixed-length vector:\n","\n","$$c = h_n = f(x_1, x_2, ..., x_n)$$\n","\n","This creates a bottleneck as:\n","\n","- The fixed-size vector must contain all information about the source sequence\n","- Information degradation becomes severe for longer sequences\n","- Early tokens' information fades due to vanishing gradients\n","- Model performance degrades significantly for long sequences\n","\n","## Attention\n","\n","### Conceptual Understanding\n","\n","Attention mechanisms address the bottleneck problem by:\n","- Allowing the decoder to \"focus\" on different parts of the source sequence\n","- Creating direct connections between decoder states and encoder states\n","- Dynamically weighting the importance of source tokens for each output token\n","\n","### Sequence-to-Sequence with Attention\n","\n","#### Encoder RNN\n","\n","The encoder creates a sequence of hidden states:\n","\n","$$h_j = \\text{EncoderRNN}(x_j, h_{j-1})$$\n","\n","Each hidden state $h_j$ represents information about the input at position $j$ contextualized by previous inputs.\n","\n","#### Decoder with Attention\n","\n","Instead of using only the last hidden state, the decoder:\n","1. Computes attention weights for each encoder hidden state\n","2. Creates a context vector as a weighted sum of encoder states\n","3. Uses this context vector alongside its own hidden state to generate output\n","\n","## Attention: In Equations\n","\n","### Attention Score Computation\n","\n","For a decoder state $s_i$ and encoder states $h_1, h_2, ..., h_n$:\n","\n","$$e_{ij} = \\text{score}(s_{i-1}, h_j)$$\n","\n","The score function can be implemented in various ways:\n","\n","1. **Dot product attention**:\n","   $$e_{ij} = s_{i-1}^T h_j$$\n","\n","2. **General attention**:\n","   $$e_{ij} = s_{i-1}^T W_a h_j$$\n","\n","3. **Additive/concat attention**:\n","   $$e_{ij} = v_a^T \\tanh(W_a[s_{i-1}; h_j])$$\n","\n","### Attention Weights\n","\n","The scores are normalized using softmax:\n","\n","$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{n} \\exp(e_{ik})}$$\n","\n","### Context Vector\n","\n","The context vector is computed as:\n","\n","$$c_i = \\sum_{j=1}^{n} \\alpha_{ij} h_j$$\n","\n","### Output Generation\n","\n","The output distribution is computed using:\n","\n","$$p(y_i|y_1, ..., y_{i-1}, X) = \\text{softmax}(W_o[s_i; c_i])$$\n","\n","Where:\n","- $s_i = \\text{DecoderRNN}(y_{i-1}, s_{i-1}, c_i)$\n","- $W_o$ is a learnable parameter matrix\n","\n","## Attention is Great!\n","\n","### Key Benefits\n","\n","- **Solves the bottleneck problem**: No need to compress all information into one vector\n","- **Handles long-range dependencies**: Direct connections between related words\n","- **Provides interpretability**: Attention weights show which input tokens influence each output\n","- **Improves gradient flow**: Creates shorter paths for backpropagation\n","- **Enables better translation of rare words**: By focusing explicitly on their representations\n","\n","## Attention Variants\n","\n","### 1. Global vs. Local Attention\n","\n","- **Global attention**: Attends to all source positions\n","- **Local attention**: Attends only to a window of positions around predicted aligned position\n","\n","### 2. Hard vs. Soft Attention\n","\n","- **Soft attention**: Differentiable, uses weighted sum of all positions\n","- **Hard attention**: Stochastic, selects one position to attend to (requires reinforcement learning)\n","\n","### 3. Self-Attention (Intra-attention)\n","\n","- Relates different positions within the same sequence\n","- Forms the basis for Transformer architectures\n","- Computes attention between each element and all other elements in the sequence\n","\n","### 4. Multi-head Attention\n","\n","- Runs attention multiple times in parallel\n","- Each attention \"head\" learns different aspects of relationships\n","- Outputs are concatenated and linearly transformed\n","\n","$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n","\n","Where each head is computed as:\n","\n","$$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n","\n","### 5. Hierarchical Attention\n","\n","- Different levels of attention (word-level, sentence-level)\n","- Especially useful for document-level tasks\n","\n","## Attention as a General Deep Learning Technique\n","\n","Attention has evolved beyond machine translation to become a fundamental component in modern deep learning:\n","\n","- **Computer vision**: Understanding image regions (visual attention)\n","- **Speech recognition**: Focusing on relevant audio frames\n","- **Natural language understanding**: Creating contextualized representations\n","- **Multimodal learning**: Aligning elements across modalities\n","- **Graph neural networks**: Attending to relevant nodes in a graph\n","\n","## Recent Advancements\n","\n","- **Transformer architecture**: Replaced RNNs with self-attention for state-of-the-art results\n","- **Pre-trained language models**: Models like BERT, GPT and T5 heavily rely on attention\n","- **Efficient attention variants**: Sparse, linear, and local attention to reduce computational complexity\n","- **Attention pruning**: Dynamic mechanisms to attend only to relevant tokens\n","- **Rotary position embeddings (RoPE)**: Superior handling of positional information in attention\n","- **Multi-query attention**: Reduces memory requirements while maintaining performance\n","\n","## Pros and Cons\n","\n","### Pros\n","\n","- **Flexibility**: Adaptable to various sequence lengths\n","- **Parallelization**: Unlike RNNs, can be computed in parallel\n","- **Performance**: Consistently outperforms non-attention models\n","- **Interpretability**: Attention weights provide insights into model decisions\n","- **Versatility**: Works across diverse domains and tasks\n","\n","### Cons\n","\n","- **Computational complexity**: $O(n^2)$ for sequence length $n$ in standard attention\n","- **Memory requirements**: Stores attention matrices for all combinations\n","- **Training instability**: Can be sensitive to initialization and hyperparameters\n","- **Limited context length**: Practical limitations for very long sequences\n","- **Potential attention collapse**: Models may focus too much on certain patterns"],"metadata":{"id":"ochYof_YQH-1"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import Tuple, Optional\n","\n","class Encoder(nn.Module):\n","    \"\"\"Encoder module for sequence-to-sequence learning with customizable RNN.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        num_layers: int = 1,\n","        dropout: float = 0.1\n","    ) -> None:\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        # Embedding layer for input tokens\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        # GRU for encoding sequences\n","        self.gru = nn.GRU(\n","            embed_dim,\n","            hidden_dim,\n","            num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(\n","        self,\n","        src: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Forward pass of encoder.\n","\n","        Args:\n","            src: Input tensor of shape (batch_size, seq_len)\n","\n","        Returns:\n","            outputs: Encoder outputs of shape (batch_size, seq_len, hidden_dim)\n","            hidden: Final hidden state of shape (num_layers, batch_size, hidden_dim)\n","        \"\"\"\n","        # Embed input tokens\n","        embedded = self.dropout(self.embedding(src))  # (batch_size, seq_len, embed_dim)\n","\n","        # Pass through GRU\n","        outputs, hidden = self.gru(embedded)  # outputs: (batch_size, seq_len, hidden_dim)\n","\n","        return outputs, hidden\n","\n","\n","class Attention(nn.Module):\n","    \"\"\"Additive attention mechanism implementation.\"\"\"\n","\n","    def __init__(\n","        self,\n","        hidden_dim: int\n","    ) -> None:\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n","        self.v = nn.Linear(hidden_dim, 1, bias=False)\n","\n","    def forward(\n","        self,\n","        hidden: torch.Tensor,\n","        encoder_outputs: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Forward pass of attention mechanism.\n","\n","        Args:\n","            hidden: Decoder hidden state of shape (batch_size, hidden_dim)\n","            encoder_outputs: Encoder outputs of shape (batch_size, src_len, hidden_dim)\n","\n","        Returns:\n","            context: Context vector of shape (batch_size, hidden_dim)\n","            attention_weights: Attention weights of shape (batch_size, src_len)\n","        \"\"\"\n","        batch_size = encoder_outputs.size(0)\n","        src_len = encoder_outputs.size(1)\n","\n","        # Repeat decoder hidden state src_len times\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # (batch_size, src_len, hidden_dim)\n","\n","        # Calculate energy\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, src_len, hidden_dim)\n","\n","        # Calculate attention scores\n","        attention = self.v(energy).squeeze(2)  # (batch_size, src_len)\n","\n","        # Apply softmax to get attention weights\n","        attention_weights = F.softmax(attention, dim=1)  # (batch_size, src_len)\n","\n","        # Calculate context vector\n","        context = torch.bmm(\n","            attention_weights.unsqueeze(1),\n","            encoder_outputs\n","        ).squeeze(1)  # (batch_size, hidden_dim)\n","\n","        return context, attention_weights\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"Decoder module with attention mechanism.\"\"\"\n","\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        num_layers: int = 1,\n","        dropout: float = 0.1\n","    ) -> None:\n","        super().__init__()\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","\n","        # Embedding layer for output tokens\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","        # Attention mechanism\n","        self.attention = Attention(hidden_dim)\n","        # GRU for decoding\n","        self.gru = nn.GRU(\n","            embed_dim + hidden_dim,\n","            hidden_dim,\n","            num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","        # Output layer\n","        self.fc_out = nn.Linear(embed_dim + hidden_dim * 2, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(\n","        self,\n","        input: torch.Tensor,\n","        hidden: torch.Tensor,\n","        encoder_outputs: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Forward pass of decoder (single step).\n","\n","        Args:\n","            input: Input token of shape (batch_size)\n","            hidden: Previous hidden state of shape (num_layers, batch_size, hidden_dim)\n","            encoder_outputs: Encoder outputs of shape (batch_size, src_len, hidden_dim)\n","\n","        Returns:\n","            prediction: Output prediction of shape (batch_size, output_dim)\n","            hidden: New hidden state of shape (num_layers, batch_size, hidden_dim)\n","            attention_weights: Attention weights of shape (batch_size, src_len)\n","        \"\"\"\n","        # Add sequence dimension\n","        input = input.unsqueeze(1)  # (batch_size, 1)\n","\n","        # Embed input token\n","        embedded = self.dropout(self.embedding(input))  # (batch_size, 1, embed_dim)\n","\n","        # Get attention context vector\n","        top_hidden = hidden[-1]  # (batch_size, hidden_dim)\n","        context, attention_weights = self.attention(top_hidden, encoder_outputs)  # context: (batch_size, hidden_dim)\n","\n","        # Concatenate embedded input and context vector\n","        gru_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)  # (batch_size, 1, embed_dim + hidden_dim)\n","\n","        # Pass through GRU\n","        output, hidden = self.gru(gru_input, hidden)  # output: (batch_size, 1, hidden_dim)\n","\n","        # Prepare for output prediction\n","        embedded = embedded.squeeze(1)  # (batch_size, embed_dim)\n","        output = output.squeeze(1)  # (batch_size, hidden_dim)\n","        context = context.squeeze(1)  # (batch_size, hidden_dim)\n","\n","        # Predict next token\n","        prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))  # (batch_size, output_dim)\n","\n","        return prediction, hidden, attention_weights\n","\n","\n","class Seq2Seq(nn.Module):\n","    \"\"\"Sequence-to-Sequence model with attention mechanism.\"\"\"\n","\n","    def __init__(\n","        self,\n","        encoder: Encoder,\n","        decoder: Decoder,\n","        device: torch.device\n","    ) -> None:\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","        # Ensure encoder and decoder have compatible hidden dimensions\n","        assert encoder.hidden_dim == decoder.hidden_dim, \\\n","            \"Encoder and decoder hidden dimensions must match!\"\n","\n","    def forward(\n","        self,\n","        src: torch.Tensor,\n","        trg: torch.Tensor,\n","        teacher_forcing_ratio: float = 0.5\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Forward pass of sequence-to-sequence model.\n","\n","        Args:\n","            src: Source sequence of shape (batch_size, src_len)\n","            trg: Target sequence of shape (batch_size, trg_len)\n","            teacher_forcing_ratio: Probability of using teacher forcing\n","\n","        Returns:\n","            outputs: Decoder outputs of shape (batch_size, trg_len, output_dim)\n","        \"\"\"\n","        batch_size = src.size(0)\n","        trg_len = trg.size(1)\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        # Tensor to store decoder outputs\n","        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n","\n","        # Encode source sequence\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # First input to decoder is <sos> token\n","        input = trg[:, 0]\n","\n","        # Decode sequence\n","        for t in range(1, trg_len):\n","            # Get decoder output\n","            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n","\n","            # Store output\n","            outputs[:, t, :] = output\n","\n","            # Decide whether to use teacher forcing\n","            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n","\n","            # Get predicted token\n","            top1 = output.argmax(1)\n","\n","            # Use teacher forcing or predicted token\n","            input = trg[:, t] if teacher_force else top1\n","\n","        return outputs\n","\n","\n","# Example usage and initialization\n","if __name__ == \"__main__\":\n","    # # Check torch version and availability\n","    # assert torch.__version__ >= \"1.8.0\", \"PyTorch version 1.8.0 or higher required!\"\n","\n","    # Set device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Model hyperparameters\n","    INPUT_DIM = 5000  # Source vocabulary size\n","    OUTPUT_DIM = 5000  # Target vocabulary size\n","    ENC_EMB_DIM = 256\n","    DEC_EMB_DIM = 256\n","    HID_DIM = 512\n","    N_LAYERS = 2\n","    ENC_DROPOUT = 0.5\n","    DEC_DROPOUT = 0.5\n","\n","    # Initialize encoder and decoder\n","    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","    # Initialize sequence-to-sequence model\n","    model = Seq2Seq(enc, dec, device).to(device)\n","\n","    # Example forward pass with dummy data\n","    batch_size = 32\n","    src_len = 10\n","    trg_len = 12\n","\n","    src = torch.randint(0, INPUT_DIM, (batch_size, src_len)).to(device)\n","    trg = torch.randint(0, OUTPUT_DIM, (batch_size, trg_len)).to(device)\n","\n","    outputs = model(src, trg)\n","    print(f\"Output shape: {outputs.shape}\")  # Should be (batch_size, trg_len, OUTPUT_DIM)"],"metadata":{"id":"DjgfxyOLR13-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"--kskvG8VLVK"},"execution_count":null,"outputs":[]}]}