{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM5Ed/WqNv6iLNdXa8JBWy1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","**Definition of Language Modeling:**\n","\n","Language Modeling (LM) is the task of assigning a probability distribution over sequences of words or, more generally, tokens. Mathematically, given a sequence of tokens $ \\mathbf{w} = (w_1, w_2, ..., w_T) $, a language model calculates the probability $ P(\\mathbf{w}) $, or more commonly, the conditional probability of each token given the preceding tokens, as per the chain rule of probability:\n","\n","$$ P(\\mathbf{w}) = P(w_1, w_2, ..., w_T) = P(w_1) P(w_2|w_1) P(w_3|w_1, w_2) ... P(w_T|w_1, w_2, ..., w_{T-1}) = \\prod_{t=1}^{T} P(w_t|w_{<t}) $$\n","\n","where $ w_{<t} $ denotes the sequence of tokens preceding $ w_t $, i.e., $ (w_1, w_2, ..., w_{t-1}) $. The objective of a language model is to learn this probability distribution from a corpus of text data.\n","\n","**Language Modeling as a Task in NLP:**\n","\n","Language Modeling is not merely an isolated academic exercise; it is a foundational task within NLP with far-reaching applications. Its objective is to enable machines to understand, generate, and interact with human language in a statistically coherent manner.  Historically, approaches to Language Modeling ranged from n-gram models to probabilistic context-free grammars. However, with the advent of neural networks, a paradigm shift occurred towards more sophisticated, data-driven models.  The core concept lies in estimating the likelihood of a word sequence being \"natural\" or \"grammatical.\" In practical terms, a well-trained language model should assign a higher probability to sequences that are more likely to occur in natural language and lower probabilities to sequences that are less likely or nonsensical.\n","\n","Consider the applications of Language Modeling. In machine translation, a language model on the target language ensures the fluency and grammatical correctness of the translated output. In automatic speech recognition, it helps in choosing the most probable word sequence from a set of acoustic interpretations. Similarly, in text generation, an LM is crucial for generating coherent and contextually relevant text. For instance, in predictive text input, based on the preceding word sequence, a language model predicts the probability of the next word to suggest to the user.  Thus, Language Modeling serves as an essential component in systems that require understanding or generating human-like text.\n","\n","**Recurrent Neural Networks (RNNs) for Language Modeling:**\n","\n","Recurrent Neural Networks (RNNs) emerged as a particularly well-suited family of neural networks for tackling Language Modeling due to their inherent capability to process sequential data. Unlike feedforward neural networks that treat each input independently, RNNs maintain an internal state, or \"memory,\" that allows them to process sequences while taking into account the order of elements.\n","\n","The fundamental architecture of an RNN involves a recurrent connection that feeds the output of a neuron at time step $ t $ back into the input at time step $ t+1 $.  Let $ \\mathbf{x}_t $ be the input at time step $ t $, such as a word embedding for the $ t $-th word in a sequence. Let $ \\mathbf{h}_t $ be the hidden state at time $ t $, and $ \\mathbf{y}_t $ be the output at time $ t $.  A basic RNN performs the following computations:\n","\n","$$ \\mathbf{h}_t = f(\\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{b}_h) $$\n","$$ \\mathbf{y}_t = g(\\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y) $$\n","\n","where:\n","- $ \\mathbf{x}_t \\in \\mathbb{R}^{d_x} $ is the input vector at time $ t $.\n","- $ \\mathbf{h}_t \\in \\mathbb{R}^{d_h} $ is the hidden state vector at time $ t $. $ \\mathbf{h}_0 $ is typically initialized to a vector of zeros.\n","- $ \\mathbf{y}_t \\in \\mathbb{R}^{d_y} $ is the output vector at time $ t $.\n","- $ \\mathbf{W}_{xh} \\in \\mathbb{R}^{d_h \\times d_x} $, $ \\mathbf{W}_{hh} \\in \\mathbb{R}^{d_h \\times d_h} $, and $ \\mathbf{W}_{hy} \\in \\mathbb{R}^{d_y \\times d_h} $ are weight matrices.\n","- $ \\mathbf{b}_h \\in \\mathbb{R}^{d_h} $ and $ \\mathbf{b}_y \\in \\mathbb{R}^{d_y} $ are bias vectors.\n","- $ f $ and $ g $ are activation functions, commonly $ tanh $ or $ ReLU $ for $ f $, and $ softmax $ for $ g $ when the task is classification, such as predicting the next word in Language Modeling.\n","\n","In the context of Language Modeling, we can use an RNN to predict the probability of the next word $ w_t $ given the history $ w_{<t} $. We would input word embeddings $ \\mathbf{x}_t $ for each word $ w_t $ in the sequence. The output $ \\mathbf{y}_t $ at each time step can be interpreted as a probability distribution over the vocabulary. If the vocabulary size is $ V $, then $ \\mathbf{y}_t \\in \\mathbb{R}^{V} $, and after applying a softmax function, the $ i $-th component of $ \\mathbf{y}_t $ can represent $ P(w_t = v_i | w_{<t}) $, where $ v_i $ is the $ i $-th word in the vocabulary.  The model is trained by minimizing the negative log-likelihood of the actual next words in the training corpus. For a sequence $ \\mathbf{w} = (w_1, w_2, ..., w_T) $, the loss function is typically the cross-entropy loss:\n","\n","$$ L(\\theta) = - \\sum_{t=1}^{T} \\log P(w_t | w_1, ..., w_{t-1}; \\theta) $$\n","\n","where $ \\theta $ represents the parameters of the RNN (i.e., the weight matrices and bias vectors).  Training is performed using gradient descent algorithms, often backpropagation through time (BPTT), to update the parameters $ \\theta $ to minimize $ L(\\theta) $.\n","\n","**Problems with RNNs:**\n","\n","Despite their suitability for sequence modeling and initial success in Language Modeling, vanilla RNNs suffer from significant problems, primarily related to the training dynamics and their ability to capture long-range dependencies.\n","\n","**1. Vanishing and Exploding Gradients:**  During backpropagation through time, gradients are propagated backward through each time step.  Due to the repeated matrix multiplications in the recurrent connections (specifically $ \\mathbf{W}_{hh} $), gradients can either exponentially shrink (vanish) or exponentially grow (explode).  This is particularly problematic for long sequences.\n","\n","Consider the gradient of the loss function with respect to the hidden state at a distant time step $ k $ in the past, when computing gradients at time $ t > k $. This gradient involves a product of Jacobian matrices of the form $ \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} = \\frac{\\partial f}{\\partial \\mathbf{h}_{t-1}} \\mathbf{W}_{hh} $.  If the largest singular value of $ \\mathbf{W}_{hh} $ is less than 1, repeated multiplications will lead to vanishing gradients. If it's greater than 1, it can lead to exploding gradients.\n","\n","Vanishing gradients prevent the model from learning long-range dependencies because the error signal from later time steps does not propagate effectively back to earlier time steps. Exploding gradients, on the other hand, can cause numerical instability during training.\n","\n","**2. Difficulty in Capturing Long-Range Dependencies:**  As a consequence of vanishing gradients, standard RNNs struggle to retain information over long sequences. The influence of early parts of the sequence diminishes over time, limiting the model's ability to use context from far in the past when processing the current input.  For Language Modeling, this means that while an RNN might be good at predicting the next word based on the immediately preceding few words, it may fail to capture dependencies that span across sentences or paragraphs.\n","\n","**Recap on RNNs/LMs:**\n","\n","In summary, Recurrent Neural Networks provided a significant advancement for Language Modeling compared to earlier statistical methods. RNNs, by design, process sequences and maintain a state, making them naturally suited to model the conditional probability of a word given its history.  The mathematical formulation of RNNs involves recurrent connections encapsulated in hidden state updates and output computations, trained to minimize prediction error using backpropagation through time.\n","\n","**Pros of RNNs for Language Modeling:**\n","\n","- Ability to process sequences of arbitrary length.\n","- Capture context information through hidden states.\n","- Parameters are shared across time steps, leading to efficient learning.\n","- Demonstrated empirical success in various NLP tasks, including language modeling.\n","\n","**Cons of RNNs for Language Modeling:**\n","\n","- Vanishing and exploding gradient problems hinder training, especially for long sequences.\n","- Difficulty in capturing long-range dependencies due to gradient issues.\n","- Sequential computation limits parallelization and can be slow for long sequences.\n","\n","**Research Improvements and Further Directions:**\n","\n","To mitigate the problems of vanilla RNNs, several advancements have been proposed and have become standard practice in modern NLP.\n","\n","- **Long Short-Term Memory Networks (LSTMs) and Gated Recurrent Units (GRUs):** These are specialized types of RNNs designed to address the vanishing gradient problem. They introduce gating mechanisms that regulate the flow of information through the hidden state, allowing them to learn and retain long-range dependencies more effectively. Mathematically, LSTMs and GRUs introduce more complex update equations for the hidden state, involving gates that control what information to remember, forget, and output at each time step.\n","\n","- **Attention Mechanisms:** Attention mechanisms allow the model to weigh the importance of different parts of the input sequence when making predictions. In the context of RNNs, attention mechanisms can be used to selectively focus on relevant parts of the history when predicting the next word, effectively bypassing the bottleneck of compressing all historical information into a fixed-size hidden state.\n","\n","- **Transformers:**  The Transformer architecture, which is primarily based on attention mechanisms and dispenses with recurrence entirely, has revolutionized NLP and Language Modeling. Transformers offer better parallelization, overcome the vanishing gradient problem, and excel at capturing long-range dependencies. They have become the state-of-the-art for many language understanding and generation tasks.\n","\n","- **Addressing Exploding Gradients:** Techniques like gradient clipping are commonly used to stabilize training and prevent exploding gradients in RNNs. This involves scaling down gradients if their norm exceeds a certain threshold.\n","\n","In conclusion, while vanilla RNNs present inherent limitations, they served as a crucial stepping stone in the evolution of neural network-based Language Modeling.  The issues identified with RNNs spurred the development of more advanced architectures like LSTMs, GRUs, and Transformers, which have significantly improved the capability of models to understand and generate human language. Current research continues to focus on improving efficiency, handling even longer contexts, and enhancing the reasoning and understanding capabilities of language models."],"metadata":{"id":"z1IF6DTw6LlZ"}},{"cell_type":"markdown","source":["# Language Modeling\n","\n","## Definition and Mathematical Foundation\n","\n","Language modeling is the task of estimating the probability distribution over sequences of linguistic tokens. Given a sequence $w_1, w_2, \\ldots, w_T$, a language model computes the joint probability $P(w_1, w_2, \\ldots, w_T)$. According to probability theory, this joint distribution can be factorized using the chain rule:\n","\n","$$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^{T} P(w_t|w_1, w_2, \\ldots, w_{t-1})$$\n","\n","The computational objective is to minimize the negative log-likelihood of observed sequences:\n","\n","$$\\mathcal{L} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P(w_t|w_1, w_2, \\ldots, w_{t-1})$$\n","\n","Perplexity, a standard evaluation metric, is defined as:\n","\n","$$\\text{PPL} = 2^{\\mathcal{L}}$$\n","\n","Lower perplexity indicates better prediction capability, aligning with information theory principles where optimal models minimize encoding length.\n","\n","## N-gram Language Models\n","\n","N-gram language models implement the Markov assumption, which posits that future tokens depend only on a fixed number of previous tokens. For an n-gram model:\n","\n","$$P(w_t|w_1, \\ldots, w_{t-1}) \\approx P(w_t|w_{t-n+1}, \\ldots, w_{t-1})$$\n","\n","Maximum likelihood estimation computes these probabilities directly from corpus frequencies:\n","\n","$$P(w_t|w_{t-n+1}, \\ldots, w_{t-1}) = \\frac{C(w_{t-n+1}, \\ldots, w_{t-1}, w_t)}{C(w_{t-n+1}, \\ldots, w_{t-1})}$$\n","\n","Where $C(\\cdot)$ denotes count frequency in the training corpus.\n","\n","The n-gram approach suffers from data sparsity, requiring smoothing techniques. Kneser-Ney smoothing, a sophisticated approach, combines absolute discounting with context diversity:\n","\n","$$P_{KN}(w_i|w_{i-n+1}^{i-1}) = \\frac{\\max(C(w_{i-n+1}^i) - d, 0)}{\\sum_w C(w_{i-n+1}^{i-1}w)} + \\lambda(w_{i-n+1}^{i-1})P_{KN}(w_i|w_{i-n+2}^{i-1})$$\n","\n","Where $d$ is a discount parameter and $\\lambda(\\cdot)$ is a normalization coefficient.\n","\n","## Fixed-Window Neural Language Model\n","\n","Neural language models replace symbolic n-gram counting with distributed representations, enabling generalization across semantically similar contexts. In a fixed-window architecture:\n","\n","1. Each context word $w_i$ is mapped to an embedding vector:\n","   $$e(w_i) = E_{w_i}$$ where $E \\in \\mathbb{R}^{|V| \\times d}$\n","\n","2. Context words are concatenated or otherwise combined:\n","   $$x = [e(w_{t-n+1}); e(w_{t-n+2}); \\ldots; e(w_{t-1})]$$\n","\n","3. The representation passes through neural transformation:\n","   $$h = \\sigma(W_h x + b_h)$$\n","\n","4. A softmax layer produces next-token probabilities:\n","   $$P(w_t|w_{t-n+1}, \\ldots, w_{t-1}) = \\frac{\\exp(v_{w_t}^T h + b_{w_t})}{\\sum_{w' \\in V}\\exp(v_{w'}^T h + b_{w'})}$$\n","\n","The model is trained end-to-end using backpropagation with cross-entropy loss:\n","\n","$$\\mathcal{L}(\\theta) = -\\sum_{t=n}^T \\log P_\\theta(w_t|w_{t-n+1}, \\ldots, w_{t-1})$$\n","\n","The Bengio et al. (2003) model demonstrated that neural architectures could outperform n-gram models by capturing semantic relationships through learned word representations.\n","\n","## Technical Analysis\n","\n","**Advantages of n-gram models:**\n","- Computational efficiency ($O(|V|)$ for prediction)\n","- Interpretability of probabilities\n","- Effective with sufficient data for small n\n","\n","**Limitations of n-gram models:**\n","- Exponential parameter growth with n\n","- Inability to generalize to unseen n-grams\n","- Limited context window\n","\n","**Advantages of neural language models:**\n","- Generalization through distributed representations\n","- Parameter sharing across contexts\n","- Implicit smoothing through non-linear transformations\n","\n","**Limitations of fixed-window neural models:**\n","- Fixed context size limits long-range dependency modeling\n","- Computational complexity scales with window size\n","- Loss of token position information with simple concatenation\n","\n","Research directions focus on extending context handling through recurrent, convolutional, and attention-based architectures, enabling modeling of arbitrarily long dependencies while maintaining computational efficiency."],"metadata":{"id":"znJWtxkoBBki"}},{"cell_type":"markdown","source":["Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by maintaining an internal state, or memory, that reflects information from previous inputs in the sequence. Unlike feedforward neural networks that process inputs independently, RNNs leverage their sequential nature to handle inputs of varying lengths and capture temporal dependencies within the data.  Fundamentally, an RNN operates by iterating over the elements of an input sequence, where at each time step, it processes the current input and updates its internal state based on both the current input and the previous state.\n","\n","Mathematically, a basic RNN can be defined by the following set of equations for each time step $t$ in an input sequence $x = (x_1, x_2, ..., x_T)$:\n","\n","$$ h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h) $$\n","$$ o_t = W_{ho} h_t + b_o $$\n","$$ y_t = \\psi(o_t) $$\n","\n","where:\n","$x_t \\in \\mathbb{R}^{d_x}$ is the input at time step $t$.\n","$h_t \\in \\mathbb{R}^{d_h}$ is the hidden state at time step $t$, which acts as the memory of the network. $h_0$ is typically initialized to a vector of zeros or randomly.\n","$o_t \\in \\mathbb{R}^{d_o}$ is the output before the final activation function at time step $t$.\n","$y_t \\in \\mathbb{R}^{d_y}$ is the output at time step $t$.\n","$W_{xh} \\in \\mathbb{R}^{d_h \\times d_x}$ is the weight matrix for the input-to-hidden connection.\n","$W_{hh} \\in \\mathbb{R}^{d_h \\times d_h}$ is the weight matrix for the hidden-to-hidden recurrent connection.\n","$W_{ho} \\in \\mathbb{R}^{d_y \\times d_h}$ is the weight matrix for the hidden-to-output connection.\n","$b_h \\in \\mathbb{R}^{d_h}$ is the bias vector for the hidden state.\n","$b_o \\in \\mathbb{R}^{d_y}$ is the bias vector for the output.\n","$\\phi$ is the activation function for the hidden state (e.g., tanh, ReLU), introducing non-linearity.\n","$\\psi$ is the activation function for the output (e.g., softmax for classification, sigmoid for probability, or identity for regression), depending on the task.\n","\n","The core concept of recurrence is embodied in the term $W_{hh} h_{t-1}$, which signifies that the current hidden state $h_t$ is not only a function of the current input $x_t$ but also of the previous hidden state $h_{t-1}$. This recurrent connection allows information from earlier parts of the sequence to persist and influence the processing of later parts.  Unfolding an RNN over time reveals its chain-like structure, where each node in the chain is associated with a time step and receives input from the current input $x_t$ and the hidden state from the previous time step $h_{t-1}$.\n","\n","For language modeling, a simple RNN can be directly adapted to predict the next word in a sequence given the preceding words.  In the context of A Simple RNN Language Model, we are interested in modeling the probability distribution over sequences of words, as discussed previously in language modeling fundamentals. To build an RNN language model, we need to redefine the input and output of the RNN in terms of linguistic units, typically words or sub-word units.\n","\n","First, words are typically represented as dense vectors using word embeddings. Let $V$ be the vocabulary of words.  We can create an embedding matrix $E \\in \\mathbb{R}^{d_e \\times |V|}$, where $d_e$ is the embedding dimension and $|V|$ is the size of the vocabulary. For each word $w$ in the vocabulary, we have an embedding vector $e_w \\in \\mathbb{R}^{d_e}$. When processing a word sequence, the input at each time step $x_t$ becomes the embedding of the $t^{th}$ word in the sequence.  So if $w_t$ is the $t^{th}$ word, then $x_t = e_{w_t}$.\n","\n","In an RNN Language Model, the task at each time step $t$ is to predict the next word $w_{t+1}$ given the history of words up to $w_t$.  Therefore, the output $y_t$ of the RNN should represent a probability distribution over the vocabulary $V$.  This is achieved by setting the output dimension $d_y = |V|$ and using a softmax activation function $\\psi$ in the output layer. The output $o_t = W_{ho} h_t + b_o$ is passed through a softmax function to produce the probability distribution $y_t$:\n","\n","$$ y_t = \\text{softmax}(o_t) $$\n","\n","The $j^{th}$ component of $y_t$, denoted as $y_{t,j}$, represents the predicted probability of the $j^{th}$ word in the vocabulary being the next word in the sequence, given the history processed up to time $t$.  Mathematically,\n","\n","$$ y_{t,j} = P(w_{t+1} = v_j | w_1, w_2, ..., w_t) = \\frac{\\exp(o_{t,j})}{\\sum_{k=1}^{|V|} \\exp(o_{t,k})} $$\n","\n","where $v_j$ is the $j^{th}$ word in the vocabulary, and $o_{t,j}$ is the $j^{th}$ component of $o_t$.\n","\n","Thus, an RNN Language Model processes a sequence of word embeddings as input and, at each time step, outputs a probability distribution over the vocabulary, representing the model's prediction for the next word in the sequence. By sequentially processing the input and updating its hidden state, the RNN can, in principle, capture dependencies across the entire history of the input sequence, overcoming the fixed context window limitations of n-gram and fixed-window neural language models.\n","\n","When we talk about RNN Language Models in a broader context, it's essential to acknowledge that the \"simple RNN\" described above, often referred to as a vanilla RNN or Elman network, is just one architecture.  While theoretically capable of learning long-range dependencies, simple RNNs in practice often suffer from the vanishing gradient problem during training. This issue arises because gradients are backpropagated through time, and with each step back in time, the gradients can exponentially shrink, especially when using activation functions like tanh or sigmoid. This makes it difficult for simple RNNs to learn and retain information from inputs that are far in the past.\n","\n","To mitigate the vanishing gradient problem and to better capture long-range dependencies, more sophisticated RNN architectures have been developed, most notably Long Short-Term Memory networks (LSTMs) and Gated Recurrent Units (GRUs). LSTMs and GRUs introduce gating mechanisms within the RNN cell that control the flow of information through time. These gates allow the network to selectively remember or forget information over long sequences, making them much more effective at capturing long-range dependencies compared to simple RNNs.  While the fundamental principle of recurrence remains, LSTMs and GRUs replace the simple hidden state update with more complex cell state and gating operations.\n","\n","Finally, Training an RNN Language Model is typically done using cross-entropy loss.  Given a training corpus, we want to adjust the parameters of the RNN (weights and biases) and the word embeddings to maximize the probability of observing the actual word sequences in the corpus. This is equivalent to minimizing the negative log-likelihood of the data. For a given training sequence of words $(w_1, w_2, ..., w_m)$, the cross-entropy loss at time step $t$ is defined as the negative log probability of the true next word $w_{t+1}$ given the predicted probability distribution $y_t$:\n","\n","$$ L_t = - \\log P(w_{t+1} | w_1, w_2, ..., w_t) = - \\log y_{t, index(w_{t+1})} $$\n","\n","where $index(w_{t+1})$ is the index of the true next word $w_{t+1}$ in the vocabulary, and $y_{t, index(w_{t+1})}$ is the predicted probability for this word from the softmax output at time $t$.  The total loss for a sequence is the sum of the losses over all time steps:\n","\n","$$ L = \\sum_{t=1}^{m-1} L_t = - \\sum_{t=1}^{m-1} \\log y_{t, index(w_{t+1})} $$\n","\n","During training, we use optimization algorithms like stochastic gradient descent (SGD), or more commonly, variants like Adam, to minimize this loss function.  To compute gradients for parameter updates, we use Backpropagation Through Time (BPTT). BPTT is an extension of the standard backpropagation algorithm adapted for RNNs.  It involves unfolding the RNN over the length of the sequence and then performing backpropagation through this unfolded network. This process computes gradients of the loss with respect to all model parameters, including weights in $W_{xh}, W_{hh}, W_{ho}$, biases $b_h, b_o$, and potentially the word embedding matrix $E$ if embeddings are also learned during training.\n","\n","The pros of RNN language models, compared to n-gram and fixed-window neural models, are their ability to handle variable-length sequences and, in theory, capture long-range dependencies. However, simple RNNs suffer from vanishing gradients, limiting their practical capacity to learn very long-term dependencies.  LSTMs and GRUs substantially improve upon this limitation.  A con for all RNNs is that they process sequences sequentially, which can be slower compared to models that can process input in parallel.  For research improvement, attention mechanisms and transformers have emerged as powerful alternatives that address the limitations of RNNs, especially in capturing long-range dependencies and enabling parallel computation, becoming the dominant architecture in state-of-the-art language models. However, RNNs remain a foundational concept for understanding sequence processing and language modeling, highlighting the crucial role of recurrent connections and state maintenance for handling sequential data."],"metadata":{"id":"ap6a6Pf5CBGQ"}},{"cell_type":"markdown","source":["In the practical deployment of Recurrent Neural Network (RNN) language models, a significant computational bottleneck arises when considering training over an entire corpus in a single step.  The cumulative nature of RNNs, where hidden states at each time step depend on all preceding inputs, implies that processing a long sequence or the entire training corpus at once necessitates storing activations and intermediate computations for all time steps in memory. For a corpus of substantial size, this approach, termed batch gradient descent over the entire corpus, becomes exceptionally memory-intensive, often exceeding the capacity of available computational resources. Additionally, the computational cost of performing a forward and backward pass across such a large dataset before a weight update becomes prohibitively expensive in terms of time.\n","\n","To mitigate these computational challenges, a common strategy is to process the training data in smaller, manageable units, typically at the sentence or document level. Treating a sentence (or a short document) as a distinct training instance offers a practical compromise. It limits the length of sequences unrolled for backpropagation, thereby reducing memory consumption, while still allowing the RNN to learn sequential dependencies within the sentence context.\n","\n","Recalling the principles of Stochastic Gradient Descent (SGD), we recognize its utility in handling large datasets by iteratively updating model parameters based on gradients computed from small subsets of the data.  In the context of RNN language model training, SGD is applied by partitioning the training corpus into mini-batches of sentences (or documents). For each mini-batch, we compute the loss and gradients, and update the model parameters. This approach, known as mini-batch SGD, provides a computationally feasible way to train RNNs on large text corpora.  It offers a trade-off between the accuracy of gradient approximation (compared to full batch gradient descent) and the computational efficiency of updates.\n","\n","To train the parameters of RNNs, we employ Backpropagation Through Time (BPTT).  BPTT is the adapted form of the backpropagation algorithm for recurrent neural networks, designed to handle the temporal dimension inherent in sequential data.  Consider an RNN language model being trained on a sentence $W = (w_1, w_2, ..., w_T)$. To understand BPTT, we first visualize the RNN as being \"unrolled\" over the length of the sentence $T$. This unfolding creates a feedforward network that is $T$ steps deep, where each layer corresponds to a time step in the sequence.  At each time step $t$ (from $t=1$ to $T$), the RNN receives input $x_t$ (word embedding of $w_t$), calculates the hidden state $h_t$ based on $x_t$ and $h_{t-1}$, and produces an output $y_t$, which is the predicted probability distribution over the vocabulary for the next word.\n","\n","The loss function, typically cross-entropy, is computed at each time step by comparing the predicted probability distribution $y_t$ with the actual next word $w_{t+1}$ (for language modeling, we predict $w_{t+1}$ given context up to $w_t$). Let $L_t$ be the loss at time step $t$. The total loss for the sentence is the sum of losses at each time step, $L = \\sum_{t=1}^{T} L_t$. After computing the total loss for a sentence (or mini-batch of sentences), BPTT proceeds in reverse order through the unfolded network to calculate gradients of the loss with respect to all model parameters.\n","\n","Fundamentally, Backpropagation for RNNs relies on the Multivariable Chain Rule. The chain rule in calculus allows us to compute the derivative of a composite function. In the context of neural networks, specifically RNNs, we need to compute the derivatives of the loss function with respect to each parameter (weights and biases) of the network.  Because of the recurrent connections and the sequential nature of RNNs, the gradients at each time step depend on the gradients from subsequent time steps.\n","\n","Consider the parameters we need to train in a simple RNN language model: the input-to-hidden weight matrix $W_{xh}$, the hidden-to-hidden weight matrix $W_{hh}$, the hidden-to-output weight matrix $W_{ho}$, and bias vectors $b_h, b_o$.  The hidden state at time $t$ is given by $h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$, and the output before softmax is $o_t = W_{ho} h_t + b_o$. The loss at time $t$ is $L_t$. We want to compute gradients like $\\frac{\\partial L}{\\partial W_{ho}}$, $\\frac{\\partial L}{\\partial W_{hh}}$, $\\frac{\\partial L}{\\partial W_{xh}}$, $\\frac{\\partial L}{\\partial b_o}$, $\\frac{\\partial L}{\\partial b_h}$.\n","\n","Applying the multivariable chain rule, for example to compute $\\frac{\\partial L}{\\partial W_{ho}}$, we see that the output $o_t$ is directly dependent on $W_{ho}$. The loss $L$ is a function of all outputs $y_1, y_2, ..., y_T$, and each $y_t$ is derived from $o_t$. Therefore, we use the chain rule:\n","\n","$$ \\frac{\\partial L}{\\partial W_{ho}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial y_t} \\frac{\\partial y_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial W_{ho}} $$\n","\n","Similarly, for $\\frac{\\partial L}{\\partial W_{hh}}$, the influence of $W_{hh}$ on the loss is more indirect because $W_{hh}$ affects $h_t$, which then affects $o_t$ and finally $L_t$.  Also, $h_t$ depends on $h_{t-1}$, which in turn depends on $W_{hh}$ and so on for all preceding time steps. Thus, the gradient computation requires summing up contributions across all time steps and backpropagating through the recurrent connections:\n","\n","$$ \\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial y_t} \\frac{\\partial y_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{hh}} + \\sum_{t=2}^{T} \\frac{\\partial L}{\\partial y_t} \\frac{\\partial y_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial W_{hh}} + ... $$\n","\n","This expansion illustrates the \"through time\" aspect of backpropagation.  The gradients at time $t$ depend not only on the immediate computations at time $t$ but also on the history of computations from previous time steps due to the recurrent connections.  The process of BPTT effectively computes these gradients by backpropagating the error signal from the output layer back through each time step to the beginning of the sequence.\n","\n","In summary, the training procedure for RNN language models using BPTT and mini-batch SGD is as follows:\n","1. Divide the training corpus into mini-batches of sentences (or documents).\n","2. For each mini-batch:\n","    a. For each sentence in the mini-batch:\n","        i. Perform a forward pass through the unfolded RNN for the length of the sentence, computing hidden states $h_t$ and output probabilities $y_t$ at each time step.\n","        ii. Calculate the loss $L$ for the sentence, typically using cross-entropy.\n","    b. Sum the losses across all sentences in the mini-batch to get the total batch loss.\n","    c. Perform a backward pass (BPTT) to compute gradients of the batch loss with respect to all trainable parameters ($W_{xh}, W_{hh}, W_{ho}, b_h, b_o$, and potentially word embeddings if they are being trained).\n","    d. Update the model parameters using an optimization algorithm (e.g., SGD, Adam) based on the computed gradients.\n","3. Repeat step 2 for a number of epochs over the entire training corpus.\n","\n","After training a language model, Evaluating Language Models is crucial to assess its performance. The standard evaluation metric for Language Models is perplexity. Perplexity measures how well a language model predicts a sample of text.  It is essentially the inverse probability of the test set, normalized by the number of words. A lower perplexity indicates a better language model.\n","\n","Mathematically, given a test set of word sequence $W = (w_1, w_2, ..., w_m)$, the perplexity $PP(W)$ is defined as:\n","\n","$$ PP(W) = P(w_1, w_2, ..., w_m)^{-1/m} $$\n","\n","Where $P(w_1, w_2, ..., w_m)$ is the probability of the test set sequence as assigned by the language model. Using the chain rule decomposition:\n","\n","$$ PP(W) = \\left( \\prod_{i=1}^{m} P(w_i | w_1, ..., w_{i-1}) \\right) ^{-1/m}  = \\exp \\left( - \\frac{1}{m} \\sum_{i=1}^{m} \\log P(w_i | w_1, ..., w_{i-1}) \\right) $$\n","\n","Perplexity can be interpreted as the average branching factor of the language model. Imagine at each word, the model is predicting the probability distribution of the next word. Perplexity approximates the number of choices the model is, on average, equally unsure about when predicting the next word. For instance, a perplexity of 100 means that for each word, the model is as confused as if it had to choose uniformly at random from 100 equally likely words.  Therefore, a lower perplexity implies the model is more confident and accurate in its predictions, thus indicating a better language model.  Perplexity directly reflects the uncertainty of the language model in predicting the next word at each step, making it a comprehensive and widely accepted metric for evaluating language model performance. Improvements in language modeling research often aim at reducing perplexity on standard benchmark datasets."],"metadata":{"id":"OZP03cmEDIUk"}},{"cell_type":"markdown","source":["# Problems with RNNs: Vanishing and Exploding Gradients\n","\n","Recurrent Neural Networks (RNNs) propagate information through sequential computation stages, where the hidden state at time $t$ is computed recursively as:\n","\n","$$h_t = \\sigma(W_{hx}x_t + W_{hh}h_{t-1} + b_h)$$\n","\n","During backpropagation through time (BPTT), the gradient of loss $\\mathcal{L}_T$ with respect to parameters at earlier time steps requires computing the gradient through the recurrent hidden state transitions. For a parameter $\\theta$ at time step $k$, this involves:\n","\n","$$\\frac{\\partial \\mathcal{L}_T}{\\partial \\theta_k} = \\frac{\\partial \\mathcal{L}_T}{\\partial h_T} \\cdot \\frac{\\partial h_T}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial \\theta_k}$$\n","\n","The problematic term is $\\frac{\\partial h_T}{\\partial h_k}$, which by the chain rule expands to:\n","\n","$$\\frac{\\partial h_T}{\\partial h_k} = \\prod_{t=k+1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}} = \\prod_{t=k+1}^{T} W_{hh}^T \\cdot \\text{diag}(\\sigma'(W_{hx}x_t + W_{hh}h_{t-1} + b_h))$$\n","\n","This product of Jacobian matrices determines whether gradients vanish or explode over long sequences.\n","\n","The vanishing gradient intuition arises from analyzing the norm of the Jacobian matrix. For the sigmoid activation function $\\sigma(z) = \\frac{1}{1+e^{-z}}$, the derivative is bounded by $\\sigma'(z) \\leq 0.25$. If the largest singular value of $W_{hh}$ is less than 4, each multiplication in the Jacobian product reduces the gradient norm, causing exponential decay over time steps.\n","\n","For a formal proof sketch in the linear case, consider a simplified RNN with linear activation:\n","\n","$$h_t = W_{hh}h_{t-1} + W_{hx}x_t$$\n","\n","The Jacobian matrix becomes simply $\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}$. After $n$ time steps, the gradient term is:\n","\n","$$\\frac{\\partial h_{t+n}}{\\partial h_t} = W_{hh}^n$$\n","\n","By eigendecomposition, $W_{hh} = Q\\Lambda Q^{-1}$ where $\\Lambda$ contains eigenvalues $\\lambda_i$. Therefore:\n","\n","$$W_{hh}^n = Q\\Lambda^n Q^{-1}$$\n","\n","If $|\\lambda_i| < 1$ for all eigenvalues, then $\\lim_{n \\to \\infty} \\Lambda^n = 0$, causing gradients to vanish. Conversely, if any $|\\lambda_i| > 1$, then elements of $\\Lambda^n$ grow exponentially, causing gradient explosion.\n","\n","The effect of vanishing gradients on RNN language models (RNN-LM) is particularly severe. The probability distribution for predicting token $w_t$ depends on capturing relevant context from previous tokens. With vanishing gradients, parameters receive diminishing updates from distant context:\n","\n","$$\\frac{\\partial \\mathcal{L}_t}{\\partial W_{hh}} \\approx \\sum_{k=t-\\tau}^{t} \\frac{\\partial \\mathcal{L}_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_{hh}} \\approx \\sum_{k=t-\\tau}^{t} \\delta_t \\cdot \\prod_{j=k+1}^{t} W_{hh}^T \\text{diag}(\\sigma'(a_j)) \\cdot h_{k-1}^T$$\n","\n","As $\\tau$ increases, the contributions from earlier time steps become negligible, effectively limiting the RNN-LM to modeling short-range dependencies despite its theoretical capacity for unbounded context.\n","\n","Exploding gradients present complementary challenges by destabilizing training through extreme parameter updates. When $\\|\\frac{\\partial h_T}{\\partial h_k}\\| \\gg 1$, the computed gradients grow exponentially with sequence length, causing:\n","\n","1. Numerical overflow (NaN values)\n","2. Catastrophic parameter updates that destroy learned information\n","3. Unstable loss landscapes that prevent convergence\n","\n","Mathematically, the parameter update with learning rate $\\alpha$ becomes:\n","\n","$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta_t)$$\n","\n","With exploding gradients, $\\|\\nabla_\\theta \\mathcal{L}(\\theta_t)\\| \\to \\infty$, making any fixed learning rate inappropriate.\n","\n","Gradient clipping provides an effective solution for exploding gradients by constraining the gradient norm while preserving direction. Given a threshold $c$, the clipped gradient is:\n","\n","$$\\tilde{g} = \\min\\left(1, \\frac{c}{\\|g\\|}\\right) \\cdot g$$\n","\n","where $g = \\nabla_\\theta \\mathcal{L}(\\theta_t)$. This operation is mathematically equivalent to adaptive rescaling of the learning rate based on gradient magnitude:\n","\n","$$\\theta_{t+1} = \\theta_t - \\alpha' \\cdot g$$\n","\n","where $\\alpha' = \\alpha \\cdot \\min\\left(1, \\frac{c}{\\|g\\|}\\right)$.\n","\n","Addressing the vanishing gradient problem has led to several architectural innovations:\n","\n","1. Long Short-Term Memory (LSTM) networks introduce gated mechanisms to control information flow:\n","\n","$$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$\n","$$i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i)$$\n","$$\\tilde{C}_t = \\tanh(W_C[h_{t-1}, x_t] + b_C)$$\n","$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n","$$o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o)$$\n","$$h_t = o_t \\odot \\tanh(C_t)$$\n","\n","The memory cell $C_t$ establishes gradient flow through an additive path, where the Jacobian becomes:\n","\n","$$\\frac{\\partial C_t}{\\partial C_{t-1}} = \\text{diag}(f_t)$$\n","\n","With forget gate $f_t$ close to 1, gradients can propagate effectively over long sequences.\n","\n","2. Gated Recurrent Units (GRUs) implement a simplified gating mechanism:\n","\n","$$z_t = \\sigma(W_z[h_{t-1}, x_t] + b_z)$$\n","$$r_t = \\sigma(W_r[h_{t-1}, x_t] + b_r)$$\n","$$\\tilde{h}_t = \\tanh(W[r_t \\odot h_{t-1}, x_t] + b)$$\n","$$h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n","\n","3. Residual connections create direct pathways for gradient flow:\n","\n","$$h_t = h_{t-1} + f(h_{t-1}, x_t)$$\n","\n","4. Layer normalization stabilizes hidden state distributions:\n","\n","$$\\text{LN}(h) = \\gamma \\odot \\frac{h - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n","\n","where $\\mu$ and $\\sigma^2$ are the mean and variance computed over the hidden dimension.\n","\n","Technical advantages of these solutions include enhanced gradient flow stability, improved learning of long-range dependencies, and faster convergence. Limitations persist in the form of increased computational complexity, additional hyperparameters requiring tuning, and remaining challenges with extremely long sequences.\n","\n","Research directions continue to explore transformer architectures with self-attention, sparse recurrent models, and hierarchical structures to address fundamental limitations while maintaining computational efficiency."],"metadata":{"id":"e4VM__AjE8xd"}},{"cell_type":"markdown","source":["# Neural Networks, Language Modeling, and Recurrent Neural Networks\n","\n","Neural networks form the backbone of modern machine learning approaches, functioning as computational systems inspired by biological neural networks. The fundamental unit—a neuron—processes inputs through an activation function: $f(x) = \\sigma(w \\cdot x + b)$ where $w$ represents weights, $x$ inputs, $b$ bias, and $\\sigma$ the activation function. Multi-layer networks enable complex pattern recognition through forward propagation: $$h^{(l)} = \\sigma(W^{(l)}h^{(l-1)} + b^{(l)})$$ and optimize via backpropagation through computation of gradients: $$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial h^{(l)}} \\frac{\\partial h^{(l)}}{\\partial W^{(l)}}$$\n","\n","Language modeling constitutes a cornerstone task in natural language processing, defined as computing the probability distribution over sequences of words. Mathematically, given a sequence of tokens $(w_1, w_2, ..., w_t)$, language modeling computes $P(w_1, w_2, ..., w_t)$ using the chain rule of probability: $$P(w_1, w_2, ..., w_t) = \\prod_{i=1}^{t} P(w_i | w_1, w_2, ..., w_{i-1})$$ The objective function typically minimizes negative log-likelihood: $$\\mathcal{L} = -\\sum_{i=1}^{t} \\log P(w_i | w_1, w_2, ..., w_{i-1})$$\n","\n","This task necessitates neural architectures capable of processing sequential data while maintaining contextual information—introducing Recurrent Neural Networks. RNNs maintain a hidden state that captures information from previous tokens: $$h_t = f(h_{t-1}, x_t)$$ where $h_t$ represents the hidden state at time $t$, $x_t$ the input at time $t$, and $f$ a non-linear function. The standard RNN formulation utilizes: $$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$ $$y_t = W_{hy}h_t + b_y$$ where $W_{hh}$, $W_{xh}$, and $W_{hy}$ are weight matrices, and $b_h$ and $b_y$ are bias vectors.\n","\n","For language modeling specifically, inputs are typically token embeddings: $x_t = E[w_t]$ where $E$ represents an embedding matrix. The output layer projects to vocabulary size with softmax normalization: $$P(w_{t+1}|w_1, ..., w_t) = \\text{softmax}(W_{hy}h_t + b_y)$$\n","\n","RNNs face significant challenges despite their theoretical capacity to model arbitrary-length sequences. The vanishing gradient problem manifests when backpropagating through many time steps: $$\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}}\\frac{\\partial h_{t+1}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}}W_{hh}^T\\text{diag}(1-\\tanh^2(W_{hh}h_t + W_{xh}x_{t+1} + b_h))$$ Repeated matrix multiplication with eigenvalues $|\\lambda| < 1$ causes exponential decay of gradients: $$\\frac{\\partial \\mathcal{L}}{\\partial h_t} \\approx \\frac{\\partial \\mathcal{L}}{\\partial h_{t+n}}\\prod_{i=0}^{n-1}W_{hh}^T\\text{diag}(1-\\tanh^2(W_{hh}h_{t+i} + W_{xh}x_{t+i+1} + b_h))$$\n","\n","Conversely, the exploding gradient problem occurs when eigenvalues exceed 1, causing training instability. Gradient clipping provides partial mitigation: $$\\hat{g} = \\min\\left(1, \\frac{c}{||g||}\\right) \\cdot g$$ where $g$ is the gradient and $c$ is a threshold.\n","\n","Long-term dependency modeling remains problematic as information from distant tokens decays exponentially: $$I(x_t; h_{t+k}) \\leq I(x_t; h_{t+1}) \\leq \\log\\left(1+\\frac{\\text{Var}(W_{xh}x_t)}{\\text{Var}(W_{hh}h_{t-1})}\\right)$$ where $I$ represents mutual information.\n","\n","Advanced architectures including Long Short-Term Memory (LSTM) networks introduce gating mechanisms: $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$ $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$ $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$ $$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$ $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$ $$h_t = o_t \\odot \\tanh(C_t)$$ These mechanisms facilitate improved gradient flow and long-term dependency modeling.\n","\n","RNNs and language models laid critical groundwork for modern NLP despite inherent limitations. Their mathematical formulation established sequential processing paradigms, while revealing fundamental constraints in modeling long-range dependencies. These insights directly influenced architectural innovations including attention mechanisms and transformer-based models which address the contextual limitations while maintaining the probabilistic foundations of language modeling."],"metadata":{"id":"j8z43GnT7ZUj"}},{"cell_type":"code","source":[],"metadata":{"id":"gWuCLDgr7aI-"},"execution_count":null,"outputs":[]}]}