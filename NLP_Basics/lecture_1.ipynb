{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK1MA0GgpTOd"
   },
   "source": [
    "### Word Vectors: A Detailed Technical Explanation\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Word vectors, also known as word embeddings, are mathematical representations of words in a continuous vector space, where semantically similar words are mapped to nearby points.\n",
    "- Scientifically, word vectors are defined as dense, real-valued vectors of fixed dimensionality, typically denoted as $ \\mathbf{v_w} \\in \\mathbb{R}^d $, where $ d $ is the dimensionality of the vector space, and $ w $ represents a word in a vocabulary $ V $.\n",
    "- The objective of word vectors is to `capture semantic, syntactic, and contextual relationships between words`, enabling machines to process natural language in a computationally efficient and meaningful way.\n",
    "\n",
    "The scientific proof of their efficacy lies in their ability to preserve linguistic relationships, such as analogies, through vector arithmetic. For instance, the famous example $ \\mathbf{v_{king}} - \\mathbf{v_{man}} + \\mathbf{v_{woman}} \\approx \\mathbf{v_{queen}} $ demonstrates that word vectors encode relational knowledge, validated through empirical performance on tasks like word similarity, text classification, and machine translation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Concept (Objective)\n",
    "\n",
    "The primary objective of word vectors is to transform discrete, symbolic representations of words (e.g., one-hot encodings) into continuous, dense representations that capture linguistic properties. Unlike one-hot encodings, where each word is represented as a sparse vector of size $ |V| $ (vocabulary size) with a single 1 and all other elements 0, word vectors reduce dimensionality to a fixed size $ d \\ll |V| $ while preserving semantic meaning. This enables:\n",
    "\n",
    "1. **Semantic Representation**: Words with similar meanings (e.g., \"cat\" and \"dog\") have vectors that are close in the embedding space, measured by metrics like cosine similarity.\n",
    "2. **Syntactic Representation**: Grammatical relationships (e.g., \"run\" and \"running\") are captured through consistent vector offsets.\n",
    "3. **Generalization**: Word vectors allow models to generalize to unseen words or contexts by leveraging learned patterns in the vector space.\n",
    "\n",
    "The concept is grounded in the distributional hypothesis, which states that words appearing in similar contexts tend to have similar meanings. Word vectors operationalize this hypothesis by learning representations from large corpora using statistical or neural network-based methods.\n",
    "\n",
    "---\n",
    "\n",
    "#### Details of the Concept\n",
    "\n",
    "To develop word vectors mathematically, we rely on models that optimize an objective function to capture word co-occurrence patterns in a corpus. Below, we detail the technical foundations, mathematical formulations, and key algorithms.\n",
    "\n",
    "##### 1. **Foundational Idea: Distributional Semantics**\n",
    "The core idea is to represent a word $ w $ by the distribution of words that co-occur with it in a context window. This is formalized through co-occurrence matrices or neural network-based approaches. For example, in a corpus, the context of a word $ w $ is defined as the set of words $ c \\in C $ appearing within a window of size $ k $ around $ w $.\n",
    "\n",
    "##### 2. **Mathematical Representation**\n",
    "A word $ w $ is represented as a vector $ \\mathbf{v_w} \\in \\mathbb{R}^d $. The similarity between two words $ w_1 $ and $ w_2 $ is typically measured using cosine similarity, defined as:\n",
    "\n",
    "$$\n",
    "\\text{similarity}(w_1, w_2) = \\cos(\\theta) = \\frac{\\mathbf{v_{w_1}} \\cdot \\mathbf{v_{w_2}}}{\\|\\mathbf{v_{w_1}}\\| \\|\\mathbf{v_{w_2}}\\|}\n",
    "$$\n",
    "\n",
    "Here, $ \\mathbf{v_{w_1}} \\cdot \\mathbf{v_{w_2}} $ is the dot product, and $ \\|\\mathbf{v_{w_1}}\\| = \\sqrt{\\mathbf{v_{w_1}} \\cdot \\mathbf{v_{w_1}}} $ is the Euclidean norm. The cosine similarity ranges from -1 (opposite) to 1 (identical), with 0 indicating orthogonality (no similarity).\n",
    "\n",
    "##### 3. **Learning Word Vectors**\n",
    "Word vectors are learned using algorithms that optimize an objective function based on word-context relationships.\n",
    "- Two prominent approaches are:\n",
    "\n",
    "###### a. **Matrix Factorization (e.g., GloVe)**\n",
    "The Global Vectors (GloVe) model constructs a co-occurrence matrix $ X $, where $ X_{ij} $ represents the number of times word $ w_i $ appears in the context of word $ w_j $. The goal is to learn word vectors $ \\mathbf{v_i} $ and context vectors $ \\mathbf{u_j} $ such that their dot product approximates the logarithm of the co-occurrence probability, adjusted by a weighting function. The objective function is:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j=1}^{|V|} f(X_{ij}) \\left( \\mathbf{v_i} \\cdot \\mathbf{u_j} + b_i + b_j - \\log(X_{ij}) \\right)^2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ f(X_{ij}) $ is a weighting function (e.g., $ f(x) = (x/x_{\\text{max}})^\\alpha $ if $ x < x_{\\text{max}} $, else 1) to reduce the impact of frequent words.\n",
    "- $ b_i $ and $ b_j $ are bias terms for words and contexts, respectively.\n",
    "- The optimization minimizes the squared error between the dot product and the log co-occurrence count.\n",
    "\n",
    "After optimization, the final word vector for $ w_i $ is typically $ \\mathbf{v_i} + \\mathbf{u_i} $, normalized to unit length.\n",
    "\n",
    "###### b. **Neural Network-Based (e.g., Word2Vec)**\n",
    "The Word2Vec model uses a neural network to predict word-context pairs, offering two architectures: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
    "\n",
    "- **CBOW**: Predicts a target word $ w_t $ given its context words $ \\{w_{t-k}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+k}\\} $. The objective is to maximize the log-likelihood of the target word:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{T} \\sum_{t=1}^T \\log P(w_t | w_{t-k}, \\ldots, w_{t+k})\n",
    "$$\n",
    "\n",
    "The probability $ P(w_t | \\cdot) $ is modeled using a softmax function over the dot product of the target word vector $ \\mathbf{v_{w_t}} $ and the average context vector $ \\mathbf{\\bar{u}} = \\frac{1}{2k} \\sum_{j=t-k, j \\neq t}^{t+k} \\mathbf{u_{w_j}} $:\n",
    "\n",
    "$$\n",
    "P(w_t | \\cdot) = \\frac{\\exp(\\mathbf{v_{w_t}} \\cdot \\mathbf{\\bar{u}})}{\\sum_{w' \\in V} \\exp(\\mathbf{v_{w'}} \\cdot \\mathbf{\\bar{u}})}\n",
    "$$\n",
    "\n",
    "- **Skip-Gram**: Predicts context words $ \\{w_{t-k}, \\ldots, w_{t+k}\\} $ given a target word $ w_t $. The objective is to maximize:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{T} \\sum_{t=1}^T \\sum_{-k \\leq j \\leq k, j \\neq 0} \\log P(w_{t+j} | w_t)\n",
    "$$\n",
    "\n",
    "The probability $ P(w_{t+j} | w_t) $ is similarly modeled:\n",
    "\n",
    "$$\n",
    "P(w_{t+j} | w_t) = \\frac{\\exp(\\mathbf{u_{w_{t+j}}} \\cdot \\mathbf{v_{w_t}})}{\\sum_{w' \\in V} \\exp(\\mathbf{u_{w'}} \\cdot \\mathbf{v_{w_t}})}\n",
    "$$\n",
    "\n",
    "To make training efficient, techniques like negative sampling or hierarchical softmax are used to approximate the denominator in the softmax.\n",
    "\n",
    "##### 4. **Dimensionality and Interpretability**\n",
    "The dimensionality $ d $ of word vectors is a hyperparameter, typically ranging from 50 to 300. Lower $ d $ reduces computational cost but may lose expressiveness, while higher $ d $ increases representational capacity but risks overfitting or redundancy. The interpretability of word vectors emerges from their geometric properties, such as clustering of synonyms and linear analogies.\n",
    "\n",
    "##### 5. **Evaluation**\n",
    "Word vectors are evaluated intrinsically (e.g., word similarity tasks using datasets like WordSim-353) and extrinsically (e.g., performance in downstream tasks like sentiment analysis). Intrinsic evaluation uses metrics like Spearman’s rank correlation between human similarity judgments and cosine similarity scores.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusions (Technical Pros, Cons, and Research Improvements)\n",
    "\n",
    "##### Pros:\n",
    "1. **Efficiency**: Word vectors reduce the dimensionality of word representations from $ |V| $ to $ d $, making them computationally efficient compared to one-hot encodings.\n",
    "2. **Semantic Richness**: They capture nuanced semantic and syntactic relationships, enabling tasks like analogy solving and word sense disambiguation.\n",
    "3. **Generalization**: Pre-trained word vectors (e.g., GloVe, Word2Vec) can be fine-tuned or used as features in various NLP tasks, improving generalization to unseen data.\n",
    "4. **Mathematical Elegance**: The use of linear algebra (e.g., vector arithmetic) and optimization techniques provides a rigorous framework for language modeling.\n",
    "\n",
    "##### Cons:\n",
    "1. **Static Representations**: Word vectors are static, meaning a word like \"bank\" has the same vector regardless of context (e.g., financial bank vs. river bank). This limits their ability to handle polysemy and homonymy.\n",
    "2. **Data Dependency**: The quality of word vectors depends heavily on the size and quality of the training corpus. Biases in the corpus (e.g., gender or racial biases) are often encoded in the vectors.\n",
    "3. **Out-of-Vocabulary (OOV) Words**: Words not seen during training (e.g., neologisms) cannot be represented, requiring fallback strategies like random initialization.\n",
    "4. **Interpretability Challenges**: While geometric properties are insightful, the exact meaning of individual vector dimensions is not interpretable, limiting theoretical understanding.\n",
    "\n",
    "##### Research Improvements:\n",
    "1. **Contextual Embeddings**: Recent models like BERT and ELMo address the static nature of word vectors by generating contextual embeddings, where a word’s representation varies based on its context. These models use transformer architectures and optimize objectives like masked language modeling.\n",
    "2. **Bias Mitigation**: Research focuses on debiasing word vectors by identifying and neutralizing bias directions in the vector space, using techniques like projection onto a fair subspace.\n",
    "3. **Subword Information**: Models like FastText incorporate subword information (e.g., character n-grams) to handle OOV words and improve morphological generalization.\n",
    "4. **Theoretical Understanding**: Advances in information theory and geometry aim to explain why word vectors work, such as analyzing the role of singular value decomposition in capturing latent semantics.\n",
    "5. **Efficiency**: Techniques like quantization and pruning are explored to reduce the memory footprint of word vectors, making them viable for resource-constrained environments.\n",
    "\n",
    "By addressing these limitations, the field continues to evolve, leveraging word vectors as foundational tools in modern natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqSaoxQApQnI"
   },
   "source": [
    "### Loss Functions\n",
    "\n",
    "**Definition:**  \n",
    "A loss function, also known as a cost function or objective function, is a mathematical function that quantifies the difference between the predicted output of a model and the actual target values. It serves as a measure of how well the model is performing. The goal of training a machine learning model is to minimize this loss function.\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "Let $y$ be the true target value and $\\hat{y}$ be the predicted value. The loss function $L(y, \\hat{y})$ can be defined as:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = \\text{measure of discrepancy between } y \\text{ and } \\hat{y}\n",
    "$$\n",
    "\n",
    "**Common Loss Functions:**\n",
    "\n",
    "1. **Mean Squared Error (MSE):**  \n",
    "   Used in regression tasks, MSE measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "\n",
    "   - **Pros:** Simple to compute, differentiable, and convex.\n",
    "   - **Cons:** Sensitive to outliers, penalizes large errors heavily.\n",
    "\n",
    "2. **Cross-Entropy Loss:**  \n",
    "   Used in classification tasks, it measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "   $$\n",
    "   L(y, \\hat{y}) = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "   $$\n",
    "\n",
    "   - **Pros:** Effective for classification, penalizes incorrect classifications heavily.\n",
    "   - **Cons:** Can lead to numerical instability if $\\hat{y}_i$ is close to 0 or 1.\n",
    "\n",
    "**Conclusion:**  \n",
    "Loss functions are crucial in machine learning as they guide the optimization process. The choice of loss function depends on the specific task (regression, classification, etc.) and the desired properties (robustness, differentiability, etc.). Research improvements include developing loss functions that are more robust to outliers or that better capture the underlying data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Optimization: Gradient Descent\n",
    "\n",
    "**Definition:**  \n",
    "Gradient Descent is an iterative optimization algorithm used to minimize a function by moving in the direction of the steepest descent, as defined by the negative of the gradient. It is widely used in machine learning to optimize loss functions.\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "Given a loss function $L(\\theta)$, where $\\theta$ represents the parameters of the model, the goal is to find the value of $\\theta$ that minimizes $L(\\theta)$. The update rule for Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta_t$ is the current parameter value at iteration $t$,\n",
    "- $\\eta$ is the learning rate (step size),\n",
    "- $\\nabla L(\\theta_t)$ is the gradient of the loss function with respect to $\\theta$.\n",
    "\n",
    "**Concept:**  \n",
    "The gradient $\\nabla L(\\theta_t)$ points in the direction of the steepest ascent. By moving in the opposite direction (negative gradient), the algorithm reduces the loss function value. The learning rate $\\eta$ controls the size of the steps taken during optimization.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to implement.\n",
    "- Guaranteed to converge to a local minimum for convex functions.\n",
    "\n",
    "**Cons:**\n",
    "- Can be slow to converge, especially for large datasets.\n",
    "- Sensitive to the choice of learning rate $\\eta$.\n",
    "- May get stuck in local minima for non-convex functions.\n",
    "\n",
    "**Conclusion:**  \n",
    "Gradient Descent is a fundamental optimization technique in machine learning. Research improvements include adaptive learning rate methods (e.g., Adam, RMSprop) and second-order optimization methods (e.g., Newton's method) to improve convergence speed and stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Definition:**  \n",
    "Stochastic Gradient Descent (SGD) is a variant of Gradient Descent that updates the model parameters using only a single data point (or a small batch of data points) at each iteration, rather than the entire dataset. This introduces noise into the gradient estimation, which can help escape local minima and speed up convergence.\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "Given a loss function $L(\\theta)$, the update rule for SGD is:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t; x_i, y_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta_t$ is the current parameter value at iteration $t$,\n",
    "- $\\eta$ is the learning rate,\n",
    "- $\\nabla L(\\theta_t; x_i, y_i)$ is the gradient of the loss function with respect to $\\theta$ for a single data point $(x_i, y_i)$.\n",
    "\n",
    "**Concept:**  \n",
    "SGD approximates the true gradient by using a single data point or a small batch, which introduces stochasticity. This randomness can help the algorithm escape local minima and converge faster, especially for large datasets.\n",
    "\n",
    "**Pros:**\n",
    "- Faster convergence compared to standard Gradient Descent for large datasets.\n",
    "- Can escape local minima due to the noise introduced by stochastic updates.\n",
    "\n",
    "**Cons:**\n",
    "- Noisy updates can lead to oscillations and slower convergence.\n",
    "- Requires careful tuning of the learning rate and batch size.\n",
    "\n",
    "**Conclusion:**  \n",
    "SGD is a powerful optimization technique, particularly for large-scale machine learning problems. Research improvements include momentum-based methods (e.g., Nesterov Accelerated Gradient) and adaptive learning rate methods (e.g., Adam) to stabilize and accelerate convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im8-XbMTs5nI"
   },
   "source": [
    "### RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "**Definition:**  \n",
    "RMSprop is an adaptive learning rate optimization algorithm designed to address the limitations of standard Gradient Descent and Stochastic Gradient Descent (SGD). It adjusts the learning rate for each parameter based on the magnitude of recent gradients, which helps stabilize and accelerate convergence.\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "RMSprop maintains a moving average of the squared gradients for each parameter. The update rule is as follows:\n",
    "\n",
    "1. Compute the gradient of the loss function $L(\\theta)$ with respect to the parameters $\\theta$:\n",
    "   $$\n",
    "   g_t = \\nabla L(\\theta_t)\n",
    "   $$\n",
    "\n",
    "2. Update the moving average of squared gradients:\n",
    "   $$\n",
    "   E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "   $$\n",
    "   where $\\gamma$ is the decay rate (typically set to 0.9).\n",
    "\n",
    "3. Update the parameters:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "   $$\n",
    "   where $\\eta$ is the learning rate, and $\\epsilon$ is a small constant (e.g., $10^{-8}$) to avoid division by zero.\n",
    "\n",
    "**Concept:**  \n",
    "RMSprop adapts the learning rate for each parameter by dividing the gradient by the square root of the moving average of squared gradients. This normalization reduces the learning rate for parameters with large gradients and increases it for parameters with small gradients, leading to more stable updates.\n",
    "\n",
    "**Pros:**\n",
    "- Adapts learning rates for each parameter, improving convergence.\n",
    "- Effective for non-convex optimization problems.\n",
    "- Reduces oscillations in the optimization process.\n",
    "\n",
    "**Cons:**\n",
    "- Requires tuning of the decay rate $\\gamma$.\n",
    "- May still struggle with saddle points or plateaus.\n",
    "\n",
    "**Conclusion:**  \n",
    "RMSprop is a robust optimization algorithm that addresses the limitations of standard Gradient Descent. Research improvements include combining RMSprop with momentum-based methods (e.g., Adam) to further enhance performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "**Definition:**  \n",
    "Nesterov Accelerated Gradient (NAG) is a momentum-based optimization algorithm that improves upon standard Gradient Descent by incorporating a \"look-ahead\" step. This allows the algorithm to anticipate the future position of the parameters, leading to faster convergence.\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "NAG uses the following update rules:\n",
    "\n",
    "1. Compute the gradient at the \"look-ahead\" position:\n",
    "   $$\n",
    "   g_t = \\nabla L(\\theta_t - \\gamma v_{t-1})\n",
    "   $$\n",
    "   where $\\gamma$ is the momentum coefficient, and $v_{t-1}$ is the velocity vector from the previous iteration.\n",
    "\n",
    "2. Update the velocity vector:\n",
    "   $$\n",
    "   v_t = \\gamma v_{t-1} + \\eta g_t\n",
    "   $$\n",
    "\n",
    "3. Update the parameters:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\theta_t - v_t\n",
    "   $$\n",
    "\n",
    "**Concept:**  \n",
    "NAG improves upon standard momentum by evaluating the gradient at the \"look-ahead\" position ($\\theta_t - \\gamma v_{t-1}$) rather than the current position ($\\theta_t$). This anticipatory step allows the algorithm to correct its trajectory more effectively, leading to faster convergence.\n",
    "\n",
    "**Pros:**\n",
    "- Faster convergence compared to standard Gradient Descent and momentum.\n",
    "- Effective for both convex and non-convex optimization problems.\n",
    "\n",
    "**Cons:**\n",
    "- Requires tuning of the momentum coefficient $\\gamma$.\n",
    "- Computationally more expensive than standard Gradient Descent.\n",
    "\n",
    "**Conclusion:**  \n",
    "NAG is a powerful optimization algorithm that accelerates convergence by incorporating a look-ahead step. Research improvements include combining NAG with adaptive learning rate methods (e.g., Adam) to further enhance performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Definition:**  \n",
    "Adam is an adaptive learning rate optimization algorithm that combines the benefits of RMSprop and momentum-based methods. It maintains moving averages of both the gradients and the squared gradients, allowing it to adapt the learning rate for each parameter dynamically.\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "Adam uses the following update rules:\n",
    "\n",
    "1. Compute the gradient of the loss function $L(\\theta)$ with respect to the parameters $\\theta$:\n",
    "   $$\n",
    "   g_t = \\nabla L(\\theta_t)\n",
    "   $$\n",
    "\n",
    "2. Update the moving average of the gradients (first moment):\n",
    "   $$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "   $$\n",
    "\n",
    "3. Update the moving average of the squared gradients (second moment):\n",
    "   $$\n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "   $$\n",
    "\n",
    "4. Correct the bias in the moving averages (optional but recommended):\n",
    "   $$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "   $$\n",
    "\n",
    "5. Update the parameters:\n",
    "   $$\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "   $$\n",
    "   where $\\eta$ is the learning rate, $\\beta_1$ and $\\beta_2$ are decay rates (typically set to 0.9 and 0.999, respectively), and $\\epsilon$ is a small constant (e.g., $10^{-8}$).\n",
    "\n",
    "**Concept:**  \n",
    "Adam combines the benefits of momentum (first moment) and RMSprop (second moment) to adapt the learning rate for each parameter dynamically. The bias correction step ensures that the moving averages are accurate, especially in the early stages of training.\n",
    "\n",
    "**Pros:**\n",
    "- Combines the benefits of momentum and adaptive learning rates.\n",
    "- Effective for a wide range of optimization problems.\n",
    "- Requires minimal tuning of hyperparameters.\n",
    "\n",
    "**Cons:**\n",
    "- Computationally more expensive than simpler methods like SGD.\n",
    "- May still struggle with certain types of non-convex optimization problems.\n",
    "\n",
    "**Conclusion:**  \n",
    "Adam is a state-of-the-art optimization algorithm that combines the strengths of momentum and adaptive learning rates. Research improvements include developing variants of Adam (e.g., AdaMax, Nadam) to address specific limitations and further enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-DZ8Kz2thL9"
   },
   "source": [
    "------\n",
    "# How to evaluate word vectors?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M4heBi5uTef"
   },
   "source": [
    "# Evaluation of Word Vectors: Theoretical Foundations and Technical Approaches\n",
    "\n",
    "Word vectors, mathematical representations of lexical units in continuous vector space, require robust evaluation frameworks to determine their quality and efficacy. These evaluations assess the extent to which distributional semantic models capture linguistic phenomena and semantic relationships.\n",
    "\n",
    "## Mathematical Foundation of Word Vectors\n",
    "\n",
    "Word vectors map tokens to dense representations in $\\mathbb{R}^d$ where $d$ represents dimensionality. Formally, a word embedding function $f: V \\rightarrow \\mathbb{R}^d$ maps vocabulary $V$ to vector space. The geometric relationship between vectors $\\vec{v}_1$ and $\\vec{v}_2$ represents semantic similarity, typically measured via cosine similarity:\n",
    "\n",
    "$$\\text{sim}(\\vec{v}_1, \\vec{v}_2) = \\frac{\\vec{v}_1 \\cdot \\vec{v}_2}{||\\vec{v}_1|| \\cdot ||\\vec{v}_2||} = \\frac{\\sum_{i=1}^{d} v_{1i} v_{2i}}{\\sqrt{\\sum_{i=1}^{d} v_{1i}^2} \\sqrt{\\sum_{i=1}^{d} v_{2i}^2}}$$\n",
    "\n",
    "Evaluation methodologies for these vectors bifurcate into intrinsic and extrinsic approaches, each assessing different qualities of the vector representations.\n",
    "\n",
    "## Intrinsic Word Vector Evaluation\n",
    "\n",
    "Intrinsic evaluation examines inherent properties of vectors without application to downstream tasks. These methods assess semantic and syntactic relationships directly encoded in vector space.\n",
    "\n",
    "The word analogy task represents a quintessential intrinsic evaluation method. Given word pairs $(a, a')$ and $(b, b')$ sharing a relationship, we compute:\n",
    "\n",
    "$$\\vec{v}_{a'} - \\vec{v}_a \\approx \\vec{v}_{b'} - \\vec{v}_b$$\n",
    "\n",
    "Hence, $\\vec{v}_{b'} \\approx \\vec{v}_b + \\vec{v}_{a'} - \\vec{v}_a$. The evaluation metrics calculate accuracy of predicted $\\vec{v}_{b'}$ through:\n",
    "\n",
    "$$\\text{argmax}_{w \\in V \\setminus \\{a, a', b\\}} \\frac{\\vec{v}_w \\cdot (\\vec{v}_b + \\vec{v}_{a'} - \\vec{v}_a)}{||\\vec{v}_w|| \\cdot ||\\vec{v}_b + \\vec{v}_{a'} - \\vec{v}_a||}$$\n",
    "\n",
    "Syntactic analogies test morphological relationships (e.g., \"walk:walked::run:ran\"), while semantic analogies test concept relationships (e.g., \"man:woman::king:queen\"). Mikolov et al. demonstrated that their word2vec embeddings achieve 65-70% accuracy on these tasks, establishing a performance benchmark.\n",
    "\n",
    "## GloVe Visualization\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) vectors exhibit linear substructures in vector space, visualizable through dimensionality reduction techniques. Mathematically, GloVe embeddings are trained to satisfy:\n",
    "\n",
    "$$\\vec{v}_i^T \\vec{v}_j + b_i + b_j = \\log(X_{ij})$$\n",
    "\n",
    "Where $X_{ij}$ represents co-occurrence counts, and $b_i$, $b_j$ are bias terms.\n",
    "\n",
    "For visualization, Principal Component Analysis (PCA) or t-SNE transforms high-dimensional vectors to 2D or 3D space:\n",
    "\n",
    "$$\\text{PCA}: X \\rightarrow U\\Sigma V^T$$\n",
    "\n",
    "Where $U$ contains eigenvectors of $XX^T$. For GloVe vectors, visualization reveals semantic clusters and linear relationships between conceptually related terms. The linear structure enables vector arithmetic operations like:\n",
    "\n",
    "$$\\vec{v}_{\\text{queen}} \\approx \\vec{v}_{\\text{king}} - \\vec{v}_{\\text{man}} + \\vec{v}_{\\text{woman}}$$\n",
    "\n",
    "This property confirms that GloVe encodes semantic relationships as vector offsets, providing empirical evidence of distributional semantics theory.\n",
    "\n",
    "## Meaning Similarity Evaluation\n",
    "\n",
    "Semantic similarity evaluations assess correlation between vector-based similarity measures and human judgments. Standard datasets include WordSim-353, SimLex-999, and MEN, containing word pairs with human-assigned similarity scores.\n",
    "\n",
    "The evaluation computes Spearman's rank correlation coefficient:\n",
    "\n",
    "$$\\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}$$\n",
    "\n",
    "Where $d_i$ represents difference in ranks between human judgment and vector similarity for pair $i$, and $n$ is the number of pairs.\n",
    "\n",
    "Performance varies by embedding method and dimensionality. For 300-dimensional vectors, typical correlations range from 0.6-0.8 on WordSim-353, with retrofitted vectors incorporating knowledge graphs achieving higher correlations (0.7-0.85).\n",
    "\n",
    "## Correlation Evaluation\n",
    "\n",
    "Beyond rank correlation, evaluation methodologies employ parametric correlation measures such as Pearson's correlation coefficient:\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "Where $x_i$ represents human judgment scores and $y_i$ represents vector-derived similarity scores.\n",
    "\n",
    "Statistical significance testing determines whether correlation differs significantly from zero:\n",
    "\n",
    "$$t = r\\sqrt{\\frac{n-2}{1-r^2}}$$\n",
    "\n",
    "With $n-2$ degrees of freedom. The null hypothesis ($H_0: \\rho = 0$) is rejected when $|t| > t_{\\alpha/2, n-2}$.\n",
    "\n",
    "Multiple correlation metrics address different aspects of similarity. For instance, outlier detection quantifies error analysis through:\n",
    "\n",
    "$$z_i = \\frac{|x_i - y_i|}{\\sigma_e}$$\n",
    "\n",
    "Where $\\sigma_e$ represents standard deviation of residuals.\n",
    "\n",
    "## Extrinsic Word Vector Evaluation\n",
    "\n",
    "Extrinsic evaluation assesses vector quality through performance on downstream NLP tasks. The mathematical formalism varies by task, but generally involves using vectors as features in a supervised learning framework:\n",
    "\n",
    "$$P(y|x) = f(\\theta, \\Phi(x))$$\n",
    "\n",
    "Where $\\Phi(x)$ represents word vector features, $\\theta$ represents model parameters, and $y$ represents target outputs.\n",
    "\n",
    "Common extrinsic evaluation tasks include:\n",
    "\n",
    "1. Named Entity Recognition (NER): $F_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
    "\n",
    "2. Sentiment Analysis: $\\text{Accuracy} = \\frac{\\text{correct predictions}}{\\text{total predictions}}$\n",
    "\n",
    "3. Machine Translation: $\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$\n",
    "\n",
    "4. Question Answering: $\\text{Exact Match} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{1}(\\hat{y}_i = y_i)$\n",
    "\n",
    "Research demonstrates that intrinsic and extrinsic evaluations often exhibit limited correlation. Vectors optimized for analogy tasks may underperform on sentiment analysis, suggesting task-specific optimization requirements.\n",
    "\n",
    "## Word Senses and Word Sense Ambiguity\n",
    "\n",
    "Word sense ambiguity presents distinct challenges for vector evaluation. Traditional word embeddings conflate multiple senses into single vectors, reducing disambiguation ability.\n",
    "\n",
    "Mathematically, polysemous words possess probability distributions over senses:\n",
    "\n",
    "$$P(s|w) = \\frac{P(w|s)P(s)}{P(w)} = \\frac{P(w|s)P(s)}{\\sum_{s' \\in S_w} P(w|s')P(s')}$$\n",
    "\n",
    "Where $S_w$ represents the set of possible senses for word $w$.\n",
    "\n",
    "Advanced models address this through sense embeddings, representing each sense separately:\n",
    "\n",
    "$$\\vec{v}_{w,s} = f(w, s, \\text{context})$$\n",
    "\n",
    "Evaluation protocols for sense embeddings employ Word Sense Disambiguation (WSD) benchmarks, measuring accuracy against human-annotated corpora:\n",
    "\n",
    "$$\\text{WSD Accuracy} = \\frac{\\text{correctly disambiguated instances}}{\\text{total instances}}$$\n",
    "\n",
    "Multi-sense embedding models like MSSG (Multi-Sense Skip-Gram) learn multiple vectors per word using context clustering:\n",
    "\n",
    "$$\\vec{v}_{w,k} = \\text{centroid}(\\{\\vec{c}_i | \\text{sense}(w, c_i) = k\\})$$\n",
    "\n",
    "Where $\\vec{c}_i$ represents context vectors and $\\text{sense}(w, c_i)$ assigns contexts to sense clusters.\n",
    "\n",
    "Evaluations reveal that sense-aware embeddings improve performance on similarity tasks by 2-5% but introduce computational complexity scaling with sense inventory size, demonstrating the fundamental tradeoff between representational power and computational efficiency.\n",
    "\n",
    "Technical advantages of modern evaluation frameworks include standardization of metrics, reproducibility across embedding architectures, and correlation analysis between intrinsic and extrinsic performance. Limitations include dataset biases, lack of cross-lingual evaluation standards, and insufficient assessment of contextual embeddings' dynamic properties.\n",
    "\n",
    "Future research directions include developing evaluation frameworks for contextual embeddings, cross-lingual vector quality metrics, and quantitative measures for social biases encoded in vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijoyNO5MuRGG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSt8SCX/0ekYtgSejz184l",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
