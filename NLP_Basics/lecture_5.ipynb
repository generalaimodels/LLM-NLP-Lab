{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO/tQv6YEMJIQD+XssfqVYJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Long Short-Term Memory RNNs (LSTMs)\n","\n","## Definition\n","Long Short-Term Memory networks (LSTMs) are specialized recurrent neural networks designed to model long-term dependencies in sequential data. Introduced by Hochreiter and Schmidhuber in 1997, LSTMs overcome the limitations of traditional RNNs through a sophisticated gating mechanism that regulates information flow through the network.\n","\n","## Mathematical Formulation\n","\n","The LSTM cell operates through the following equations at time step $t$:\n","\n","**Input Gate:**\n","$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n","\n","**Forget Gate:**\n","$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n","\n","**Cell Update:**\n","$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n","\n","**Cell State Update:**\n","$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n","\n","**Output Gate:**\n","$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n","\n","**Hidden State Update:**\n","$$h_t = o_t \\odot \\tanh(C_t)$$\n","\n","Where:\n","- $x_t$ represents the input vector at time $t$\n","- $h_{t-1}$ is the previous hidden state\n","- $C_{t-1}$ is the previous cell state\n","- $W$ matrices are trainable weights\n","- $b$ vectors are bias terms\n","- $\\sigma$ denotes the sigmoid function\n","- $\\odot$ represents element-wise multiplication\n","\n","## Core Principles\n","\n","### Cell Architecture\n","- **Memory Cell (Cell State)**: Central component that acts as an information highway running through the sequence\n","- **Three Gates**: Input, forget, and output gates that regulate information flow\n","- **Controlled Information Flow**: Selective reading, writing, and forgetting operations\n","\n","### Gating Mechanisms\n","- **Forget Gate**: Determines what information to discard from the cell state\n","- **Input Gate**: Controls what new information to incorporate into the cell state\n","- **Output Gate**: Filters what parts of the cell state to output as the hidden state\n","\n","## How LSTM Solves Vanishing Gradients\n","\n","### The Problem\n","In standard RNNs, gradients flowing backward through time diminish exponentially due to repeated multiplication with the recurrent weight matrix, causing:\n","- Loss of long-range dependencies\n","- Stalled learning for early time steps\n","- Unstable training dynamics\n","\n","### LSTM's Solutions\n","\n","#### 1. Constant Error Carousel (CEC)\n","- The cell state provides an uninterrupted gradient pathway through time\n","- The key equation revealing this mechanism:\n","$$\\frac{\\partial C_t}{\\partial C_{t-1}} = f_t$$\n","\n","When $f_t \\approx 1$, gradients can flow backward with minimal decay\n","\n","#### 2. Additive Update Structure\n","- Unlike standard RNNs which use multiplicative updates, LSTMs use additive updates:\n","$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n","- This prevents gradient decay through repeated multiplication\n","\n","#### 3. Gated Information Flow\n","- Gates control which information persists through time\n","- Forget gates typically initialize with biases toward 1, creating near-identity mappings early in training\n","- This establishes gradient highways that prevent vanishing\n","\n","#### 4. Protected Memory Cell\n","- Cell state is only partially exposed to non-linearities\n","- Limits compounding effects of activation functions that contribute to gradient vanishing\n","\n","## Is Vanishing/Exploding Gradient Just an RNN Problem?\n","\n","No, these issues affect various neural architectures, though they are most pronounced in recurrent networks.\n","\n","### Other Affected Architectures\n","\n","#### Deep Feedforward Networks\n","- Very deep networks suffer from gradient decay/explosion across layers\n","- Each additional layer compounds the gradient transformation problem\n","\n","#### Convolutional Neural Networks\n","- Pre-ResNet, deep CNNs faced significant gradient issues\n","- Networks beyond ~20 layers became increasingly difficult to train\n","\n","#### Transformers\n","- Despite self-attention, very deep transformers can experience gradient problems\n","- Mitigated through normalization techniques and residual connections\n","\n","### Universal Solutions\n","- **Architectural shortcuts**: Residual connections, highway networks, dense connections\n","- **Normalization techniques**: Batch normalization, layer normalization\n","- **Initialization strategies**: Careful weight initialization (Xavier/Glorot, He)\n","- **Gradient stabilization**: Gradient clipping, gradient scaling\n","- **Activation functions**: ReLU variants reduce vanishing compared to sigmoid/tanh\n","\n","## Pros and Cons of LSTMs\n","\n","### Advantages\n","- Effective modeling of long-term dependencies\n","- Robust gradient flow through time\n","- Selective memory retention through gating\n","- Superior performance on sequential tasks compared to vanilla RNNs\n","- Interpretable internal states through gate activations\n","\n","### Disadvantages\n","- Computationally expensive (3-4Ã— parameters compared to vanilla RNNs)\n","- Sequential computation limits parallelization\n","- Still struggles with very long sequences (thousands of steps)\n","- Complex architecture increases training difficulty\n","- Outperformed by Transformers in many modern NLP tasks\n","- High memory requirements for backpropagation through time\n","\n","## Recent Advancements\n","\n","### Architectural Innovations\n","- **Peephole Connections**: Allow gates to access cell state directly\n","- **ConvLSTM**: Incorporate convolutional operations for spatial-temporal data\n","- **Bidirectional LSTMs**: Process sequences in both forward and backward directions\n","- **Attention-augmented LSTMs**: Combine recurrent processing with attention mechanisms\n","\n","### Training Improvements\n","- **Layer normalization**: Stabilizes LSTM training\n","- **Chrono initialization**: Specialized initialization for forget gates\n","- **Zoneout regularization**: Alternative to dropout for recurrent connections\n","\n","### Hardware Optimizations\n","- **Quantized LSTMs**: Reduced precision for efficiency\n","- **Sparse LSTM variants**: Reduced computational overhead through pruning\n","- **Optimized CUDA implementations**: Hardware-specific optimizations\n","\n","### Hybrid Architectures\n","- **LSTM-Transformer hybrids**: Leveraging strengths of both architectures\n","- **Hierarchical LSTMs**: Capturing information at multiple time scales\n","- **Neural Ordinary Differential Equations (NODE)**: Continuous-time variants of LSTMs\n","\n"],"metadata":{"id":"ifBDy5wZut8M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SgtFQyYuqWc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class LSTMCell(nn.Module):\n","    \"\"\"\n","    Custom implementation of LSTM cell from scratch.\n","    Attributes:\n","        input_size (int): Size of input vector\n","        hidden_size (int): Size of hidden state vector\n","        weight_ih (Tensor): Input-hidden weights\n","        weight_hh (Tensor): Hidden-hidden weights\n","        bias_ih (Tensor): Input-hidden bias\n","        bias_hh (Tensor): Hidden-hidden bias\n","    \"\"\"\n","\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTMCell, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # Initialize weights using Xavier/Glorot initialization\n","        std = math.sqrt(2.0 / (input_size + hidden_size))\n","\n","        # Combined weights for input-hidden transformations (4 gates)\n","        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size) * std)\n","        # Combined weights for hidden-hidden transformations (4 gates)\n","        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size) * std)\n","        # Combined biases\n","        self.bias_ih = nn.Parameter(torch.zeros(4 * hidden_size))\n","        self.bias_hh = nn.Parameter(torch.zeros(4 * hidden_size))\n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Forward pass of LSTM cell.\n","        Args:\n","            x (Tensor): Input tensor of shape (batch_size, input_size)\n","            hidden (tuple): Tuple of (h_prev, c_prev) previous hidden and cell states\n","        Returns:\n","            tuple: (h_t, c_t) new hidden and cell states\n","        \"\"\"\n","        h_prev, c_prev = hidden\n","\n","        # Linear transformations\n","        gates = (torch.mm(x, self.weight_ih.t()) + self.bias_ih +\n","                torch.mm(h_prev, self.weight_hh.t()) + self.bias_hh)\n","\n","        # Split into four gates (input, forget, cell, output)\n","        chunk_size = self.hidden_size\n","        i_t = gates[:, :chunk_size]  # Input gate\n","        f_t = gates[:, chunk_size:2*chunk_size]  # Forget gate\n","        g_t = gates[:, 2*chunk_size:3*chunk_size]  # Cell gate\n","        o_t = gates[:, 3*chunk_size:]  # Output gate\n","\n","        # Apply activation functions\n","        i_t = torch.sigmoid(i_t)\n","        f_t = torch.sigmoid(f_t)\n","        g_t = torch.tanh(g_t)\n","        o_t = torch.sigmoid(o_t)\n","\n","        # Update cell state\n","        c_t = f_t * c_prev + i_t * g_t\n","\n","        # Update hidden state\n","        h_t = o_t * torch.tanh(c_t)\n","\n","        return h_t, c_t\n","\n","    def init_hidden(self, batch_size, device):\n","        \"\"\"\n","        Initialize hidden and cell states.\n","        Args:\n","            batch_size (int): Batch size\n","            device (torch.device): Device to create tensors on\n","        Returns:\n","            tuple: (h_0, c_0) initialized hidden and cell states\n","        \"\"\"\n","        h_0 = torch.zeros(batch_size, self.hidden_size, device=device)\n","        c_0 = torch.zeros(batch_size, self.hidden_size, device=device)\n","        return h_0, c_0\n","\n","\n","class LSTMNetwork(nn.Module):\n","    \"\"\"\n","    LSTM network wrapper for multiple time steps.\n","    Attributes:\n","        lstm_cell (LSTMCell): LSTM cell implementation\n","        num_layers (int): Number of LSTM layers\n","        hidden_size (int): Size of hidden state\n","    \"\"\"\n","\n","    def __init__(self, input_size, hidden_size, num_layers=1):\n","        super(LSTMNetwork, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # Create LSTM cells for each layer\n","        self.lstm_cells = nn.ModuleList([\n","            LSTMCell(input_size if i == 0 else hidden_size, hidden_size)\n","            for i in range(num_layers)\n","        ])\n","\n","    def forward(self, x, hidden=None):\n","        \"\"\"\n","        Forward pass of LSTM network.\n","        Args:\n","            x (Tensor): Input tensor of shape (batch_size, seq_length, input_size)\n","            hidden (list): List of initial hidden states for each layer\n","        Returns:\n","            tuple: (output, hidden) final output and hidden states\n","        \"\"\"\n","        batch_size, seq_length, _ = x.size()\n","        device = x.device\n","\n","        # Initialize hidden states if not provided\n","        if hidden is None:\n","            hidden = [\n","                self.lstm_cells[i].init_hidden(batch_size, device)\n","                for i in range(self.num_layers)\n","            ]\n","\n","        output = []\n","\n","        # Process sequence\n","        for t in range(seq_length):\n","            x_t = x[:, t, :]  # Current time step input\n","\n","            # Process through each layer\n","            for layer_idx in range(self.num_layers):\n","                hidden[layer_idx] = self.lstm_cells[layer_idx](\n","                    x_t, hidden[layer_idx]\n","                )\n","                x_t = hidden[layer_idx][0]  # Hidden state becomes input to next layer\n","\n","            output.append(x_t)\n","\n","        # Stack outputs\n","        output = torch.stack(output, dim=1)\n","        return output, hidden\n","\n","\n","# Example usage and testing\n","if __name__ == \"__main__\":\n","    # # Check if torch is available and get version\n","    # assert torch.__version__ >= \"1.9.0\", \"PyTorch version 1.9.0 or higher required\"\n","\n","    # Parameters\n","    input_size = 10\n","    hidden_size = 20\n","    num_layers = 20\n","    batch_size = 32\n","    seq_length = 15\n","\n","    # Create sample data\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    x = torch.randn(batch_size, seq_length, input_size).to(device)\n","\n","    # Initialize network\n","    model = LSTMNetwork(input_size, hidden_size, num_layers).to(device)\n","\n","    # Forward pass\n","    output, hidden = model(x)\n","\n","    # Verify output shapes\n","    assert output.shape == (batch_size, seq_length, hidden_size)\n","    assert len(hidden) == num_layers\n","    assert all(h.shape == (batch_size, hidden_size) for h, c in hidden)\n","    assert all(c.shape == (batch_size, hidden_size) for h, c in hidden)\n","\n","    print(\"All tests passed!\")\n","    print(f\"Output shape: {output.shape}\")\n","    print(f\"Number of layers: {len(hidden)}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"eakhtbHZws8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I28IiLaaFbrG"},"execution_count":null,"outputs":[]}]}