{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOXWt/oZmDnj3SI1vDmyIP7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Optimizing for Human Preferences: RLHF and DPO\n","\n","Optimizing machine learning models, particularly large language models (LLMs), to align with human preferences is a critical task in modern artificial intelligence, especially in applications requiring safe, ethical, and user-friendly interactions. Two prominent approaches to achieve this alignment are **Reinforcement Learning from Human Feedback (RLHF)** and **Direct Preference Optimization (DPO)**. This document provides a detailed, end-to-end explanation of both methods, covering definitions, mathematical foundations, core principles, detailed concepts, importance, pros and cons, and recent advancements.\n","\n","---\n","\n","## 1. Reinforcement Learning from Human Feedback (RLHF)\n","\n","### 1.1 Definition\n","Reinforcement Learning from Human Feedback (RLHF) is a machine learning paradigm that leverages human-provided feedback, typically in the form of pairwise comparisons or rankings, to fine-tune a pre-trained model. RLHF is particularly useful for optimizing large language models (LLMs) to exhibit behaviors that are not easily captured by traditional supervised learning, such as generating helpful, truthful, and safe responses.\n","\n","### 1.2 Mathematical Equations\n","The goal of RLHF is to optimize a policy (e.g., a language model) to maximize a reward signal provided by a learned reward model. The key mathematical components are:\n","\n","- **Policy**: A language model parameterized by $ \\theta $, denoted as $ \\pi_\\theta(y|x) $, which generates an output $ y $ given an input $ x $.\n","- **Reward Model**: A function $ RM_\\phi(x, y) $ parameterized by $ \\phi $, which produces a scalar reward $ r $ for a given input-output pair $ (x, y) $.\n","- **Objective**: Maximize the expected reward over the policyâ€™s outputs while ensuring the model does not deviate too far from its initial behavior (pre-trained or instruction-finetuned model). The RLHF objective is:\n","  $$\n","  J(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y|x)}[RM_\\phi(x, y)] - \\beta \\cdot D_{KL}(\\pi_\\theta(y|x) || \\pi_{\\text{ref}}(y|x))\n","  $$\n","  Here, $ \\beta $ is a hyperparameter controlling the strength of the KL-divergence penalty, and $ \\pi_{\\text{ref}} $ is the reference policy (usually the pre-trained model).\n","\n","- **Reward Model Training**: The reward model $ RM_\\phi $ is trained on a dataset of human pairwise preferences, where for each input $ x $, humans compare two outputs $ y_1 $ and $ y_2 $, indicating which is preferred (e.g., $ y_1 \\succ y_2 $). The reward model is trained to maximize the likelihood of human preferences using a Bradley-Terry model:\n","  $$\n","  p(y_1 \\succ y_2 | x) = \\frac{\\exp(RM_\\phi(x, y_1))}{\\exp(RM_\\phi(x, y_1)) + \\exp(RM_\\phi(x, y_2))}\n","  $$\n","  The loss function for training the reward model is:\n","  $$\n","  \\mathcal{L}(\\phi) = -\\mathbb{E}_{(x, y_1, y_2) \\sim \\mathcal{D}}[\\log p(y_1 \\succ y_2 | x)]\n","  $$\n","\n","- **Policy Optimization**: The policy $ \\pi_\\theta $ is optimized using a reinforcement learning algorithm, typically Proximal Policy Optimization (PPO), to maximize the expected reward while penalizing deviation from the reference policy.\n","\n","### 1.3 Core Principles\n","RLHF operates on the following core principles:\n","- **Human Feedback as a Signal**: Human preferences are used to construct a reward model, which serves as a proxy for human judgment.\n","- **Reinforcement Learning**: The policy is fine-tuned using RL to maximize the learned reward model, ensuring alignment with human preferences.\n","- **Regularization via KL-Divergence**: The KL-divergence penalty ensures that the fine-tuned model does not deviate excessively from its pre-trained behavior, preserving generalization and preventing overfitting to the reward model.\n","- **Online Sampling**: During RL training, the policy generates samples, which are evaluated by the reward model, and the policy is updated iteratively.\n","\n","### 1.4 Detailed Explanation of Concepts\n","#### 1.4.1 Pre-trained Model\n","The starting point for RLHF is a pre-trained language model, often instruction-finetuned, which already possesses general language understanding and generation capabilities. This model serves as the reference policy $ \\pi_{\\text{ref}} $ and provides a strong initialization for RLHF.\n","\n","#### 1.4.2 Reward Model\n","The reward model $ RM_\\phi $ is a critical component of RLHF. It is trained on a dataset of human comparisons, where humans evaluate pairs of outputs $ (y_1, y_2) $ for a given input $ x $ and indicate preferences. The reward model learns to assign higher scores to preferred outputs, enabling the policy to optimize for human-aligned behavior.\n","\n","#### 1.4.3 RL Optimization\n","RLHF employs reinforcement learning to fine-tune the policy $ \\pi_\\theta $. The RL algorithm (e.g., PPO) iteratively:\n","1. Samples outputs $ y $ from the current policy $ \\pi_\\theta $ given inputs $ x $.\n","2. Evaluates the reward $ RM_\\phi(x, y) $ for each sample.\n","3. Updates the policy parameters $ \\theta $ to maximize the expected reward, subject to the KL-divergence constraint.\n","\n","#### 1.4.4 KL-Divergence Penalty\n","The KL-divergence penalty ensures that the fine-tuned policy does not deviate too far from the reference policy, mitigating issues such as overfitting to the reward model or generating unrealistic outputs.\n","\n","### 1.5 Why RLHF is Important to Know\n","- **Alignment with Human Values**: RLHF enables models to align with complex human preferences, which are difficult to encode directly in supervised learning objectives.\n","- **Applications**: RLHF is widely used in applications requiring safe and helpful AI, such as chatbots, content generation, and decision-making systems.\n","- **Improvement over Pre-training and Fine-tuning**: RLHF provides significant gains over traditional pre-training and supervised fine-tuning by optimizing for user satisfaction and ethical considerations.\n","\n","### 1.6 Pros and Cons\n","#### Pros:\n","- **Effective Alignment**: RLHF effectively aligns models with human preferences, improving quality, safety, and usefulness.\n","- **Flexibility**: It can incorporate diverse types of human feedback, such as comparisons, rankings, or scalar ratings.\n","- **Generalization**: The KL-divergence penalty ensures the model retains generalization capabilities from pre-training.\n","\n","#### Cons:\n","- **Computational Expense**: RLHF is computationally expensive due to the need for online sampling, reward model training, and policy optimization.\n","- **Complexity**: The RLHF pipeline is complex, involving multiple components (reward model, RL algorithm, regularization) that require careful tuning.\n","- **Hyperparameter Sensitivity**: Performance is highly sensitive to hyperparameters, such as the KL-divergence penalty strength $ \\beta $ and learning rates.\n","- **Online Sampling Bottleneck**: Online sampling of outputs during RL training is slow, limiting scalability.\n","\n","### 1.7 Recent Advancements in RLHF\n","- **Improved RL Algorithms**: Advances in RL algorithms, such as Trust Region Policy Optimization (TRPO) and PPO, have improved the stability and efficiency of RLHF.\n","- **Scalable Reward Modeling**: Techniques for scaling reward model training, such as using synthetic data or active learning, have reduced reliance on human feedback.\n","- **Multi-objective RLHF**: Recent work has explored optimizing for multiple objectives (e.g., helpfulness and truthfulness) simultaneously, enhancing model robustness.\n","\n","---\n","\n","## 2. Direct Preference Optimization (DPO)\n","\n","### 2.1 Definition\n","Direct Preference Optimization (DPO) is a simpler and more efficient alternative to RLHF for aligning language models with human preferences. Instead of using reinforcement learning, DPO directly optimizes the policy to match human preferences by leveraging a closed-form relationship between the policy and the reward model. DPO eliminates the need for an explicit reward model during policy optimization, making it computationally more efficient.\n","\n","### 2.2 Mathematical Equations\n","DPO is based on the insight that human preferences can be directly used to optimize the policy without an intermediary reward model. The key mathematical components are:\n","\n","- **Preference Dataset**: A dataset of human comparisons, where for each input $ x $, two outputs $ y_w $ (preferred, \"winner\") and $ y_l $ (less preferred, \"loser\") are provided.\n","- **Policy Objective**: DPO optimizes the policy $ \\pi_\\theta $ to maximize the likelihood of human preferences, formulated as:\n","  $$\n","  p(y_w \\succ y_l | x) = \\frac{\\pi_\\theta(y_w|x)}{\\pi_\\theta(y_w|x) + \\pi_\\theta(y_l|x)}\n","  $$\n","  This is derived from the Bradley-Terry model, assuming an implicit reward model underlies the preferences.\n","\n","- **Loss Function**: DPO minimizes the following loss function, which encourages the policy to assign higher probabilities to preferred outputs:\n","  $$\n","  \\mathcal{L}_{\\text{DPO}}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]\n","  $$\n","  Here, $ \\sigma $ is the sigmoid function, $ \\beta $ is a hyperparameter controlling the strength of regularization, and $ \\pi_{\\text{ref}} $ is the reference policy (pre-trained model).\n","\n","- **Relationship to RLHF**: DPO can be shown to implicitly optimize a reward model, but it avoids the explicit RL step by directly updating the policy parameters $ \\theta $ using supervised learning.\n","\n","### 2.3 Core Principles\n","DPO operates on the following core principles:\n","- **Preference-based Learning**: Human preferences are directly used to guide policy optimization, bypassing the need for an explicit reward model.\n","- **Closed-form Solution**: DPO leverages a theoretical connection between preference probabilities and policy probabilities to derive a closed-form loss function.\n","- **Supervised Learning**: Unlike RLHF, DPO uses supervised learning techniques, making it simpler and more efficient.\n","- **Regularization**: Similar to RLHF, DPO includes a regularization term to prevent the policy from deviating excessively from the reference policy.\n","\n","### 2.4 Detailed Explanation of Concepts\n","#### 2.4.1 Preference Dataset\n","The starting point for DPO is a dataset of human comparisons, where each entry consists of an input $ x $ and a pair of outputs $ (y_w, y_l) $, with $ y_w $ being preferred over $ y_l $. This dataset is similar to the one used for training the reward model in RLHF.\n","\n","#### 2.4.2 Implicit Reward Model\n","DPO assumes that human preferences are generated according to an implicit reward model, but it does not explicitly train or use such a model. Instead, it directly optimizes the policy to match the observed preferences.\n","\n","#### 2.4.3 Policy Optimization\n","DPO optimizes the policy $ \\pi_\\theta $ by minimizing the DPO loss function, which encourages the policy to assign higher probabilities to preferred outputs. This optimization is performed using standard supervised learning techniques, such as gradient descent, making it simpler than RLHF.\n","\n","#### 2.4.4 Regularization\n","The DPO loss includes a regularization term that penalizes deviations from the reference policy $ \\pi_{\\text{ref}} $, similar to the KL-divergence penalty in RLHF. This ensures the model retains its pre-trained capabilities while aligning with human preferences.\n","\n","### 2.5 Why DPO is Important to Know\n","- **Simplicity and Efficiency**: DPO offers a simpler and more efficient alternative to RLHF, making it accessible for practitioners with limited computational resources.\n","- **Comparable Performance**: DPO has been shown to achieve performance comparable to RLHF in aligning models with human preferences, but with lower computational cost.\n","- **Scalability**: DPOâ€™s supervised learning approach scales better than RLHF, especially for large models and datasets.\n","\n","### 2.6 Pros and Cons\n","#### Pros:\n","- **Simplicity**: DPO eliminates the need for reinforcement learning, reducing complexity and implementation challenges.\n","- **Efficiency**: DPO is computationally more efficient than RLHF, as it avoids online sampling and RL optimization.\n","- **Stability**: DPO is less sensitive to hyperparameters compared to RLHF, making it easier to tune.\n","- **Scalability**: DPOâ€™s supervised learning approach scales well to large models and datasets.\n","\n","#### Cons:\n","- **Limited Expressiveness**: DPO relies on a specific form of the preference model (Bradley-Terry), which may not capture all nuances of human preferences.\n","- **Dependence on Reference Policy**: DPOâ€™s performance depends heavily on the quality of the reference policy, as it regularizes against it.\n","- **Less Explored**: DPO is a newer method compared to RLHF, so it has fewer empirical studies and less community knowledge.\n","\n","### 2.7 Recent Advancements in DPO\n","- **Theoretical Insights**: Recent work has provided deeper theoretical insights into the equivalence between DPO and RLHF, showing that DPO implicitly optimizes a reward model under certain conditions.\n","- **Extensions to Multi-objective Optimization**: DPO has been extended to optimize for multiple objectives (e.g., helpfulness and harmlessness) by modifying the preference model.\n","- **Integration with Other Methods**: DPO has been combined with techniques like contrastive learning and self-supervised learning to further improve alignment performance.\n","\n","---\n","\n","## 3. Comparison of RLHF and DPO\n","\n","| **Aspect**               | **RLHF**                              | **DPO**                              |\n","|--------------------------|---------------------------------------|--------------------------------------|\n","| **Methodology**          | Reinforcement learning               | Supervised learning                 |\n","| **Reward Model**         | Explicitly trained and used          | Implicit, not explicitly used       |\n","| **Computational Cost**   | High (online sampling, RL)           | Low (offline, supervised)           |\n","| **Complexity**           | High (multi-component pipeline)      | Low (single-step optimization)      |\n","| **Hyperparameter Sensitivity** | High                              | Low                                  |\n","| **Scalability**          | Limited by online sampling           | High                                |\n","| **Performance**          | Strong, but sensitive to tuning      | Comparable, more stable             |\n","\n","---\n","\n","## 4. Conclusion\n","Both RLHF and DPO are powerful methods for optimizing language models to align with human preferences, but they cater to different needs and constraints. RLHF, while complex and computationally expensive, offers a flexible and expressive framework for fine-tuning models, making it suitable for high-stakes applications. DPO, on the other hand, provides a simpler and more efficient alternative, making it ideal for scenarios with limited computational resources or when rapid deployment is needed. Understanding both methods is crucial for practitioners in NLP and AI alignment, as the choice between RLHF and DPO depends on the specific requirements of the task at hand. Recent advancements in both methods continue to push the boundaries of model alignment, promising even more robust and efficient solutions in the future."],"metadata":{"id":"MMEoTgs1b9Mh"}},{"cell_type":"markdown","source":["# RLHF and DPO\n","\n","## 1. Definition and Core Concepts\n","\n","Optimizing for human preferences refers to the process of aligning language model (LM) outputs with human preferences, values, and expectations. This approach moves beyond traditional maximum likelihood estimation (MLE) training to incorporate human judgment directly into model optimization.\n","\n","### 1.1 Mathematical Formulation of the Problem\n","\n","The fundamental goal is to find model parameters $\\theta$ that maximize the expected human preference reward:\n","\n","$$\\theta^* = \\arg\\max_{\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim p_{\\theta}(y|x)}[R(x, y)]$$\n","\n","Where:\n","- $x$ represents input prompts\n","- $y$ represents model outputs\n","- $\\mathcal{D}$ is the distribution of inputs\n","- $p_{\\theta}(y|x)$ is the policy (language model)\n","- $R(x, y)$ is the true (but unknown) human preference reward function\n","\n","## 2. Reinforcement Learning from Human Feedback (RLHF)\n","\n","### 2.1 RLHF Pipeline\n","\n","The RLHF process involves three primary stages:\n","\n","1. **Pretraining**: Create a base language model using self-supervised learning\n","2. **Reward Modeling**: Train a reward model from human preference data\n","3. **RL Fine-tuning**: Optimize the language model using the reward model\n","\n","### 2.2 Reward Model Training\n","\n","The reward model $\\text{RM}_{\\phi}(x, y)$ is trained on human comparison data:\n","\n","$$\\mathcal{L}_{\\text{RM}}(\\phi) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}_{\\text{pref}}}\\left[\\log\\sigma(\\text{RM}_{\\phi}(x, y_w) - \\text{RM}_{\\phi}(x, y_l))\\right]$$\n","\n","Where:\n","- $(x, y_w, y_l)$ represents an input prompt, a winning (preferred) response, and a losing response\n","- $\\mathcal{D}_{\\text{pref}}$ is the dataset of human preferences\n","- $\\sigma$ is the sigmoid function\n","\n","### 2.3 RL Optimization\n","\n","The pretrained model $p_{\\theta_{\\text{PT}}}(y|x)$ is fine-tuned to create $p_{\\theta}(y|x)$ using:\n","\n","$$\\mathcal{L}_{\\text{RLHF}}(\\theta) = -\\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim p_{\\theta}(y|x)}[\\text{RM}_{\\phi}(x, y) - \\beta D_{\\text{KL}}(p_{\\theta}(y|x) || p_{\\theta_{\\text{PT}}}(y|x))]$$\n","\n","Where:\n","- $D_{\\text{KL}}$ is the Kullback-Leibler divergence\n","- $\\beta$ is a hyperparameter controlling the strength of the KL penalty\n","\n","### 2.4 Proximal Policy Optimization (PPO)\n","\n","PPO is the most common RL algorithm used in RLHF:\n","\n","$$\\mathcal{L}_{\\text{PPO}}(\\theta) = \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim p_{\\theta_{\\text{old}}}(y|x)}\\left[\\min\\left(r_{\\theta}(x, y)A(x, y), \\text{clip}(r_{\\theta}(x, y), 1-\\epsilon, 1+\\epsilon)A(x, y)\\right)\\right]$$\n","\n","Where:\n","- $r_{\\theta}(x, y) = \\frac{p_{\\theta}(y|x)}{p_{\\theta_{\\text{old}}}(y|x)}$ is the probability ratio\n","- $A(x, y)$ is the advantage function\n","- $\\epsilon$ is a hyperparameter controlling the clip range\n","\n","### 2.5 Implementation Challenges\n","\n","RLHF faces several implementation challenges:\n","- Requires fitting value functions for advantage estimation\n","- Needs on-policy sampling, which is computationally expensive\n","- High sensitivity to hyperparameter choices\n","- Requires complex infrastructure for distributed training\n","\n","## 3. Direct Preference Optimization (DPO)\n","\n","### 3.1 Motivation and Key Insight\n","\n","DPO bypasses explicit reward modeling and directly optimizes the policy from preference data, simplifying the RLHF pipeline.\n","\n","### 3.2 Mathematical Derivation\n","\n","DPO leverages a key insight about the optimal reward model in the context of RLHF:\n","\n","$$\\text{RM}_{\\phi}(x, y) = \\beta \\log \\frac{p_{\\theta}(y|x)}{p_{\\text{ref}}(y|x)} + C(x)$$\n","\n","Where $p_{\\text{ref}}$ is the reference (pretrained) model and $C(x)$ is a constant that depends only on $x$.\n","\n","Substituting this into the preference modeling objective yields:\n","\n","$$\\mathcal{L}_{\\text{DPO}}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}_{\\text{pref}}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{p_{\\theta}(y_w|x)}{p_{\\text{ref}}(y_w|x)} - \\beta\\log\\frac{p_{\\theta}(y_l|x)}{p_{\\text{ref}}(y_l|x)}\\right)\\right]$$\n","\n","Simplifying:\n","\n","$$\\mathcal{L}_{\\text{DPO}}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}_{\\text{pref}}}\\left[\\log\\sigma\\left(\\beta\\log\\frac{p_{\\theta}(y_w|x)p_{\\text{ref}}(y_l|x)}{p_{\\theta}(y_l|x)p_{\\text{ref}}(y_w|x)}\\right)\\right]$$\n","\n","### 3.3 Implementation\n","\n","The DPO algorithm:\n","1. Start with a pretrained model $p_{\\text{ref}}(y|x)$\n","2. Create a dataset of preference pairs $(x, y_w, y_l)$\n","3. Train a new model $p_{\\theta}(y|x)$ using the DPO loss\n","\n","### 3.4 Advantages Over RLHF\n","\n","- Eliminates the need for a separate reward model\n","- No online RL sampling required\n","- Single-stage training process\n","- More stable optimization\n","- Uses standard supervised learning infrastructure\n","\n","## 4. Comparative Analysis: RLHF vs. DPO\n","\n","### 4.1 Computational Efficiency\n","\n","| Aspect | RLHF | DPO |\n","|--------|------|-----|\n","| Training stages | 3 (pretraining, reward modeling, RL) | 2 (pretraining, preference optimization) |\n","| Sampling requirements | On-policy sampling | Off-policy (dataset only) |\n","| Infrastructure complexity | High (RL framework) | Low (standard training) |\n","| Memory requirements | Higher (value functions, etc.) | Lower |\n","\n","### 4.2 Performance Characteristics\n","\n","| Aspect | RLHF | DPO |\n","|--------|------|-----|\n","| Sample efficiency | Lower | Higher |\n","| Control over KL divergence | Direct | Indirect (via $\\beta$) |\n","| Exploration capability | Higher | Lower |\n","| Sensitivity to hyperparameters | Higher | Lower |\n","\n","## 5. Importance of Optimizing for Human Preferences\n","\n","### 5.1 Alignment with Human Values\n","\n","- Enables models to follow complex human instructions\n","- Helps reduce harmful, unethical, or misleading outputs\n","- Facilitates better understanding of implicit constraints\n","\n","### 5.2 Practical Applications\n","\n","- Improving helpfulness in assistant-like applications\n","- Reducing toxicity and bias in generated content\n","- Enabling more nuanced reasoning about sensitive topics\n","- Enhancing factual accuracy and reducing hallucinations\n","\n","### 5.3 Long-term AI Safety\n","\n","- Provides a framework for aligning increasingly capable systems\n","- Helps bridge the gap between capability and alignment\n","- Creates methods for incorporating human oversight\n","\n","## 6. Pros and Cons\n","\n","### 6.1 Advantages\n","\n","- Direct incorporation of human judgment beyond what's possible with supervised learning\n","- Ability to optimize for subjective qualities not easily captured in labels\n","- Reduction in harmful outputs and improved safety properties\n","- Better instruction-following capabilities\n","\n","### 6.2 Limitations\n","\n","- Dependence on quality and diversity of human preference data\n","- Potential for preference gaming or reward hacking\n","- Scalability challenges with collecting high-quality preference data\n","- Difficulty in representing diverse human values\n","- Computational overhead compared to standard fine-tuning\n","\n","## 7. Recent Advancements\n","\n","### 7.1 DPO Variants\n","\n","- **Robust Preference Optimization (RPO)**: Addresses noise in preference data\n","- **Identity Preference Optimization (IPO)**: Simplifies the mathematical framework further\n","- **Kahneman-Tversky Optimization (KTO)**: Incorporates cognitive biases into preference modeling\n","- **Sequence Likelihood Calibration (SLiC)**: Improves token-by-token reward allocation\n","\n","### 7.2 RLHF Improvements\n","\n","- **Off-policy RLHF**: Reducing the need for expensive on-policy sampling\n","- **Rejection Sampling Fine-Tuning (RSF)**: Alternative to PPO with simpler implementation\n","- **Best-of-N Sampling**: Improving efficiency of reward model usage\n","\n","### 7.3 Combining Approaches\n","\n","- **Reinforced DPO**: Using DPO as initialization for RLHF\n","- **Constitutional AI**: Incorporating explicit rules and principles into preference optimization\n","- **Self-critique and iterative refinement**: Having models evaluate and improve their own outputs\n","\n","### 7.4 Scaling Properties\n","\n","- Emergence of better preference alignment with scale\n","- Improved sample efficiency at larger model sizes\n","- Better generalization to out-of-distribution preferences\n","\n","## 8. Conclusion\n","\n","Optimizing for human preferences represents a fundamental shift in how we align language models with human values and expectations. Both RLHF and DPO offer powerful frameworks for this optimization, with DPO providing a more streamlined approach that maintains or exceeds RLHF performance in many cases. These methods continue to evolve rapidly, with ongoing research addressing current limitations and expanding capabilities."],"metadata":{"id":"qy45ajt6cNM9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJtg24S2buYf"},"outputs":[],"source":[]}]}