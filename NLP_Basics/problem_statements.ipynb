{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qz1nFpD7hLe"
   },
   "source": [
    "# Quadratic Compute in Self-Attention Mechanisms\n",
    "\n",
    "## Definition\n",
    "\n",
    "Self-attention is a neural network mechanism that weighs the importance of different positions in a sequence for computing a representation of the sequence. Unlike recurrent models which process tokens sequentially, self-attention computes interactions between all pairs of tokens simultaneously, resulting in quadratic computational complexity with respect to sequence length.\n",
    "\n",
    "## Mathematical Formulation of Self-Attention\n",
    "\n",
    "In the standard self-attention mechanism, given an input sequence $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the sequence length and $d$ is the embedding dimension:\n",
    "\n",
    "1. Three projection matrices are used to create queries, keys, and values:\n",
    "$$Q = XW_Q, K = XW_K, V = XW_V$$\n",
    "   where \n",
    "$W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_k}$ are learnable parameters.\n",
    "\n",
    "2. The attention scores are computed as:\n",
    "   $$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "3. The output is calculated as:\n",
    "   $$O = AV$$\n",
    "\n",
    "## Computational Complexity Analysis\n",
    "\n",
    "### Self-Attention Complexity\n",
    "\n",
    "The computational bottleneck in self-attention occurs in the matrix multiplication $QK^T$:\n",
    "\n",
    "- $Q$ has dimensions $n \\times d_k$\n",
    "- $K^T$ has dimensions $d_k \\times n$\n",
    "- $QK^T$ has dimensions $n \\times n$\n",
    "- Computing $QK^T$ requires $O(n^2d_k)$ operations\n",
    "\n",
    "The total complexity for the self-attention operation is:\n",
    "$$O(n^2d_k) + O(n^2d_k) = O(n^2d_k)$$\n",
    "\n",
    "This demonstrates quadratic growth $O(n^2)$ with respect to sequence length $n$.\n",
    "\n",
    "### Recurrent Model Complexity\n",
    "\n",
    "In contrast, a recurrent neural network (RNN) processes tokens sequentially:\n",
    "\n",
    "For each timestep $t$ from $1$ to $n$:\n",
    "$$h_t = f(h_{t-1}, x_t)$$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the input at time $t$\n",
    "- $f$ is a function with fixed computational cost per timestep\n",
    "\n",
    "The total complexity is:\n",
    "$$O(n \\cdot c) = O(n)$$\n",
    "\n",
    "Where $c$ is the constant cost per timestep (dependent on hidden dimension but independent of sequence length).\n",
    "\n",
    "## Implications and Efficiency Challenges\n",
    "\n",
    "The quadratic complexity of self-attention presents significant challenges:\n",
    "\n",
    "1. **Memory constraints**: Storing the $n \\times n$ attention matrix requires $O(n^2)$ memory\n",
    "2. **Computational bottleneck**: Processing long sequences becomes prohibitively expensive\n",
    "3. **Context length limitations**: Most transformer models have practical limits (e.g., 2048, 4096, or 8192 tokens)\n",
    "\n",
    "## Efficiency Solutions\n",
    "\n",
    "Several approaches address the quadratic complexity issue:\n",
    "\n",
    "1. **Sparse attention patterns**: Only compute attention for a subset of token pairs\n",
    "$$O(n \\cdot s \\cdot d_k)$$ where $s \\ll n$\n",
    "\n",
    "2. **Linear attention mechanisms**: Reformulate attention to avoid explicit $n \\times n$ matrix\n",
    "$$\\text{Attention}(Q,K,V) = \\phi(Q)(\\phi(K)^TV)$$\n",
    "\n",
    "Where \n",
    "$\\phi$ is a kernel function allowing $O(n)$ complexity\n",
    "\n",
    "3. **Recurrence-enhanced models**: Combine self-attention with recurrent mechanisms to obtain benefits of both approaches\n",
    "\n",
    "4. **State space models**: Alternative sequence modeling approach with linear complexity\n",
    "$$x_t = Ax_{t-1} + Bu_t, y_t = Cx_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXtHV8c27TYk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWQYZ9zG70xK"
   },
   "source": [
    "# Quadratic Compute in Self-Attention: A Detailed Analysis\n",
    "\n",
    "In this exposition, we dive into the concept of quadratic compute complexity in self-attention mechanisms, a cornerstone of modern transformer-based architectures in natural language processing (NLP), large language models (LLMs), and generative AI (Gen-AI). We will contrast this with the linear complexity of recurrent models, providing a comprehensive end-to-end understanding of the topic. This analysis is tailored for AI researchers and scientists, emphasizing mathematical rigor, technical depth, and practical implications.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Definition of Self-Attention\n",
    "\n",
    "Self-attention is a mechanism used in transformer models to capture dependencies between tokens (e.g., words, subwords, or characters) in a sequence, regardless of their distance in the input. Unlike recurrent models, which process sequences sequentially, self-attention computes interactions between all pairs of tokens in parallel, enabling efficient modeling of long-range dependencies.\n",
    "\n",
    "### Key Idea of Self-Attention\n",
    "In self-attention, each token in a sequence attends to every other token, producing a weighted sum of token representations based on their relevance (attention scores). This process, however, comes at a computational cost, which we will analyze in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mathematical Formulation of Self-Attention\n",
    "\n",
    "To understand the computational complexity, let us first define the mathematical operations involved in self-attention.\n",
    "\n",
    "### 2.1 Input Representation\n",
    "Consider an input sequence of length $n$, represented as a matrix $X \\in \\mathbb{R}^{n \\times d}$, where:\n",
    "- $n$ is the sequence length (number of tokens),\n",
    "- $d$ is the dimensionality of each token's embedding.\n",
    "\n",
    "### 2.2 Query, Key, and Value Matrices\n",
    "In self-attention, the input $X$ is linearly transformed into three matrices: Query ($Q$), Key ($K$), and Value ($V$), using learned weight matrices $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$. These transformations are defined as:\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "Here, $Q, K, V \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "### 2.3 Attention Scores\n",
    "The attention mechanism computes a score for each pair of tokens, determining how much focus one token should place on another. The attention scores are computed as the scaled dot-product of queries and keys, followed by a softmax normalization:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d}} \\right)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $Q K^T \\in \\mathbb{R}^{n \\times n}$ is the matrix of raw attention scores, where each element $(i, j)$ represents the unnormalized attention score of token $i$ attending to token $j$.\n",
    "- $\\sqrt{d}$ is a scaling factor to stabilize gradients during training.\n",
    "- $A \\in \\mathbb{R}^{n \\times n}$ is the matrix of normalized attention weights, where each row sums to 1.\n",
    "\n",
    "### 2.4 Output of Self-Attention\n",
    "The final output of the self-attention layer is computed by weighting the value matrix $V$ with the attention weights $A$:\n",
    "\n",
    "$$\n",
    "\\text{Output} = A V\n",
    "$$\n",
    "\n",
    "Here, $\\text{Output} \\in \\mathbb{R}^{n \\times d}$, preserving the input dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Computational Complexity of Self-Attention\n",
    "\n",
    "Now, we analyze the computational cost of self-attention, focusing on why it grows quadratically with sequence length $n$.\n",
    "\n",
    "### 3.1 Step-by-Step Complexity Analysis\n",
    "\n",
    "#### Step 1: Computing $Q, K, V$\n",
    "The computation of $Q, K, V$ involves matrix multiplications of $X \\in \\mathbb{R}^{n \\times d}$ with $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$. The complexity of each matrix multiplication is:\n",
    "\n",
    "$$\n",
    "O(n d^2)\n",
    "$$\n",
    "\n",
    "Since this is done three times (for $Q$, $K$, and $V$), the total complexity for this step is:\n",
    "\n",
    "$$\n",
    "O(3 n d^2)\n",
    "$$\n",
    "\n",
    "#### Step 2: Computing Attention Scores ($Q K^T$)\n",
    "The raw attention scores are computed as $Q K^T$, where $Q \\in \\mathbb{R}^{n \\times d}$ and $K \\in \\mathbb{R}^{n \\times d}$. The resulting matrix $Q K^T \\in \\mathbb{R}^{n \\times n}$. The complexity of this matrix multiplication is:\n",
    "\n",
    "$$\n",
    "O(n^2 d)\n",
    "$$\n",
    "\n",
    "This step is critical because it involves computing interactions between all pairs of tokens, leading to a quadratic dependency on $n$.\n",
    "\n",
    "#### Step 3: Softmax Normalization\n",
    "The softmax operation is applied to each row of $Q K^T$ to produce the attention weights $A$. For each of the $n$ rows, we normalize over $n$ elements, so the complexity of this step is:\n",
    "\n",
    "$$\n",
    "O(n^2)\n",
    "$$\n",
    "\n",
    "#### Step 4: Computing the Output ($A V$)\n",
    "The attention weights $A \\in \\mathbb{R}^{n \\times n}$ are multiplied by $V \\in \\mathbb{R}^{n \\times d}$ to produce the output. The complexity of this matrix multiplication is:\n",
    "\n",
    "$$\n",
    "O(n^2 d)\n",
    "$$\n",
    "\n",
    "#### Step 5: Total Complexity\n",
    "Combining all steps, the total computational complexity of self-attention is dominated by the quadratic terms in $n$:\n",
    "\n",
    "$$\n",
    "O(n d^2 + n^2 d + n^2 + n^2 d)\n",
    "$$\n",
    "\n",
    "For large $n$, the $n^2 d$ terms dominate, so the overall complexity is:\n",
    "\n",
    "$$\n",
    "O(n^2 d)\n",
    "$$\n",
    "\n",
    "This confirms that the computation grows quadratically with respect to the sequence length $n$.\n",
    "\n",
    "### 3.2 Space Complexity\n",
    "In addition to computational complexity, self-attention also has a quadratic space complexity due to the storage of the attention matrix $A \\in \\mathbb{R}^{n \\times n}$. The space required is:\n",
    "\n",
    "$$\n",
    "O(n^2)\n",
    "$$\n",
    "\n",
    "This can become a significant bottleneck for long sequences, as modern GPUs have limited memory.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Comparison with Recurrent Models\n",
    "\n",
    "To appreciate the quadratic complexity of self-attention, let us contrast it with the computational complexity of recurrent models, such as RNNs or LSTMs, which are traditional architectures for sequence processing.\n",
    "\n",
    "### 4.1 Recurrent Model Overview\n",
    "In recurrent models, tokens are processed sequentially, with the hidden state $h_t$ at time $t$ depending on the current input $x_t$ and the previous hidden state $h_{t-1}$. The update rule for a simple RNN is:\n",
    "\n",
    "$$\n",
    "h_t = \\sigma(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $W_h \\in \\mathbb{R}^{d \\times d}$ is the hidden state weight matrix,\n",
    "- $W_x \\in \\mathbb{R}^{d \\times d}$ is the input weight matrix,\n",
    "- $\\sigma$ is a non-linear activation function,\n",
    "- $h_t, x_t \\in \\mathbb{R}^d$.\n",
    "\n",
    "### 4.2 Complexity Analysis of Recurrent Models\n",
    "For a sequence of length $n$, the recurrent model processes each token one at a time. At each time step $t$, the computation involves matrix-vector multiplications of $W_h h_{t-1}$ and $W_x x_t$, each with complexity $O(d^2)$. Thus, the complexity per time step is:\n",
    "\n",
    "$$\n",
    "O(d^2)\n",
    "$$\n",
    "\n",
    "Since there are $n$ time steps, the total computational complexity for the entire sequence is:\n",
    "\n",
    "$$\n",
    "O(n d^2)\n",
    "$$\n",
    "\n",
    "### 4.3 Space Complexity of Recurrent Models\n",
    "The space complexity of recurrent models is linear in $n$, as we only need to store the hidden state $h_t \\in \\mathbb{R}^d$ at each time step. Thus, the space complexity is:\n",
    "\n",
    "$$\n",
    "O(n d)\n",
    "$$\n",
    "\n",
    "### 4.4 Key Difference: Linear vs. Quadratic\n",
    "- **Recurrent Models**: Computational complexity grows linearly with sequence length, i.e., $O(n d^2)$.\n",
    "- **Self-Attention**: Computational complexity grows quadratically with sequence length, i.e., $O(n^2 d)$.\n",
    "\n",
    "This difference arises because recurrent models process tokens sequentially, considering only local dependencies at each step, whereas self-attention computes global dependencies by considering all pairs of tokens simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Implications of Quadratic Complexity\n",
    "\n",
    "The quadratic complexity of self-attention has significant practical implications, especially for long sequences. Below, we discuss the challenges and potential solutions.\n",
    "\n",
    "### 5.1 Challenges\n",
    "- **Memory Bottleneck**: The $O(n^2)$ space complexity of the attention matrix limits the maximum sequence length that can be processed on modern GPUs. For example, with $n = 10,000$, the attention matrix requires storing $10^8$ elements, which can exceed GPU memory limits.\n",
    "- **Compute Cost**: The $O(n^2 d)$ computational complexity makes training and inference prohibitively expensive for long sequences, especially in applications like document-level NLP, genomics, or high-resolution image processing (where $n$ can be very large).\n",
    "- **Scalability**: Quadratic complexity hinders the scalability of transformer models to tasks involving extremely long sequences, such as video processing or time-series analysis.\n",
    "\n",
    "### 5.2 Mitigating Quadratic Complexity\n",
    "To address the quadratic complexity of self-attention, researchers have proposed several efficient transformer variants. Below, we briefly outline some approaches:\n",
    "\n",
    "- **Sparse Attention Mechanisms**:\n",
    "  - Models like Longformer and BigBird reduce complexity by using sparse attention patterns, such as sliding windows or global tokens, achieving $O(n)$ or $O(n \\log n)$ complexity.\n",
    "  - Example: In Longformer, each token attends to a fixed-size window of neighboring tokens, reducing the attention matrix size from $n \\times n$ to $n \\times w$, where $w$ is the window size.\n",
    "\n",
    "- **Low-Rank Approximations**:\n",
    "  - Models like Linformer and Performer approximate the attention matrix $Q K^T$ using low-rank factorization, reducing complexity to $O(n d)$.\n",
    "  - Example: Performer uses kernel-based approximations to avoid explicitly computing the $n \\times n$ attention matrix.\n",
    "\n",
    "- **Memory-Efficient Implementations**:\n",
    "  - Techniques like gradient checkpointing and reversible layers reduce memory usage, enabling longer sequences to be processed within fixed memory budgets.\n",
    "\n",
    "- **Recurrence-Based Hybrids**:\n",
    "  - Models like Transformer-XL and Compressive Transformers combine recurrence with attention, achieving linear or near-linear complexity while retaining some benefits of global attention.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Example: Scaling Sequence Length\n",
    "\n",
    "To illustrate the impact of quadratic complexity, consider the following example:\n",
    "\n",
    "- **Sequence Length**: $n = 1,000$ vs. $n = 10,000$.\n",
    "- **Embedding Dimension**: $d = 512$.\n",
    "- **Hardware**: A GPU with 16 GB of memory.\n",
    "\n",
    "### 6.1 Self-Attention\n",
    "- For $n = 1,000$, the attention matrix $A$ has $1,000 \\times 1,000 = 10^6$ elements. Assuming each element is a 4-byte float, the memory required is $4 \\times 10^6 = 4$ MB, which is feasible.\n",
    "- For $n = 10,000$, the attention matrix has $10,000 \\times 10,000 = 10^8$ elements, requiring $4 \\times 10^8 = 400$ MB. This quickly scales beyond GPU memory limits when combined with other model components (e.g., activations, gradients).\n",
    "- Computational cost: For $n = 10,000$, the complexity is $O(10,000^2 \\times 512) = O(5.12 \\times 10^{10})$, which is 100 times higher than for $n = 1,000$.\n",
    "\n",
    "### 6.2 Recurrent Models\n",
    "- For $n = 1,000$, the complexity is $O(1,000 \\times 512^2) = O(2.62 \\times 10^8)$.\n",
    "- For $n = 10,000$, the complexity is $O(10,000 \\times 512^2) = O(2.62 \\times 10^9)$, which is only 10 times higher than for $n = 1,000$.\n",
    "- Memory usage remains linear, requiring $O(n d)$ space, making recurrent models more feasible for long sequences in terms of memory.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "The quadratic compute complexity of self-attention, arising from the need to compute all pairs of interactions, is a fundamental challenge in scaling transformer models to long sequences. Mathematically, this is expressed as $O(n^2 d)$ computational complexity and $O(n^2)$ space complexity, in stark contrast to the linear $O(n d^2)$ complexity of recurrent models. While self-attention enables powerful modeling of global dependencies, its quadratic scaling poses significant challenges for memory, compute, and scalability.\n",
    "\n",
    "For AI researchers and scientists, understanding this complexity is crucial for designing efficient models, selecting appropriate architectures for specific tasks, and exploring innovations in efficient attention mechanisms. Future research directions include developing hybrid models, leveraging sparsity, and exploring hardware-aware optimizations to mitigate the quadratic bottleneck while preserving the expressive power of transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TXWsV-y-HRx"
   },
   "source": [
    "# second problem statement\n",
    "\n",
    "# Position Representations in Neural Networks\n",
    "\n",
    "Position representations are critical in sequence modeling tasks, particularly in Natural Language Processing (NLP) and other domains involving sequential data. They enable models to capture the order of elements in a sequence, which is essential for understanding context, syntax, and semantics. In this detailed exposition, we explore the concept of position representations, starting with absolute position indices, and then delving into advanced techniques such as relative linear position attention and rotary embeddings. We aim to provide a comprehensive understanding of these methods, their mathematical underpinnings, and their implications for model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Definition of Position Representations\n",
    "\n",
    "Position representations refer to the mechanisms used to encode the position or order of tokens (e.g., words, characters, or other elements) in a sequence, enabling models to distinguish between different positions and capture sequential dependencies. In transformer-based architectures, which lack inherent sequential processing (unlike recurrent neural networks), explicit position representations are necessary to inject positional information into the model.\n",
    "\n",
    "Mathematically, a position representation is a function $f(p)$ that maps a position index $p \\in \\{1, 2, \\dots, n\\}$ (where $n$ is the sequence length) to a vector in a $d$-dimensional space, such that this vector can be added to or combined with token embeddings to provide positional context.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Absolute Position Indices\n",
    "\n",
    "### 2.1 Definition\n",
    "Absolute position indices are the simplest form of position representation, where each token in a sequence is assigned a unique integer index based on its position. These indices are then transformed into position embeddings, which are added to the token embeddings before being fed into the model.\n",
    "\n",
    "### 2.2 Mathematical Formulation\n",
    "Given a sequence of tokens $x_1, x_2, \\dots, x_n$, each token $x_i$ is represented by an embedding vector $e_i \\in \\mathbb{R}^d$. The position index $p_i = i$ is mapped to a position embedding $pe_i \\in \\mathbb{R}^d$ using a fixed or learned function. The input to the model is then:\n",
    "\n",
    "$$ z_i = e_i + pe_i $$\n",
    "\n",
    "#### Fixed Absolute Position Embeddings (Sinusoidal)\n",
    "In the original transformer model (Vaswani et al., 2017), sinusoidal position embeddings are used, defined as:\n",
    "\n",
    "$$ pe(p, 2k) = \\sin\\left(\\frac{p}{10000^{2k/d}}\\right) $$\n",
    "$$ pe(p, 2k+1) = \\cos\\left(\\frac{p}{10000^{2k/d}}\\right) $$\n",
    "\n",
    "where:\n",
    "- $p$ is the position index,\n",
    "- $k$ is the dimension index ($0 \\leq k < d/2$),\n",
    "- $d$ is the embedding dimension.\n",
    "\n",
    "These embeddings are fixed (not learned) and provide a unique encoding for each position, with the wavelength of the sinusoidal functions increasing exponentially with dimension, ensuring that positions are distinguishable.\n",
    "\n",
    "#### Learned Absolute Position Embeddings\n",
    "Alternatively, position embeddings can be treated as learnable parameters, initialized randomly and optimized during training. In this case, $pe_i$ is a vector in a lookup table $PE \\in \\mathbb{R}^{n \\times d}$, where $PE[p]$ is the embedding for position $p$.\n",
    "\n",
    "### 2.3 Advantages\n",
    "- **Simplicity:** Absolute position indices are straightforward to implement and computationally efficient.\n",
    "- **Uniqueness:** Each position is assigned a distinct embedding, making it easy for the model to differentiate positions.\n",
    "- **Generalization:** Sinusoidal embeddings generalize well to sequences longer than those seen during training, as they are deterministic and not tied to a specific sequence length.\n",
    "\n",
    "### 2.4 Limitations\n",
    "- **Lack of Relative Information:** Absolute position embeddings do not explicitly encode the relative distance between tokens, which is often more important in tasks requiring understanding of local context or syntax (e.g., \"the cat on the mat\" vs. \"the mat on the cat\").\n",
    "- **Fixed Encoding:** Sinusoidal embeddings are not adaptable to task-specific requirements, and learned embeddings may not generalize well to unseen sequence lengths.\n",
    "- **Scalability:** For very long sequences, absolute embeddings require a large lookup table (in the learned case) or may fail to capture meaningful long-range dependencies (in the sinusoidal case).\n",
    "\n",
    "### 2.5 Are Absolute Indices the Best We Can Do?\n",
    "The simplicity of absolute position indices makes them a good baseline, but they are not optimal for all tasks. Advanced techniques, such as relative position representations and rotary embeddings, address some of the limitations by focusing on relative distances between tokens and improving generalization. We now explore these advanced methods in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Relative Linear Position Attention\n",
    "\n",
    "### 3.1 Definition\n",
    "Relative linear position attention is a mechanism that incorporates the relative distance between tokens directly into the attention computation, rather than relying on absolute position embeddings. This approach is particularly effective for tasks where the relative ordering of tokens is more important than their absolute positions, such as in natural language understanding and generation.\n",
    "\n",
    "### 3.2 Mathematical Formulation\n",
    "In standard transformer attention, the attention score between a query $q_i$ (at position $i$) and a key $k_j$ (at position $j$) is computed as:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} $$\n",
    "\n",
    "where $d_k$ is the dimensionality of the key vectors. The attention weights are then obtained via a softmax operation:\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l=1}^n \\exp(a_{il})} $$\n",
    "\n",
    "In relative linear position attention, the attention score is augmented with a term that depends on the relative distance $r_{ij} = i - j$ between positions $i$ and $j$. This is achieved by introducing relative position embeddings $r_{ij}$, which are added to the attention computation. The modified attention score is:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j + q_i \\cdot r_{ij}}{\\sqrt{d_k}} $$\n",
    "\n",
    "Here, $r_{ij}$ is a vector that encodes the relative distance $i - j$. To make this scalable, the relative position embeddings are often parameterized as a function of the distance, rather than storing a separate embedding for every possible pair of positions.\n",
    "\n",
    "#### Relative Position Embeddings\n",
    "The relative position embeddings $r_{ij}$ can be implemented in several ways:\n",
    "- **Clamped Relative Positions:** To limit the number of parameters, the relative distance $i - j$ is often clamped to a maximum value $m$, such that $r_{ij} = r_{\\text{clamp}(i-j, -m, m)}$. The embeddings $r_k$ for $k \\in \\{-m, \\dots, m\\}$ are then learned parameters.\n",
    "- **Sinusoidal Relative Positions:** Similar to absolute sinusoidal embeddings, relative distances can be encoded using sinusoidal functions, but based on $i - j$ rather than $i$ or $j$ individually.\n",
    "\n",
    "#### Efficient Implementation\n",
    "To make relative position attention computationally efficient, the attention computation can be reformulated using a technique called \"relative position bias.\" Instead of explicitly adding $q_i \\cdot r_{ij}$ to the attention scores, the relative position terms are precomputed and added as a bias matrix to the attention logits. This approach, used in models like T5 (Raffel et al., 2020), significantly reduces memory and computational overhead.\n",
    "\n",
    "### 3.3 Advantages\n",
    "- **Relative Focus:** By encoding relative distances, this approach naturally captures local context and syntactic relationships, which are critical in tasks like language modeling and translation.\n",
    "- **Generalization:** Relative position attention generalizes better to longer sequences, as it does not depend on absolute positions but only on the distances between tokens.\n",
    "- **Efficiency:** With techniques like relative position bias, this method can be implemented efficiently, even for long sequences.\n",
    "\n",
    "### 3.4 Limitations\n",
    "- **Complexity:** Relative position attention introduces additional complexity in the attention mechanism, requiring careful implementation to maintain efficiency.\n",
    "- **Task Dependence:** While effective for tasks requiring relative context, it may not be optimal for tasks where absolute positions are more important (e.g., time-series data with fixed temporal references).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Rotary Embeddings (RoPE)\n",
    "\n",
    "### 4.1 Definition\n",
    "Rotary Position Embeddings (RoPE) are a type of relative position encoding that uses a rotation operation in the embedding space to incorporate positional information. Introduced in the context of transformer models, RoPE is designed to encode relative positions efficiently while preserving the benefits of absolute position encodings. Unlike relative linear position attention, RoPE modifies the query and key vectors directly, embedding positional information into the attention mechanism in a multiplicative manner.\n",
    "\n",
    "### 4.2 Mathematical Formulation\n",
    "RoPE operates by applying a rotation to the query and key vectors based on their positions. For a position $p$, the embedding vector $x \\in \\mathbb{R}^d$ is transformed using a rotation matrix that depends on $p$. To achieve this, the embedding space is treated as a series of 2D planes, and each pair of dimensions $(2k, 2k+1)$ is rotated by an angle proportional to $p$.\n",
    "\n",
    "#### Rotation Matrix\n",
    "For a pair of dimensions $(2k, 2k+1)$, the rotation matrix is:\n",
    "\n",
    "$$ R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix} $$\n",
    "\n",
    "The angle $\\theta$ is chosen to depend on the position $p$ and the dimension index $k$. Specifically:\n",
    "\n",
    "$$ \\theta_k(p) = p \\cdot \\omega_k $$\n",
    "\n",
    "where $\\omega_k$ is a frequency that varies with dimension, typically set as:\n",
    "\n",
    "$$ \\omega_k = \\frac{1}{10000^{2k/d}} $$\n",
    "\n",
    "This is similar to the sinusoidal position embeddings, but instead of adding a position vector, RoPE applies a rotation.\n",
    "\n",
    "#### Applying RoPE\n",
    "For a token embedding $x_i \\in \\mathbb{R}^d$ at position $i$, the rotated embedding $x_i'$ is computed by applying the rotation matrix to each pair of dimensions. For simplicity, consider the embedding as a series of 2D vectors $[x_{i,2k}, x_{i,2k+1}]$. The rotated embedding is:\n",
    "\n",
    "$$ x_{i,2k}' = x_{i,2k} \\cos\\theta_k(i) - x_{i,2k+1} \\sin\\theta_k(i) $$\n",
    "$$ x_{i,2k+1}' = x_{i,2k} \\sin\\theta_k(i) + x_{i,2k+1} \\cos\\theta_k(i) $$\n",
    "\n",
    "In matrix form, the full embedding $x_i$ is transformed as:\n",
    "\n",
    "$$ x_i' = R(\\theta(i)) x_i $$\n",
    "\n",
    "where $R(\\theta(i))$ is a block-diagonal matrix with 2D rotation matrices along the diagonal.\n",
    "\n",
    "#### Attention with RoPE\n",
    "In the attention mechanism, the query and key vectors $q_i$ and $k_j$ are rotated based on their positions $i$ and $j$, respectively:\n",
    "\n",
    "$$ q_i' = R(\\theta(i)) q_i $$\n",
    "$$ k_j' = R(\\theta(j)) k_j $$\n",
    "\n",
    "The attention score is then:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i' \\cdot k_j'}{\\sqrt{d_k}} = \\frac{(R(\\theta(i)) q_i) \\cdot (R(\\theta(j)) k_j)}{\\sqrt{d_k}} $$\n",
    "\n",
    "A key property of RoPE is that the dot product $q_i' \\cdot k_j'$ depends only on the relative distance $i - j$. This can be shown by noting that:\n",
    "\n",
    "$$ q_i' \\cdot k_j' = q_i \\cdot (R(\\theta(i))^T R(\\theta(j))) k_j $$\n",
    "\n",
    "Since $R(\\theta)$ is a rotation matrix, $R(\\theta)^T R(\\phi) = R(\\phi - \\theta)$, so:\n",
    "\n",
    "$$ q_i' \\cdot k_j' = q_i \\cdot R(\\theta(j) - \\theta(i)) k_j $$\n",
    "\n",
    "Thus, the attention score depends on $i - j$, naturally encoding relative positions without additional parameters.\n",
    "\n",
    "### 4.3 Advantages\n",
    "- **Relative Encoding:** RoPE inherently encodes relative positions, making it effective for tasks requiring local context and syntactic understanding.\n",
    "- **Parameter Efficiency:** Unlike learned position embeddings, RoPE has no additional parameters, as the rotations are deterministic functions of position.\n",
    "- **Generalization:** RoPE generalizes well to longer sequences, as the rotation angles are continuous and not tied to a specific sequence length.\n",
    "- **Stability:** The rotation operation preserves the norm of the vectors, ensuring numerical stability during training.\n",
    "\n",
    "### 4.4 Limitations\n",
    "- **Complexity:** Implementing RoPE requires careful handling of the rotation operations, which may increase computational overhead compared to absolute position embeddings.\n",
    "- **Task Dependence:** While effective for relative position tasks, RoPE may not be optimal for tasks where absolute positions are critical.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Comparison of Position Representations\n",
    "\n",
    "To provide a comprehensive understanding, we compare absolute position indices, relative linear position attention, and rotary embeddings across several dimensions:\n",
    "\n",
    "| **Aspect**               | **Absolute Position Indices**       | **Relative Linear Position Attention** | **Rotary Embeddings (RoPE)** |\n",
    "|--------------------------|-------------------------------------|----------------------------------------|-----------------------------|\n",
    "| **Encoding Type**        | Absolute                          | Relative                              | Relative                    |\n",
    "| **Mechanism**            | Addition of position embeddings   | Addition of relative position terms in attention | Rotation of query/key vectors |\n",
    "| **Parameters**           | Fixed (sinusoidal) or learned     | Learned (relative embeddings or bias) | None (deterministic)        |\n",
    "| **Generalization**       | Limited (learned) or good (sinusoidal) | Good                                  | Excellent                   |\n",
    "| **Computational Cost**   | Low                               | Moderate                              | Moderate                    |\n",
    "| **Task Suitability**     | General                           | Tasks requiring relative context      | Tasks requiring relative context |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Considerations\n",
    "\n",
    "### 6.1 Choosing the Right Position Representation\n",
    "- **Task Requirements:** For tasks where absolute positions are critical (e.g., time-series data with fixed temporal references), absolute position embeddings may suffice. For tasks requiring relative context (e.g., NLP, speech processing), relative linear position attention or RoPE is preferable.\n",
    "- **Sequence Length:** For long sequences, RoPE and relative position attention are more scalable, as they do not require storing embeddings for every position.\n",
    "- **Computational Resources:** Absolute position embeddings are the most efficient, while RoPE and relative position attention introduce additional computational overhead.\n",
    "\n",
    "### 6.2 Implementation Tips\n",
    "- **Framework Support:** Modern deep learning frameworks like PyTorch and TensorFlow provide built-in support for absolute position embeddings and rotary embeddings (e.g., via libraries like Hugging Face Transformers).\n",
    "- **Evaluation Metrics:** When experimenting with position representations, evaluate model performance on metrics sensitive to positional understanding, such as BLEU score (for translation) or perplexity (for language modeling).\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "Position representations are a cornerstone of sequence modeling, enabling models to capture the order and context of tokens. While absolute position indices provide a simple and effective baseline, they are limited by their inability to encode relative distances and their lack of adaptability. Relative linear position attention addresses these limitations by incorporating relative distances into the attention mechanism, making it suitable for tasks requiring local context. Rotary embeddings (RoPE) offer a parameter-efficient and generalizable alternative, encoding relative positions through rotations in the embedding space.\n",
    "\n",
    "For researchers and AI scientists, understanding these methods in detail—along with their mathematical formulations and practical implications—is crucial for designing state-of-the-art models tailored to specific tasks. By carefully selecting and implementing position representations, one can significantly enhance model performance, scalability, and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BkHukjQ-ZPp"
   },
   "source": [
    "# Position Representations in Transformer Models\n",
    "\n",
    "## 1. Absolute Position Representations\n",
    "\n",
    "### Definition\n",
    "Position representations encode sequential order in transformer models, which are otherwise permutation-invariant due to their self-attention mechanism.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Standard absolute positional encodings in the original transformer are sinusoidal:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(pos/10000^{2i/d_{model}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(pos/10000^{2i/d_{model}}\\right)$$\n",
    "\n",
    "Where $pos$ is the position, $i$ is the dimension index, and $d_{model}$ is the embedding dimension.\n",
    "\n",
    "### Limitations\n",
    "Absolute position embeddings:\n",
    "- Struggle with extrapolation beyond training sequence lengths\n",
    "- Cannot efficiently model relative distances between tokens\n",
    "- May not efficiently transfer positional information in deep networks\n",
    "\n",
    "## 2. Relative Linear Position Attention\n",
    "\n",
    "### Definition\n",
    "Relative position attention incorporates token-to-token distance information directly into the attention mechanism rather than adding position encodings to token embeddings.\n",
    "\n",
    "### Mathematical Formulation\n",
    "In Shaw et al.'s approach:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T + S}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where $S_{ij} = a_{ij}^K$ represents relative position information between positions $i$ and $j$.\n",
    "\n",
    "For Transformer-XL's relative attention:\n",
    "\n",
    "$$A_{i,j} = \\frac{(W_q x_i)^T(W_k x_j + R_{i-j}^K)}{\\sqrt{d}}$$\n",
    "\n",
    "Where $R_{i-j}^K$ are learned relative position embeddings for key vectors.\n",
    "\n",
    "### Advantages\n",
    "- Models token relationships directly through relative distances\n",
    "- Generalizes better to unseen sequence lengths\n",
    "- More parameter-efficient than absolute positional encodings\n",
    "\n",
    "## 3. Rotary Position Embeddings (RoPE)\n",
    "\n",
    "### Definition\n",
    "Rotary Position Embeddings (RoPE) encode relative position by applying rotation transformations to query and key vectors in the complex plane.\n",
    "\n",
    "### Mathematical Formulation\n",
    "For a token at position $m$ with embedding vector $\\mathbf{x}_m$, RoPE applies:\n",
    "\n",
    "$$f(\\mathbf{x}_m, m) = e^{im\\theta} \\mathbf{x}_m$$\n",
    "\n",
    "For the 2D case, a pair of dimensions $(x_{m,2d}, x_{m,2d+1})$ is transformed as:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\cos(m\\theta_d) & -\\sin(m\\theta_d) \\\\\n",
    "\\sin(m\\theta_d) & \\cos(m\\theta_d)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{m,2d} \\\\\n",
    "x_{m,2d+1}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Where $\\theta_d = 10000^{-2d/D}$, $D$ is the embedding dimension, and $m$ is the position index.\n",
    "\n",
    "The dot product of vectors at positions $m$ and $n$ becomes:\n",
    "\n",
    "$$\\langle f(\\mathbf{q}_m, m), f(\\mathbf{k}_n, n) \\rangle = \\langle e^{im\\theta}\\mathbf{q}_m, e^{in\\theta}\\mathbf{k}_n \\rangle = e^{i(m-n)\\theta}\\langle \\mathbf{q}_m, \\mathbf{k}_n \\rangle$$\n",
    "\n",
    "This naturally encodes the relative position $(m-n)$.\n",
    "\n",
    "### Advantages\n",
    "- Explicit encoding of relative positions\n",
    "- Translation equivariance by design\n",
    "- Improves extrapolation to longer sequences\n",
    "- Superior performance in causal language models\n",
    "- Computationally efficient implementation via complex number operations\n",
    "- Preserves token content information while adding positional context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0szFrTPD5pM"
   },
   "source": [
    "# Attention Mechanisms and BIGBIRD: Random, Window, and Global Attention\n",
    "\n",
    "Attention mechanisms are pivotal in modern sequence modeling, particularly in transformer-based architectures for tasks in NLP, computer vision, and beyond. They allow models to focus on relevant parts of the input sequence, enabling effective handling of long-range dependencies. This exposition explores three specific attention mechanisms—random attention, window attention, and global attention—followed by a detailed analysis of the BIGBIRD model, which integrates these mechanisms to achieve efficient and scalable attention for long sequences. Each concept is presented with rigorous mathematical formulations, detailed explanations, and practical considerations, ensuring a comprehensive understanding for researchers and AI scientists.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Random Attention\n",
    "\n",
    "### 1.1 Definition\n",
    "Random attention is a sparse attention mechanism that reduces computational complexity by randomly selecting a subset of key-value pairs for each query to attend to, instead of computing attention scores over the entire sequence. This approach is particularly useful for long sequences, where full attention is computationally expensive.\n",
    "\n",
    "### 1.2 Mathematical Formulation\n",
    "In standard attention, for a sequence of length $n$, the attention mechanism computes scores for each query $q_i \\in \\mathbb{R}^d$ (at position $i$) against all keys $k_j \\in \\mathbb{R}^d$ (at positions $j = 1, \\dots, n$):\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}} $$\n",
    "\n",
    "The attention weights are then obtained via a softmax operation:\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l=1}^n \\exp(a_{il})} $$\n",
    "\n",
    "The output for query $i$ is:\n",
    "\n",
    "$$ o_i = \\sum_{j=1}^n \\alpha_{ij} v_j $$\n",
    "\n",
    "In random attention, instead of attending to all $n$ keys, each query $q_i$ attends to a randomly sampled subset of $m$ positions, where $m \\ll n$. Let $\\mathcal{S}_i \\subset \\{1, 2, \\dots, n\\}$ be the set of $m$ randomly selected indices for query $i$. The attention computation becomes:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}, \\quad j \\in \\mathcal{S}_i $$\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l \\in \\mathcal{S}_i} \\exp(a_{il})} $$\n",
    "\n",
    "$$ o_i = \\sum_{j \\in \\mathcal{S}_i} \\alpha_{ij} v_j $$\n",
    "\n",
    "### 1.3 Detailed Explanation\n",
    "- **Random Sampling Strategy:** The subset $\\mathcal{S}_i$ is typically sampled uniformly at random, ensuring that each position has an equal chance of being selected. This randomness introduces a form of stochastic regularization, potentially improving generalization.\n",
    "- **Sparsity:** By reducing the number of key-value pairs from $n$ to $m$, the computational complexity of attention for each query drops from $O(n)$ to $O(m)$, leading to a total complexity of $O(nm)$ per layer, compared to $O(n^2)$ for full attention.\n",
    "- **Trade-offs:** While random attention reduces computational cost, it sacrifices some information, as queries do not attend to all possible positions. This can lead to degraded performance on tasks requiring fine-grained context, but it is often effective for tasks where global context is less critical.\n",
    "\n",
    "### 1.4 Advantages\n",
    "- **Efficiency:** Significant reduction in memory and computation, especially for long sequences.\n",
    "- **Scalability:** Suitable for very long sequences where full attention is infeasible.\n",
    "- **Regularization:** Random sampling can act as a form of dropout, potentially improving robustness.\n",
    "\n",
    "### 1.5 Limitations\n",
    "- **Information Loss:** Random sampling may miss important context, particularly for tasks requiring long-range dependencies.\n",
    "- **Non-deterministic:** The randomness can lead to variability in model performance, necessitating careful tuning of $m$ and multiple runs for stability.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Window Attention\n",
    "\n",
    "### 2.1 Definition\n",
    "Window attention, also known as local or sliding window attention, restricts the attention mechanism to a fixed-size window around each query position. This approach assumes that local context is more important than distant context, making it suitable for tasks where nearby tokens provide the most relevant information, such as in language modeling or speech processing.\n",
    "\n",
    "### 2.2 Mathematical Formulation\n",
    "In window attention, for each query $q_i$ at position $i$, attention is computed over a window of size $w$ centered around $i$. The window includes positions $j$ such that $|i - j| \\leq w/2$. Let $\\mathcal{W}_i = \\{j \\mid |i - j| \\leq w/2\\}$ be the set of positions in the window. The attention computation is:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}, \\quad j \\in \\mathcal{W}_i $$\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l \\in \\mathcal{W}_i} \\exp(a_{il})} $$\n",
    "\n",
    "$$ o_i = \\sum_{j \\in \\mathcal{W}_i} \\alpha_{ij} v_j $$\n",
    "\n",
    "### 2.3 Detailed Explanation\n",
    "- **Window Size:** The parameter $w$ determines the size of the context window. A larger $w$ captures more context but increases computational cost, while a smaller $w$ is more efficient but may miss important distant dependencies.\n",
    "- **Boundary Handling:** For positions near the start or end of the sequence, the window may extend beyond the sequence boundaries. In such cases, padding or truncation strategies are used to ensure the window size remains consistent.\n",
    "- **Dilated Windows:** To capture longer-range dependencies without increasing $w$, dilated window attention can be used, where the window includes positions at intervals (e.g., $j = i - 2w, i - w, i, i + w, i + 2w$).\n",
    "\n",
    "### 2.4 Advantages\n",
    "- **Efficiency:** Computational complexity is reduced from $O(n^2)$ to $O(nw)$, making it suitable for long sequences.\n",
    "- **Locality:** Effective for tasks where local context is dominant, such as in syntactic parsing or speech processing.\n",
    "- **Deterministic:** Unlike random attention, window attention is deterministic, ensuring consistent behavior.\n",
    "\n",
    "### 2.5 Limitations\n",
    "- **Limited Context:** Window attention cannot capture long-range dependencies outside the window, which can be problematic for tasks requiring global context.\n",
    "- **Hyperparameter Sensitivity:** Performance is sensitive to the choice of window size $w$, requiring careful tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Global Attention\n",
    "\n",
    "### 3.1 Definition\n",
    "Global attention allows certain tokens, referred to as global tokens, to attend to all positions in the sequence, and vice versa (all positions can attend to global tokens). This mechanism is designed to capture long-range dependencies efficiently by introducing a small set of global tokens that act as information hubs, reducing the need for full attention across all positions.\n",
    "\n",
    "### 3.2 Mathematical Formulation\n",
    "Let $\\mathcal{G} \\subset \\{1, 2, \\dots, n\\}$ be the set of indices corresponding to global tokens, with $|\\mathcal{G}| = g$, where $g \\ll n$. The attention mechanism is modified as follows:\n",
    "- **Global Tokens:** For a query $q_i$ where $i \\in \\mathcal{G}$, attention is computed over all positions $j = 1, \\dots, n$:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}, \\quad j = 1, \\dots, n $$\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l=1}^n \\exp(a_{il})} $$\n",
    "\n",
    "$$ o_i = \\sum_{j=1}^n \\alpha_{ij} v_j $$\n",
    "\n",
    "- **Non-Global Tokens:** For a query $q_i$ where $i \\notin \\mathcal{G}$, attention is computed over a restricted set of positions, typically including the global tokens $\\mathcal{G}$ and possibly a local window $\\mathcal{W}_i$:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}, \\quad j \\in \\mathcal{G} \\cup \\mathcal{W}_i $$\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l \\in \\mathcal{G} \\cup \\mathcal{W}_i} \\exp(a_{il})} $$\n",
    "\n",
    "$$ o_i = \\sum_{j \\in \\mathcal{G} \\cup \\mathcal{W}_i} \\alpha_{ij} v_j $$\n",
    "\n",
    "### 3.3 Detailed Explanation\n",
    "- **Global Tokens:** The choice of global tokens is task-dependent. They can be predefined (e.g., special tokens like CLS in BERT), learned during training, or selected based on heuristics (e.g., uniformly spaced positions).\n",
    "- **Information Flow:** Global tokens act as bottlenecks, aggregating information from the entire sequence and disseminating it to other positions, thus enabling long-range dependencies without full attention.\n",
    "- **Sparsity:** By limiting full attention to a small set of global tokens, the computational complexity is significantly reduced compared to full attention.\n",
    "\n",
    "### 3.4 Advantages\n",
    "- **Long-Range Dependencies:** Global attention effectively captures long-range dependencies through global tokens, making it suitable for tasks requiring global context, such as document summarization.\n",
    "- **Efficiency:** Computational complexity is reduced from $O(n^2)$ to $O(gn + nw)$, where $g$ is the number of global tokens and $w$ is the window size for non-global tokens.\n",
    "- **Flexibility:** The number and placement of global tokens can be adjusted based on task requirements.\n",
    "\n",
    "### 3.5 Limitations\n",
    "- **Information Bottleneck:** The effectiveness of global attention depends on the choice of global tokens, which may not always capture all relevant information.\n",
    "- **Hyperparameter Tuning:** The number of global tokens $g$ and their placement require careful tuning, adding complexity to model design.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. BIGBIRD: A Unified Sparse Attention Mechanism\n",
    "\n",
    "### 4.1 Definition\n",
    "BIGBIRD (Zaheer et al., 2020) is a transformer-based model that combines random attention, window attention, and global attention into a unified sparse attention mechanism. It is designed to handle long sequences efficiently while maintaining performance comparable to full attention transformers, particularly for tasks in NLP, such as document understanding and question answering.\n",
    "\n",
    "### 4.2 Mathematical Formulation\n",
    "BIGBIRD’s attention mechanism integrates the three sparse attention types described above, ensuring that each query attends to a carefully chosen subset of positions. For a query $q_i$ at position $i$, the set of attended positions $\\mathcal{A}_i$ is defined as the union of three components:\n",
    "- **Window Attention Component:** A local window $\\mathcal{W}_i$ of size $w$ centered around $i$, i.e., $\\mathcal{W}_i = \\{j \\mid |i - j| \\leq w/2\\}$.\n",
    "- **Global Attention Component:** A set of global tokens $\\mathcal{G}$, where each global token attends to all positions, and all positions attend to global tokens.\n",
    "- **Random Attention Component:** A set of $r$ randomly sampled positions $\\mathcal{R}_i \\subset \\{1, 2, \\dots, n\\}$, where $r \\ll n$.\n",
    "\n",
    "Thus, the attention set for query $i$ is:\n",
    "\n",
    "$$ \\mathcal{A}_i = \\mathcal{W}_i \\cup \\mathcal{G} \\cup \\mathcal{R}_i $$\n",
    "\n",
    "The attention computation is then:\n",
    "\n",
    "$$ a_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}, \\quad j \\in \\mathcal{A}_i $$\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(a_{ij})}{\\sum_{l \\in \\mathcal{A}_i} \\exp(a_{il})} $$\n",
    "\n",
    "$$ o_i = \\sum_{j \\in \\mathcal{A}_i} \\alpha_{ij} v_j $$\n",
    "\n",
    "For global tokens $i \\in \\mathcal{G}$, the attention set is the entire sequence:\n",
    "\n",
    "$$ \\mathcal{A}_i = \\{1, 2, \\dots, n\\} $$\n",
    "\n",
    "### 4.3 Detailed Explanation\n",
    "- **Integration of Mechanisms:** BIGBIRD ensures that each query has access to local context (via window attention), global context (via global attention), and a diverse set of distant positions (via random attention). This combination balances efficiency and expressiveness.\n",
    "- **Theoretical Justification:** BIGBIRD is designed based on theoretical insights into graph connectivity. The authors prove that the combined sparse attention graph (with window, global, and random connections) is a universal approximator of sequence-to-sequence functions, similar to full attention transformers, under certain conditions.\n",
    "- **Implementation Variants:** BIGBIRD offers two variants:\n",
    "  - **BIGBIRD-ITC (Internal Transformer Construction):** Uses learned global tokens, where certain positions are designated as global based on model training.\n",
    "  - **BIGBIRD-ETC (Extended Transformer Construction):** Uses predefined global tokens, such as special tokens (e.g., CLS), which is particularly useful for tasks like classification or question answering.\n",
    "\n",
    "### 4.4 Computational Complexity\n",
    "The computational complexity of BIGBIRD’s attention mechanism is:\n",
    "\n",
    "$$ O(n(w + g + r)) $$\n",
    "\n",
    "where:\n",
    "- $n$ is the sequence length,\n",
    "- $w$ is the window size,\n",
    "- $g$ is the number of global tokens,\n",
    "- $r$ is the number of random connections.\n",
    "\n",
    "This is a significant improvement over the $O(n^2)$ complexity of full attention, making BIGBIRD scalable to sequences of length up to 10,000 or more.\n",
    "\n",
    "### 4.5 Advantages\n",
    "- **Scalability:** Enables efficient processing of very long sequences, making it suitable for tasks like document-level NLP and genomics.\n",
    "- **Performance:** Achieves performance comparable to full attention transformers, as demonstrated on benchmarks like GLUE, SQuAD, and long-document tasks.\n",
    "- **Theoretical Guarantees:** Provides provable guarantees of expressiveness, ensuring it can model complex dependencies.\n",
    "\n",
    "### 4.6 Limitations\n",
    "- **Hyperparameter Tuning:** Requires careful tuning of $w$, $g$, and $r$ to balance efficiency and performance.\n",
    "- **Complexity:** The combined mechanism is more complex to implement and debug compared to simpler sparse attention methods.\n",
    "- **Task Dependence:** While effective for long-sequence tasks, BIGBIRD may be overkill for short-sequence tasks, where full attention is feasible.\n",
    "\n",
    "### 4.7 Practical Considerations\n",
    "- **Implementation:** BIGBIRD is available in libraries like Hugging Face Transformers, with pre-trained models for various tasks. Researchers can fine-tune these models or implement custom versions based on specific requirements.\n",
    "- **Evaluation Metrics:** When evaluating BIGBIRD, consider metrics sensitive to long-range dependencies, such as ROUGE scores for summarization or F1 scores for question answering on long documents.\n",
    "- **Hardware Optimization:** To maximize efficiency, leverage hardware accelerators (e.g., GPUs, TPUs) and techniques like mixed-precision training, especially for very long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Comparison of Attention Mechanisms\n",
    "\n",
    "To provide a comprehensive understanding, we compare random attention, window attention, global attention, and BIGBIRD across several dimensions:\n",
    "\n",
    "| **Aspect**               | **Random Attention**      | **Window Attention**      | **Global Attention**      | **BIGBIRD**               |\n",
    "|--------------------------|---------------------------|---------------------------|---------------------------|---------------------------|\n",
    "| **Attention Type**       | Sparse, random            | Sparse, local             | Sparse, global            | Sparse, hybrid            |\n",
    "| **Context Captured**     | Random subset             | Local window              | Global via tokens         | Local, global, random     |\n",
    "| **Complexity**           | $O(nm)$                   | $O(nw)$                   | $O(gn)$                   | $O(n(w + g + r))$         |\n",
    "| **Long-Range Dependencies** | Limited                 | Limited                   | Strong                    | Strong                    |\n",
    "| **Deterministic**        | No                        | Yes                       | Yes                       | Partially                 |\n",
    "| **Task Suitability**     | General, efficiency-focused | Local context tasks       | Global context tasks      | Long-sequence tasks       |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "Attention mechanisms are critical for sequence modeling, but their computational complexity poses challenges for long sequences. Random attention, window attention, and global attention each offer unique trade-offs, addressing efficiency, locality, and long-range dependencies, respectively. BIGBIRD unifies these approaches into a scalable and expressive sparse attention mechanism, making it a powerful tool for tasks involving long sequences, such as document understanding, genomics, and beyond.\n",
    "\n",
    "For researchers and AI scientists, understanding these mechanisms in detail—along with their mathematical formulations, theoretical underpinnings, and practical implications—is essential for designing state-of-the-art models. By carefully selecting and tuning attention mechanisms, one can achieve optimal performance, scalability, and efficiency tailored to specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjSiSNZz71YW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCSkP7FVBAPc"
   },
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxX7I0STU2lNa0tKuGjLgY",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
